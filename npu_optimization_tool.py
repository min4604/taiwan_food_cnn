#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AMD NPU æ•ˆèƒ½æœ€ä½³åŒ–å·¥å…·
NPU Performance Optimization Tool

å°ˆé–€ç”¨æ–¼æ¸¬è©¦å’Œæœ€ä½³åŒ– AMD Ryzen AI 9HX NPU çš„ä½¿ç”¨ç‡å’Œæ•ˆèƒ½
"""

import os
import time
import glob
import numpy as np
from optimized_amd_npu import OptimizedAMDNPUInference, benchmark_npu_utilization

def find_model_files():
    """å°‹æ‰¾å¯ç”¨çš„æ¨¡å‹æª”æ¡ˆ"""
    if not os.path.exists('models'):
        print("âŒ æ‰¾ä¸åˆ° models è³‡æ–™å¤¾")
        return []
    
    model_files = glob.glob('models/*.pth')
    return model_files

def find_test_images(max_images=100):
    """å°‹æ‰¾æ¸¬è©¦åœ–ç‰‡"""
    test_dir = 'archive/tw_food_101/test'
    
    if not os.path.exists(test_dir):
        print(f"âŒ æ‰¾ä¸åˆ°æ¸¬è©¦åœ–ç‰‡ç›®éŒ„: {test_dir}")
        return []
    
    # å°‹æ‰¾å„ç¨®æ ¼å¼çš„åœ–ç‰‡
    extensions = ['*.jpg', '*.jpeg', '*.png', '*.JPG', '*.JPEG', '*.PNG']
    all_images = []
    
    for ext in extensions:
        pattern = os.path.join(test_dir, ext)
        images = glob.glob(pattern)
        all_images.extend(images)
    
    # é™åˆ¶æ•¸é‡ä¸¦æ’åº
    all_images = sorted(all_images)[:max_images]
    print(f"ğŸ“¸ æ‰¾åˆ° {len(all_images)} å¼µæ¸¬è©¦åœ–ç‰‡")
    
    return all_images

def test_npu_utilization():
    """æ¸¬è©¦ NPU ä½¿ç”¨ç‡"""
    print("ğŸš€ AMD NPU ä½¿ç”¨ç‡æ¸¬è©¦å·¥å…·")
    print("=" * 60)
    
    # æª¢æŸ¥æ¨¡å‹
    model_files = find_model_files()
    if not model_files:
        print("âŒ æ‰¾ä¸åˆ°è¨“ç·´å¥½çš„æ¨¡å‹æª”æ¡ˆ")
        print("è«‹å…ˆåŸ·è¡Œ python train_pytorch.py é€²è¡Œè¨“ç·´")
        return
    
    # é¸æ“‡æœ€æ–°çš„æ¨¡å‹
    latest_model = max(model_files, key=os.path.getctime)
    print(f"ğŸ¯ ä½¿ç”¨æ¨¡å‹: {os.path.basename(latest_model)}")
    
    # å°‹æ‰¾æ¸¬è©¦åœ–ç‰‡
    test_images = find_test_images(200)  # ä½¿ç”¨æ›´å¤šåœ–ç‰‡æ¸¬è©¦
    if not test_images:
        return
    
    # åŸ·è¡ŒåŸºæº–æ¸¬è©¦
    print(f"\nğŸ§ª é–‹å§‹ NPU æ•ˆèƒ½åŸºæº–æ¸¬è©¦...")
    results = benchmark_npu_utilization(
        latest_model, 
        test_images, 
        batch_sizes=[1, 2, 4, 8, 16, 24, 32, 48, 64]
    )
    
    return results

def run_optimized_inference():
    """åŸ·è¡Œæœ€ä½³åŒ–æ¨ç†"""
    print("âš¡ AMD NPU æœ€ä½³åŒ–æ¨ç†æ¸¬è©¦")
    print("=" * 50)
    
    # æª¢æŸ¥æ¨¡å‹
    model_files = find_model_files()
    if not model_files:
        print("âŒ æ‰¾ä¸åˆ°è¨“ç·´å¥½çš„æ¨¡å‹æª”æ¡ˆ")
        return
    
    latest_model = max(model_files, key=os.path.getctime)
    print(f"ğŸ¯ ä½¿ç”¨æ¨¡å‹: {os.path.basename(latest_model)}")
    
    # å°‹æ‰¾æ¸¬è©¦åœ–ç‰‡
    test_images = find_test_images(50)
    if not test_images:
        return
    
    # æ¸¬è©¦ä¸åŒçš„æœ€ä½³åŒ–è¨­å®š
    configs = [
        {'batch_size': 16, 'num_threads': 4, 'name': 'æ¨™æº–è¨­å®š'},
        {'batch_size': 32, 'num_threads': 6, 'name': 'é«˜ä¸¦è¡Œè¨­å®š'},
        {'batch_size': 24, 'num_threads': 8, 'name': 'æœ€å¤§ä¸¦è¡Œè¨­å®š'},
        {'batch_size': 8, 'num_threads': 2, 'name': 'ä¿å®ˆè¨­å®š'},
    ]
    
    print(f"\nğŸ“Š æ¸¬è©¦ä¸åŒæœ€ä½³åŒ–è¨­å®š...")
    
    results = []
    
    for config in configs:
        print(f"\nğŸ”§ æ¸¬è©¦: {config['name']}")
        print(f"   æ‰¹æ¬¡å¤§å°: {config['batch_size']}")
        print(f"   åŸ·è¡Œç·’æ•¸: {config['num_threads']}")
        print("-" * 40)
        
        try:
            # å»ºç«‹æœ€ä½³åŒ–æ¨ç†å¼•æ“
            npu_inference = OptimizedAMDNPUInference(
                latest_model,
                batch_size=config['batch_size'],
                num_threads=config['num_threads']
            )
            
            # åŸ·è¡Œæ¨ç†æ¸¬è©¦
            start_time = time.time()
            predictions = npu_inference.predict_image_batch(test_images)
            total_time = time.time() - start_time
            
            if predictions:
                throughput = len(test_images) / total_time
                avg_time = total_time / len(test_images)
                
                results.append({
                    'name': config['name'],
                    'batch_size': config['batch_size'],
                    'num_threads': config['num_threads'],
                    'throughput': throughput,
                    'total_time': total_time,
                    'avg_time': avg_time
                })
                
                print(f"âœ… ååé‡: {throughput:.1f} åœ–ç‰‡/ç§’")
                print(f"ğŸ“Š ç¸½æ™‚é–“: {total_time:.3f}s")
                print(f"â±ï¸  å¹³å‡æ™‚é–“: {avg_time:.3f}s/åœ–ç‰‡")
            
            # æ¸…ç†
            npu_inference.shutdown()
            del npu_inference
            
        except Exception as e:
            print(f"âŒ è¨­å®šæ¸¬è©¦å¤±æ•—: {e}")
    
    # é¡¯ç¤ºçµæœæ¯”è¼ƒ
    if results:
        print(f"\nğŸ“ˆ æœ€ä½³åŒ–è¨­å®šæ¯”è¼ƒçµæœ")
        print("=" * 80)
        print(f"{'è¨­å®šåç¨±':<12} {'æ‰¹æ¬¡':<6} {'åŸ·è¡Œç·’':<8} {'ååé‡':<12} {'ç¸½æ™‚é–“':<10} {'å¹³å‡æ™‚é–“':<10}")
        print("-" * 80)
        
        for result in results:
            print(f"{result['name']:<12} {result['batch_size']:<6} "
                  f"{result['num_threads']:<8} {result['throughput']:<12.1f} "
                  f"{result['total_time']:<10.3f} {result['avg_time']:<10.3f}")
        
        best_result = max(results, key=lambda x: x['throughput'])
        print(f"\nğŸ† æœ€ä½³è¨­å®š: {best_result['name']}")
        print(f"   æœ€é«˜ååé‡: {best_result['throughput']:.1f} åœ–ç‰‡/ç§’")
        print(f"   å»ºè­°æ‰¹æ¬¡å¤§å°: {best_result['batch_size']}")
        print(f"   å»ºè­°åŸ·è¡Œç·’æ•¸: {best_result['num_threads']}")

def monitor_npu_real_time():
    """å³æ™‚ç›£æ§ NPU ä½¿ç”¨ç‡"""
    print("ğŸ“Š NPU å³æ™‚æ•ˆèƒ½ç›£æ§")
    print("=" * 50)
    
    model_files = find_model_files()
    if not model_files:
        print("âŒ æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆ")
        return
    
    latest_model = max(model_files, key=os.path.getctime)
    test_images = find_test_images(20)
    
    if not test_images:
        return
    
    print(f"ğŸ¯ ä½¿ç”¨æ¨¡å‹: {os.path.basename(latest_model)}")
    print(f"ğŸ“¸ æ¸¬è©¦åœ–ç‰‡: {len(test_images)} å¼µ")
    print("\nğŸ”„ é–‹å§‹å³æ™‚ç›£æ§ (Ctrl+C åœæ­¢)...")
    
    try:
        # å»ºç«‹é«˜æ•ˆèƒ½è¨­å®š
        npu_inference = OptimizedAMDNPUInference(
            latest_model,
            batch_size=32,
            num_threads=6
        )
        
        iteration = 1
        total_images = 0
        total_time = 0
        
        while True:
            print(f"\nğŸ“Š ç¬¬ {iteration} æ¬¡æ¨ç†:")
            
            start_time = time.time()
            predictions = npu_inference.predict_image_batch(test_images)
            iteration_time = time.time() - start_time
            
            if predictions:
                throughput = len(test_images) / iteration_time
                total_images += len(test_images)
                total_time += iteration_time
                avg_throughput = total_images / total_time
                
                print(f"   æœ¬æ¬¡ååé‡: {throughput:.1f} åœ–ç‰‡/ç§’")
                print(f"   ç´¯è¨ˆå¹³å‡: {avg_throughput:.1f} åœ–ç‰‡/ç§’")
                print(f"   NPU ç‹€æ…‹: {'ğŸŸ¢ é«˜ä½¿ç”¨ç‡' if throughput > 20 else 'ğŸŸ¡ ä¸­ç­‰ä½¿ç”¨ç‡' if throughput > 10 else 'ğŸ”´ ä½ä½¿ç”¨ç‡'}")
            
            iteration += 1
            time.sleep(2)  # æš«åœ 2 ç§’
            
    except KeyboardInterrupt:
        print(f"\n\nâ¹ï¸  ç›£æ§å·²åœæ­¢")
        print(f"ğŸ“Š ç¸½è¨ˆè™•ç†: {total_images} å¼µåœ–ç‰‡")
        print(f"â±ï¸  ç¸½è¨ˆæ™‚é–“: {total_time:.3f}s")
        if total_time > 0:
            print(f"ğŸš€ å¹³å‡æ•ˆèƒ½: {total_images/total_time:.1f} åœ–ç‰‡/ç§’")
        
        # æ¸…ç†
        npu_inference.shutdown()
    
    except Exception as e:
        print(f"âŒ ç›£æ§å¤±æ•—: {e}")

def optimize_npu_settings():
    """è‡ªå‹•æœ€ä½³åŒ– NPU è¨­å®š"""
    print("ğŸ”§ NPU è‡ªå‹•æœ€ä½³åŒ–è¨­å®š")
    print("=" * 50)
    
    model_files = find_model_files()
    if not model_files:
        print("âŒ æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆ")
        return None
    
    latest_model = max(model_files, key=os.path.getctime)
    test_images = find_test_images(30)
    
    if not test_images:
        return None
    
    print(f"ğŸ¯ ä½¿ç”¨æ¨¡å‹: {os.path.basename(latest_model)}")
    print(f"ğŸ“¸ æ¸¬è©¦åœ–ç‰‡: {len(test_images)} å¼µ")
    print("\nğŸ” è‡ªå‹•å°‹æ‰¾æœ€ä½³è¨­å®š...")
    
    # æ¸¬è©¦åƒæ•¸çµ„åˆ
    batch_sizes = [8, 16, 24, 32, 48]
    thread_counts = [2, 4, 6, 8]
    
    best_config = None
    best_throughput = 0
    
    total_tests = len(batch_sizes) * len(thread_counts)
    current_test = 0
    
    for batch_size in batch_sizes:
        for num_threads in thread_counts:
            current_test += 1
            print(f"\nğŸ§ª æ¸¬è©¦ {current_test}/{total_tests}: æ‰¹æ¬¡={batch_size}, åŸ·è¡Œç·’={num_threads}")
            
            try:
                npu_inference = OptimizedAMDNPUInference(
                    latest_model,
                    batch_size=batch_size,
                    num_threads=num_threads
                )
                
                # åŸ·è¡Œå¤šæ¬¡æ¸¬è©¦å–å¹³å‡
                throughputs = []
                for _ in range(3):
                    start_time = time.time()
                    predictions = npu_inference.predict_image_batch(test_images)
                    elapsed = time.time() - start_time
                    
                    if predictions:
                        throughput = len(test_images) / elapsed
                        throughputs.append(throughput)
                
                if throughputs:
                    avg_throughput = np.mean(throughputs)
                    print(f"   å¹³å‡ååé‡: {avg_throughput:.1f} åœ–ç‰‡/ç§’")
                    
                    if avg_throughput > best_throughput:
                        best_throughput = avg_throughput
                        best_config = {
                            'batch_size': batch_size,
                            'num_threads': num_threads,
                            'throughput': avg_throughput
                        }
                        print(f"   ğŸ† æ–°çš„æœ€ä½³è¨­å®šï¼")
                
                npu_inference.shutdown()
                del npu_inference
                
            except Exception as e:
                print(f"   âŒ æ¸¬è©¦å¤±æ•—: {e}")
    
    if best_config:
        print(f"\nğŸ‰ è‡ªå‹•æœ€ä½³åŒ–å®Œæˆï¼")
        print(f"ğŸ† æœ€ä½³è¨­å®š:")
        print(f"   æ‰¹æ¬¡å¤§å°: {best_config['batch_size']}")
        print(f"   åŸ·è¡Œç·’æ•¸: {best_config['num_threads']}")
        print(f"   æœ€é«˜ååé‡: {best_config['throughput']:.1f} åœ–ç‰‡/ç§’")
        
        # å„²å­˜æœ€ä½³è¨­å®š
        config_file = "optimal_npu_config.txt"
        with open(config_file, 'w', encoding='utf-8') as f:
            f.write(f"AMD NPU æœ€ä½³è¨­å®š\n")
            f.write(f"æ‰¹æ¬¡å¤§å°: {best_config['batch_size']}\n")
            f.write(f"åŸ·è¡Œç·’æ•¸: {best_config['num_threads']}\n")
            f.write(f"æœ€é«˜ååé‡: {best_config['throughput']:.1f} åœ–ç‰‡/ç§’\n")
            f.write(f"æ¸¬è©¦æ™‚é–“: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        
        print(f"ğŸ’¾ æœ€ä½³è¨­å®šå·²å„²å­˜è‡³: {config_file}")
        
        return best_config
    else:
        print(f"âŒ æœªæ‰¾åˆ°æœ‰æ•ˆçš„æœ€ä½³è¨­å®š")
        return None

def main():
    """ä¸»é¸å–®"""
    while True:
        print("\nğŸš€ AMD NPU æ•ˆèƒ½æœ€ä½³åŒ–å·¥å…·")
        print("=" * 50)
        print("1. ğŸ§ª NPU ä½¿ç”¨ç‡åŸºæº–æ¸¬è©¦")
        print("2. âš¡ æœ€ä½³åŒ–æ¨ç†æ¸¬è©¦")
        print("3. ğŸ“Š å³æ™‚æ•ˆèƒ½ç›£æ§")
        print("4. ğŸ”§ è‡ªå‹•æœ€ä½³åŒ–è¨­å®š")
        print("5. ğŸ“‹ æª¢è¦–ç¡¬é«”è³‡è¨Š")
        print("6. âŒ é€€å‡º")
        
        try:
            choice = input("\nğŸ‘‰ è«‹é¸æ“‡åŠŸèƒ½ (1-6): ").strip()
            
            if choice == "1":
                test_npu_utilization()
                input("\næŒ‰ Enter éµç¹¼çºŒ...")
                
            elif choice == "2":
                run_optimized_inference()
                input("\næŒ‰ Enter éµç¹¼çºŒ...")
                
            elif choice == "3":
                monitor_npu_real_time()
                input("\næŒ‰ Enter éµç¹¼çºŒ...")
                
            elif choice == "4":
                optimize_npu_settings()
                input("\næŒ‰ Enter éµç¹¼çºŒ...")
                
            elif choice == "5":
                try:
                    import onnxruntime as ort
                    providers = ort.get_available_providers()
                    print(f"\nğŸ“‹ ONNX Runtime æä¾›è€…: {providers}")
                    
                    if 'DmlExecutionProvider' in providers:
                        print("âœ… DirectML å¯ç”¨ - AMD NPU æ”¯æ´æ­£å¸¸")
                    else:
                        print("âŒ DirectML ä¸å¯ç”¨")
                        
                except ImportError:
                    print("âŒ ONNX Runtime æœªå®‰è£")
                    
                input("\næŒ‰ Enter éµç¹¼çºŒ...")
                
            elif choice == "6":
                print("\nğŸ‘‹ é€€å‡ºç¨‹å¼ï¼Œæ„Ÿè¬ä½¿ç”¨ï¼")
                break
                
            else:
                print("âš ï¸  è«‹è¼¸å…¥ 1-6")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç¨‹å¼çµæŸï¼Œæ„Ÿè¬ä½¿ç”¨ï¼")
            break

if __name__ == '__main__':
    main()