import torch
import numpy as np
from torchvision import transforms
from PIL import Image
import os
import concurrent.futures
import threading
from queue import Queue
import time

class OptimizedAMDNPUInference:
    """
    æœ€ä½³åŒ–çš„ AMD Ryzen AI 9HX NPU æ¨ç†é¡
    å°ˆæ³¨æ–¼æé«˜ NPU ä½¿ç”¨ç‡å’Œä¸¦è¡Œè™•ç†
    """
    
    def __init__(self, model_path, img_size=224, batch_size=16, num_threads=4):
        self.img_size = img_size
        self.batch_size = batch_size  # NPU æ‰¹æ¬¡è™•ç†å¤§å°
        self.num_threads = num_threads  # ä¸¦è¡Œè™•ç†åŸ·è¡Œç·’æ•¸
        self.onnx_model_path = None
        self.ort_session = None
        self.pytorch_model = None
        
        # è¨­å®šè½‰æ›
        self.transform = transforms.Compose([
            transforms.Resize((img_size, img_size)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
        
        # åŸ·è¡Œç·’å®‰å…¨çš„ä½‡åˆ—ç³»çµ±
        self.input_queue = Queue()
        self.output_queue = Queue()
        self.processing_thread = None
        self.shutdown_flag = threading.Event()
        
        # åˆå§‹åŒ–æ¨¡å‹
        self._setup_model(model_path)
        
        # å•Ÿå‹•å¾Œå°è™•ç†åŸ·è¡Œç·’
        self._start_background_processing()
    
    def _detect_model_architecture(self, model_path):
        """å¾æ¨¡å‹æª”æ¡ˆåç¨±ä¸­æª¢æ¸¬æ¨¡å‹æ¶æ§‹"""
        filename = os.path.basename(model_path).lower()
        
        if 'efficientnet_b3' in filename:
            return 'efficientnet_b3'
        elif 'convnext_tiny' in filename:
            return 'convnext_tiny'
        elif 'regnet_y' in filename:
            return 'regnet_y'
        elif 'vit' in filename:
            return 'vit'
        elif 'resnet50' in filename:
            return 'resnet50'
        else:
            # å˜—è©¦å¾æ¨¡å‹å…§å®¹æª¢æ¸¬
            try:
                state_dict = torch.load(model_path, map_location='cpu')
                keys = list(state_dict.keys())
                
                # æ ¹æ“š state_dict çš„éµå€¼æª¢æ¸¬æ¶æ§‹
                if any('features' in key for key in keys):
                    if any('block' in key for key in keys):
                        return 'efficientnet_b3'  # EfficientNet ç‰¹å¾µ
                    elif any('stages' in key for key in keys):
                        return 'convnext_tiny'    # ConvNeXt ç‰¹å¾µ
                    else:
                        return 'efficientnet_b3'  # é è¨­ç‚º EfficientNet
                elif any('layer' in key for key in keys):
                    return 'resnet50'             # ResNet ç‰¹å¾µ
                elif any('blocks' in key for key in keys):
                    return 'vit'                  # ViT ç‰¹å¾µ
                else:
                    return 'resnet50'             # é è¨­ç‚º ResNet50
            except:
                return 'resnet50'                 # é è¨­ç‚º ResNet50
    
    def _setup_model(self, pytorch_model_path):
        """è¨­å®š AMD NPU æ¨ç†æ¨¡å‹ï¼Œé‡å°æœ€å¤§ä½¿ç”¨ç‡æœ€ä½³åŒ–"""
        try:
            # æª¢æŸ¥æ˜¯å¦æ”¯æ´ ONNX Runtime DirectML
            import onnxruntime as ort
            providers = ort.get_available_providers()
            
            if 'DmlExecutionProvider' not in providers:
                raise RuntimeError("DirectML ä¸å¯ç”¨ï¼Œç„¡æ³•ä½¿ç”¨ AMD NPU")
            
            print("âœ… æª¢æ¸¬åˆ° DirectML æ”¯æ´")
            print("ğŸš€ ç‚º AMD Ryzen AI 9 HX 370 NPU æœ€å¤§ä½¿ç”¨ç‡æœ€ä½³åŒ–...")
            
            # è½‰æ› PyTorch æ¨¡å‹ç‚º ONNX (å¦‚æœéœ€è¦)
            onnx_path = self._convert_to_onnx_optimized(pytorch_model_path)
            
            # å»ºç«‹ ONNX Runtime æœƒè©±ï¼Œå°ˆç‚ºæœ€å¤§ NPU ä½¿ç”¨ç‡æœ€ä½³åŒ–
            providers = [
                ('DmlExecutionProvider', {
                    'device_id': 0,
                    'enable_dynamic_shapes': True,
                    'disable_memory_arena': False,  # å•Ÿç”¨è¨˜æ†¶é«”æ± 
                    'enable_graph_capture': True,  # å•Ÿç”¨åœ–å½¢æ•ç²
                    'enable_dynamic_shapes': True,
                    'force_sequential_execution': False,  # å…è¨±ä¸¦è¡ŒåŸ·è¡Œ
                }),
                'CPUExecutionProvider'   # CPU ä½œç‚ºå‚™æ´
            ]
            
            # å»ºç«‹æœƒè©±é¸é …ä»¥æœ€å¤§åŒ–æ•ˆèƒ½
            session_options = ort.SessionOptions()
            
            # æ•ˆèƒ½æœ€ä½³åŒ–è¨­å®š
            session_options.enable_mem_pattern = True
            session_options.enable_cpu_mem_arena = True
            session_options.enable_mem_reuse = True
            session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            
            # ä¸¦è¡Œè¨­å®š - æœ€å¤§åŒ– NPU ä½¿ç”¨ç‡
            session_options.intra_op_num_threads = self.num_threads * 2  # å¢åŠ å…§éƒ¨ä¸¦è¡Œ
            session_options.inter_op_num_threads = self.num_threads      # å¢åŠ æ“ä½œé–“ä¸¦è¡Œ
            session_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL  # ä¸¦è¡ŒåŸ·è¡Œæ¨¡å¼
            
            # è¨˜æ†¶é«”æœ€ä½³åŒ–
            session_options.enable_profiling = False  # é—œé–‰åˆ†æä»¥æé«˜æ•ˆèƒ½
            session_options.log_severity_level = 3   # æ¸›å°‘æ—¥èªŒè¼¸å‡º
            
            self.ort_session = ort.InferenceSession(
                onnx_path, 
                providers=providers,
                sess_options=session_options
            )
            
            print(f"ğŸš€ NPU æœ€ä½³åŒ–æ¨ç†æœƒè©±å·²å»ºç«‹")
            print(f"ğŸ“Š ä½¿ç”¨æä¾›è€…: {self.ort_session.get_providers()}")
            print(f"âš¡ æ‰¹æ¬¡å¤§å°: {self.batch_size}")
            print(f"ğŸ”„ ä¸¦è¡ŒåŸ·è¡Œç·’: {self.num_threads}")
            
            # é ç†± NPU
            self._warmup_npu()
            
        except ImportError:
            print("âŒ ONNX Runtime æœªå®‰è£ï¼Œè«‹åŸ·è¡Œ install_amd_npu.bat")
            self._setup_cpu_fallback(pytorch_model_path)
        except Exception as e:
            print(f"âŒ AMD NPU è¨­å®šå¤±æ•—: {e}")
            print("ğŸ’¡ å°‡å›é€€åˆ° CPU æ¨ç†")
            self._setup_cpu_fallback(pytorch_model_path)
    
    def _convert_to_onnx_optimized(self, pytorch_model_path):
        """å°‡ PyTorch æ¨¡å‹è½‰æ›ç‚ºæœ€ä½³åŒ–çš„ ONNX æ ¼å¼"""
        onnx_path = pytorch_model_path.replace('.pth', '_optimized_npu.onnx')
        
        if os.path.exists(onnx_path):
            print(f"ğŸ“ ä½¿ç”¨ç¾æœ‰çš„æœ€ä½³åŒ– ONNX æ¨¡å‹: {onnx_path}")
            return onnx_path
        
        print("ğŸ”„ è½‰æ› PyTorch æ¨¡å‹ç‚ºæœ€ä½³åŒ– ONNX...")
        
        try:
            # æª¢æ¸¬æ¨¡å‹æ¶æ§‹
            model_architecture = self._detect_model_architecture(pytorch_model_path)
            print(f"ğŸ—ï¸  æª¢æ¸¬åˆ°æ¨¡å‹æ¶æ§‹: {model_architecture}")
            
            # è¼‰å…¥å°æ‡‰çš„æ¨¡å‹é¡å‹
            from pytorch_model import get_model
            model = get_model(model_architecture, num_classes=101, dropout_rate=0.3)
            model.load_state_dict(torch.load(pytorch_model_path, map_location='cpu'))
            model.eval()
            
            # å»ºç«‹æ”¯æ´æ‰¹æ¬¡è™•ç†çš„ç¯„ä¾‹è¼¸å…¥
            dummy_input = torch.randn(self.batch_size, 3, self.img_size, self.img_size)
            
            # åŒ¯å‡ºç‚ºæœ€ä½³åŒ–çš„ ONNX
            torch.onnx.export(
                model,
                dummy_input,
                onnx_path,
                export_params=True,
                opset_version=14,  # ä½¿ç”¨è¼ƒæ–°çš„ opset ç‰ˆæœ¬
                do_constant_folding=True,
                input_names=['input'],
                output_names=['output'],
                dynamic_axes={
                    'input': {0: 'batch_size'},
                    'output': {0: 'batch_size'}
                },
                # æœ€ä½³åŒ–è¨­å®š
                training=torch.onnx.TrainingMode.EVAL,
                verbose=False
            )
            
            print(f"âœ… æœ€ä½³åŒ– ONNX æ¨¡å‹å·²å„²å­˜: {onnx_path}")
            return onnx_path
            
        except Exception as e:
            print(f"âŒ ONNX è½‰æ›å¤±æ•—: {e}")
            raise
    
    def _warmup_npu(self):
        """é ç†± NPU ä»¥ç¢ºä¿æœ€ä½³æ•ˆèƒ½"""
        if self.ort_session:
            print("ğŸ”¥ é ç†± AMD NPU...")
            try:
                # å»ºç«‹è™›æ“¬è¼¸å…¥é€²è¡Œé ç†±
                dummy_input = np.random.randn(self.batch_size, 3, self.img_size, self.img_size).astype(np.float32)
                input_name = self.ort_session.get_inputs()[0].name
                
                # åŸ·è¡Œå¹¾æ¬¡é ç†±æ¨ç†
                for i in range(3):
                    start_time = time.time()
                    _ = self.ort_session.run(None, {input_name: dummy_input})
                    elapsed = time.time() - start_time
                    print(f"   é ç†± {i+1}/3: {elapsed:.3f}s")
                
                print("âœ… NPU é ç†±å®Œæˆï¼Œæ•ˆèƒ½æœ€ä½³åŒ–")
                
            except Exception as e:
                print(f"âš ï¸  NPU é ç†±å¤±æ•—: {e}")
    
    def _start_background_processing(self):
        """å•Ÿå‹•å¾Œå°æ‰¹æ¬¡è™•ç†åŸ·è¡Œç·’"""
        if self.ort_session:
            self.processing_thread = threading.Thread(target=self._batch_processing_worker, daemon=True)
            self.processing_thread.start()
            print("ğŸ”„ å¾Œå°æ‰¹æ¬¡è™•ç†åŸ·è¡Œç·’å·²å•Ÿå‹•")
    
    def _batch_processing_worker(self):
        """å¾Œå°æ‰¹æ¬¡è™•ç†å·¥ä½œåŸ·è¡Œç·’"""
        while not self.shutdown_flag.is_set():
            batch_items = []
            
            # æ”¶é›†æ‰¹æ¬¡è³‡æ–™
            for _ in range(self.batch_size):
                try:
                    if self.shutdown_flag.is_set():
                        break
                    item = self.input_queue.get(timeout=0.1)
                    batch_items.append(item)
                except:
                    break
            
            if batch_items:
                self._process_batch(batch_items)
    
    def _process_batch(self, batch_items):
        """è™•ç†ä¸€å€‹æ‰¹æ¬¡çš„è³‡æ–™"""
        try:
            # æº–å‚™æ‰¹æ¬¡è¼¸å…¥
            batch_input = np.stack([item['input'] for item in batch_items])
            
            # NPU æ‰¹æ¬¡æ¨ç†
            input_name = self.ort_session.get_inputs()[0].name
            start_time = time.time()
            result = self.ort_session.run(None, {input_name: batch_input})
            inference_time = time.time() - start_time
            
            output = result[0]
            predictions = np.argmax(output, axis=1)
            
            # å°‡çµæœæ”¾å›è¼¸å‡ºä½‡åˆ—
            for i, item in enumerate(batch_items):
                self.output_queue.put({
                    'id': item['id'],
                    'prediction': predictions[i],
                    'inference_time': inference_time / len(batch_items)
                })
                
            print(f"âš¡ æ‰¹æ¬¡æ¨ç†å®Œæˆ: {len(batch_items)} å¼µåœ–ç‰‡, {inference_time:.3f}s")
            
        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡è™•ç†å¤±æ•—: {e}")
            # å›é€€åˆ°å–®å¼µè™•ç†
            for item in batch_items:
                self.output_queue.put({
                    'id': item['id'],
                    'prediction': -1,
                    'inference_time': 0
                })
    
    def predict_image_batch(self, image_paths):
        """æ‰¹æ¬¡è™•ç†å¤šå¼µåœ–ç‰‡ä»¥æœ€å¤§åŒ– NPU ä½¿ç”¨ç‡"""
        if not self.ort_session:
            return self._predict_cpu_fallback(image_paths)
        
        print(f"ğŸš€ NPU æ‰¹æ¬¡æ¨ç†: {len(image_paths)} å¼µåœ–ç‰‡")
        start_time = time.time()
        
        # ä¸¦è¡Œè¼‰å…¥å’Œé è™•ç†åœ–ç‰‡
        def preprocess_image(img_path_id):
            img_path, img_id = img_path_id
            try:
                image = Image.open(img_path).convert('RGB')
                input_tensor = self.transform(image).unsqueeze(0)
                return {
                    'id': img_id,
                    'input': input_tensor.numpy()[0],
                    'path': img_path
                }
            except Exception as e:
                print(f"âš ï¸  åœ–ç‰‡é è™•ç†å¤±æ•— {img_path}: {e}")
                return None
        
        # ä½¿ç”¨å¤šåŸ·è¡Œç·’é è™•ç†
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.num_threads) as executor:
            image_path_ids = [(path, i) for i, path in enumerate(image_paths)]
            preprocessed = list(executor.map(preprocess_image, image_path_ids))
        
        # éæ¿¾å¤±æ•—çš„é …ç›®
        valid_items = [item for item in preprocessed if item is not None]
        
        if not valid_items:
            print("âŒ æ²’æœ‰æœ‰æ•ˆçš„åœ–ç‰‡å¯è™•ç†")
            return []
        
        # æ‰¹æ¬¡æ¨ç†
        predictions = []
        
        for i in range(0, len(valid_items), self.batch_size):
            batch = valid_items[i:i + self.batch_size]
            batch_input = np.stack([item['input'] for item in batch])
            
            try:
                input_name = self.ort_session.get_inputs()[0].name
                batch_start = time.time()
                result = self.ort_session.run(None, {input_name: batch_input})
                batch_time = time.time() - batch_start
                
                output = result[0]
                # è¨ˆç®— softmax ä»¥ç²å¾—ä¿¡å¿ƒåˆ†æ•¸
                batch_softmax = np.exp(output) / np.sum(np.exp(output), axis=1, keepdims=True)
                batch_predictions = np.argmax(output, axis=1)
                batch_confidences = np.max(batch_softmax, axis=1)
                
                for j, item in enumerate(batch):
                    predictions.append({
                        'id': item['id'],
                        'path': item['path'],
                        'prediction': int(batch_predictions[j]),
                        'confidence': float(batch_confidences[j]),
                        'inference_time': batch_time / len(batch)
                    })
                
                print(f"âš¡ æ‰¹æ¬¡ {i//self.batch_size + 1}: {len(batch)} å¼µåœ–ç‰‡, {batch_time:.3f}s")
                
            except Exception as e:
                print(f"âŒ æ‰¹æ¬¡æ¨ç†å¤±æ•—: {e}")
                for item in batch:
                    predictions.append({
                        'id': item['id'],
                        'path': item['path'],
                        'prediction': -1,
                        'confidence': 0.0,
                        'inference_time': 0
                    })
        
        total_time = time.time() - start_time
        throughput = len(valid_items) / total_time
        
        print(f"ğŸ‰ NPU æ‰¹æ¬¡æ¨ç†å®Œæˆ!")
        print(f"ğŸ“Š ç¸½æ™‚é–“: {total_time:.3f}s")
        print(f"âš¡ ååé‡: {throughput:.1f} åœ–ç‰‡/ç§’")
        print(f"ğŸš€ NPU ä½¿ç”¨ç‡: æœ€ä½³åŒ–æ‰¹æ¬¡è™•ç†")
        
        return predictions
    
    def predict_image(self, image_path):
        """å–®å¼µåœ–ç‰‡æ¨ç† (ç›¸å®¹æ€§æ–¹æ³•)"""
        results = self.predict_image_batch([image_path])
        return results[0]['prediction'] if results else -1
    
    def _predict_cpu_fallback(self, image_paths):
        """CPU å‚™æ´æ¨ç†"""
        if not self.pytorch_model:
            return []
        
        print("ğŸ”„ ä½¿ç”¨ CPU å‚™æ´æ¨ç†...")
        predictions = []
        
        for i, img_path in enumerate(image_paths):
            try:
                image = Image.open(img_path).convert('RGB')
                input_tensor = self.transform(image).unsqueeze(0)
                
                with torch.no_grad():
                    output = self.pytorch_model(input_tensor)
                    # è¨ˆç®— softmax ä»¥ç²å¾—ä¿¡å¿ƒåˆ†æ•¸
                    softmax_output = torch.softmax(output, dim=1)
                    confidence, predicted_class = torch.max(softmax_output, dim=1)
                
                predictions.append({
                    'id': i,
                    'path': img_path,
                    'prediction': predicted_class.item(),
                    'confidence': confidence.item(),
                    'inference_time': 0
                })
                
            except Exception as e:
                print(f"âŒ CPU æ¨ç†å¤±æ•— {img_path}: {e}")
                predictions.append({
                    'id': i,
                    'path': img_path,
                    'prediction': -1,
                    'confidence': 0.0,
                    'inference_time': 0
                })
        
        return predictions
    
    def _setup_cpu_fallback(self, pytorch_model_path):
        """è¨­å®š CPU å‚™æ´æ¨ç†"""
        try:
            # æª¢æ¸¬æ¨¡å‹æ¶æ§‹
            model_architecture = self._detect_model_architecture(pytorch_model_path)
            print(f"ğŸ—ï¸  CPU å‚™æ´æ¨¡å¼æª¢æ¸¬åˆ°æ¶æ§‹: {model_architecture}")
            
            # è¼‰å…¥å°æ‡‰çš„æ¨¡å‹é¡å‹
            from pytorch_model import get_model
            self.pytorch_model = get_model(model_architecture, num_classes=101, dropout_rate=0.3)
            self.pytorch_model.load_state_dict(torch.load(pytorch_model_path, map_location='cpu'))
            self.pytorch_model.eval()
            print("ğŸ”„ å·²è¨­å®š CPU å‚™æ´æ¨ç†")
        except Exception as e:
            print(f"âŒ CPU å‚™æ´è¨­å®šå¤±æ•—: {e}")
    
    def shutdown(self):
        """é—œé–‰è™•ç†åŸ·è¡Œç·’"""
        if self.processing_thread:
            self.shutdown_flag.set()
            self.processing_thread.join(timeout=1)
            print("ğŸ”„ å¾Œå°è™•ç†åŸ·è¡Œç·’å·²é—œé–‰")
    
    def __del__(self):
        """è§£æ§‹å‡½æ•¸"""
        self.shutdown()

def benchmark_npu_utilization(model_path, test_images, batch_sizes=[1, 4, 8, 16, 32]):
    """NPU ä½¿ç”¨ç‡åŸºæº–æ¸¬è©¦"""
    print("ğŸ§ª NPU ä½¿ç”¨ç‡åŸºæº–æ¸¬è©¦")
    print("=" * 60)
    
    results = []
    
    for batch_size in batch_sizes:
        print(f"\nğŸ“Š æ¸¬è©¦æ‰¹æ¬¡å¤§å°: {batch_size}")
        print("-" * 40)
        
        try:
            # å»ºç«‹æœ€ä½³åŒ–æ¨ç†å¼•æ“
            npu_inference = OptimizedAMDNPUInference(
                model_path, 
                batch_size=batch_size,
                num_threads=4
            )
            
            # é¸æ“‡æ¸¬è©¦åœ–ç‰‡
            test_batch = test_images[:min(len(test_images), batch_size * 3)]
            
            # åŸ·è¡ŒåŸºæº–æ¸¬è©¦
            start_time = time.time()
            predictions = npu_inference.predict_image_batch(test_batch)
            total_time = time.time() - start_time
            
            if predictions:
                throughput = len(test_batch) / total_time
                avg_time = total_time / len(test_batch)
                
                results.append({
                    'batch_size': batch_size,
                    'images': len(test_batch),
                    'total_time': total_time,
                    'throughput': throughput,
                    'avg_time': avg_time
                })
                
                print(f"âœ… å®Œæˆ: {throughput:.1f} åœ–ç‰‡/ç§’")
            else:
                print("âŒ æ¸¬è©¦å¤±æ•—")
            
            # æ¸…ç†
            npu_inference.shutdown()
            del npu_inference
            
        except Exception as e:
            print(f"âŒ æ‰¹æ¬¡å¤§å° {batch_size} æ¸¬è©¦å¤±æ•—: {e}")
    
    # é¡¯ç¤ºçµæœæ‘˜è¦
    print(f"\nğŸ“ˆ NPU æ•ˆèƒ½åŸºæº–æ¸¬è©¦çµæœ")
    print("=" * 60)
    print(f"{'æ‰¹æ¬¡å¤§å°':<8} {'åœ–ç‰‡æ•¸':<8} {'ç¸½æ™‚é–“':<10} {'ååé‡':<12} {'å¹³å‡æ™‚é–“':<10}")
    print("-" * 60)
    
    for result in results:
        print(f"{result['batch_size']:<8} {result['images']:<8} "
              f"{result['total_time']:<10.3f} {result['throughput']:<12.1f} "
              f"{result['avg_time']:<10.3f}")
    
    if results:
        best_result = max(results, key=lambda x: x['throughput'])
        print(f"\nğŸ† æœ€ä½³æ•ˆèƒ½: æ‰¹æ¬¡å¤§å° {best_result['batch_size']}, "
              f"ååé‡ {best_result['throughput']:.1f} åœ–ç‰‡/ç§’")
    
    return results

if __name__ == '__main__':
    # æ¸¬è©¦æœ€ä½³åŒ–çš„ NPU æ¨ç†
    print("ğŸš€ æ¸¬è©¦æœ€ä½³åŒ– AMD NPU æ¨ç†")
    
    # æª¢æŸ¥åŸºæœ¬æ”¯æ´
    try:
        import onnxruntime as ort
        providers = ort.get_available_providers()
        print(f"ğŸ“‹ å¯ç”¨æä¾›è€…: {providers}")
        
        if 'DmlExecutionProvider' in providers:
            print("âœ… DirectML å¯ç”¨ - æº–å‚™æœ€ä½³åŒ–æ¸¬è©¦")
        else:
            print("âŒ DirectML ä¸å¯ç”¨")
            
    except ImportError:
        print("âŒ ONNX Runtime æœªå®‰è£")