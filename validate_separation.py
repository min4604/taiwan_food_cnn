#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
é©—è­‰è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†åˆ†é›¢æ€§è…³æœ¬

åŠŸèƒ½ï¼š
1. è¨ˆç®—æ¸¬è©¦é›†ä¸­æ‰€æœ‰åœ–ç‰‡çš„æ„ŸçŸ¥é›œæ¹Š (perceptual hash)ã€‚
2. éæ­·è¨“ç·´é›†ï¼Œè¨ˆç®—æ¯å¼µåœ–ç‰‡çš„é›œæ¹Šï¼Œä¸¦èˆ‡æ¸¬è©¦é›†é€²è¡Œæ¯”å°ã€‚
3. æ‰¾å‡ºèˆ‡æ¸¬è©¦é›†åœ–ç‰‡å®Œå…¨ç›¸åŒæˆ–é«˜åº¦ç›¸ä¼¼çš„è¨“ç·´åœ–ç‰‡ã€‚
4. ç”¢ç”Ÿå ±å‘Šï¼Œä¸¦æä¾›å¯é¸çš„è‡ªå‹•åˆªé™¤åŠŸèƒ½ã€‚

ä½¿ç”¨æ­¤è…³æœ¬å¯ä»¥é¿å…è¨“ç·´è³‡æ–™ä¸­åŒ…å«æ¸¬è©¦è³‡æ–™ï¼Œç¢ºä¿æ¨¡å‹è©•ä¼°çš„å…¬æ­£æ€§ã€‚
"""

import os
import argparse
from pathlib import Path
from collections import defaultdict

try:
    from PIL import Image
    import imagehash
except ImportError:
    print("ç¼ºå°‘å¿…è¦çš„å¥—ä»¶ï¼Œè«‹åŸ·è¡Œ: pip install Pillow imagehash")
    exit(1)

def compute_hashes(directory: Path, hash_size: int = 8) -> dict:
    """
    è¨ˆç®—æŒ‡å®šç›®éŒ„ä¸‹æ‰€æœ‰åœ–ç‰‡çš„æ„ŸçŸ¥é›œæ¹Šå€¼ã€‚
    
    è¿”å›ä¸€å€‹å­—å…¸ï¼Œkey æ˜¯é›œæ¹Šå€¼ï¼Œvalue æ˜¯å°æ‡‰çš„æª”æ¡ˆè·¯å¾‘åˆ—è¡¨ã€‚
    """
    hashes = defaultdict(list)
    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp'}
    
    print(f"æ­£åœ¨è¨ˆç®— '{directory}' ä¸­çš„åœ–ç‰‡é›œæ¹Šå€¼...")
    
    image_files = [p for p in directory.rglob('*') if p.suffix.lower() in image_extensions]
    
    if not image_files:
        print(f"è­¦å‘Šï¼šåœ¨ '{directory}' ä¸­æ‰¾ä¸åˆ°ä»»ä½•åœ–ç‰‡æª”æ¡ˆã€‚")
        return {}

    for i, img_path in enumerate(image_files):
        try:
            with Image.open(img_path) as img:
                # ä½¿ç”¨ average hashï¼Œé€Ÿåº¦å¿«ä¸”æ•ˆæœå¥½
                h = imagehash.average_hash(img, hash_size=hash_size)
                hashes[h].append(str(img_path))
        except Exception as e:
            print(f"ç„¡æ³•è™•ç†æª”æ¡ˆ {img_path}: {e}")
        
        # é¡¯ç¤ºé€²åº¦
        if (i + 1) % 200 == 0 or (i + 1) == len(image_files):
            print(f"  å·²è™•ç† {i + 1}/{len(image_files)} å¼µåœ–ç‰‡", end='\r')
            
    print(f"\nå®Œæˆï¼å…±è¨ˆç®—äº† {len(image_files)} å¼µåœ–ç‰‡ï¼Œå¾—åˆ° {len(hashes)} å€‹ç¨ç«‹é›œæ¹Šã€‚")
    return hashes

def find_duplicates(test_hashes: dict, train_dir: Path, similarity_threshold: int = 5, hash_size: int = 8):
    """
    åœ¨è¨“ç·´é›†ä¸­å°‹æ‰¾èˆ‡æ¸¬è©¦é›†é‡è¤‡æˆ–ç›¸ä¼¼çš„åœ–ç‰‡ã€‚
    """
    duplicates = []
    image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp'}
    
    print(f"\næ­£åœ¨æƒæè¨“ç·´ç›®éŒ„ '{train_dir}' ä¸¦èˆ‡æ¸¬è©¦é›†æ¯”å°...")
    
    train_image_files = [p for p in train_dir.rglob('*') if p.suffix.lower() in image_extensions]

    if not train_image_files:
        print(f"è­¦å‘Šï¼šåœ¨ '{train_dir}' ä¸­æ‰¾ä¸åˆ°ä»»ä½•åœ–ç‰‡æª”æ¡ˆã€‚")
        return []

    test_hash_keys = list(test_hashes.keys())

    for i, train_img_path in enumerate(train_image_files):
        try:
            with Image.open(train_img_path) as img:
                train_hash = imagehash.average_hash(img, hash_size=hash_size)
                
                # 1. æª¢æŸ¥ç²¾ç¢ºé‡è¤‡
                if train_hash in test_hashes:
                    match_info = {
                        "train_image": str(train_img_path),
                        "test_images": test_hashes[train_hash],
                        "distance": 0,
                        "type": "ç²¾ç¢ºé‡è¤‡"
                    }
                    duplicates.append(match_info)
                    print(f"\nç™¼ç¾ç²¾ç¢ºé‡è¤‡: {train_img_path}")
                    continue # æ‰¾åˆ°ç²¾ç¢ºé‡è¤‡å°±ä¸ç”¨å†æ¯”å°ç›¸ä¼¼åº¦

                # 2. æª¢æŸ¥ç›¸ä¼¼é‡è¤‡
                for test_hash in test_hash_keys:
                    distance = train_hash - test_hash
                    if distance <= similarity_threshold:
                        match_info = {
                            "train_image": str(train_img_path),
                            "test_images": test_hashes[test_hash],
                            "distance": distance,
                            "type": "ç›¸ä¼¼é‡è¤‡"
                        }
                        duplicates.append(match_info)
                        print(f"\nç™¼ç¾ç›¸ä¼¼é‡è¤‡ (å·®ç•°åº¦ {distance}): {train_img_path}")
                        break # æ‰¾åˆ°ä¸€å€‹ç›¸ä¼¼çš„å°±å¤ äº†
        except Exception as e:
            print(f"ç„¡æ³•è™•ç†æª”æ¡ˆ {train_img_path}: {e}")
            
        # é¡¯ç¤ºé€²åº¦
        if (i + 1) % 100 == 0 or (i + 1) == len(train_image_files):
            print(f"  å·²æƒæ {i + 1}/{len(train_image_files)} å¼µè¨“ç·´åœ–ç‰‡", end='\r')
            
    print(f"\næƒæå®Œæˆï¼")
    return duplicates

def main():
    parser = argparse.ArgumentParser(description="é©—è­‰è¨“ç·´é›†èˆ‡æ¸¬è©¦é›†çš„åˆ†é›¢æ€§ï¼Œé¿å…è³‡æ–™æ´©æ¼ã€‚")
    parser.add_argument("--test-dir", type=str, default="archive/tw_food_101/test", help="æ¸¬è©¦é›†åœ–ç‰‡è³‡æ–™å¤¾è·¯å¾‘ã€‚")
    parser.add_argument("--train-dir", type=str, default="downloads/bing_images", help="è¦æª¢æŸ¥çš„è¨“ç·´é›†åœ–ç‰‡è³‡æ–™å¤¾è·¯å¾‘ã€‚")
    parser.add_argument("--threshold", type=int, default=5, help="ç›¸ä¼¼åº¦é–¾å€¼ (æ¼¢æ˜è·é›¢)ï¼Œæ•¸å€¼è¶Šå°è¡¨ç¤ºè¦æ±‚è¶Šç›¸ä¼¼ã€‚0 ä»£è¡¨å®Œå…¨ç›¸åŒã€‚")
    parser.add_argument("--delete", action="store_true", help="å¦‚æœè¨­å®šæ­¤æ——æ¨™ï¼Œå°‡æœƒè‡ªå‹•åˆªé™¤åœ¨è¨“ç·´é›†ä¸­æ‰¾åˆ°çš„é‡è¤‡åœ–ç‰‡ã€‚")
    
    args = parser.parse_args()

    test_dir = Path(args.test_dir)
    train_dir = Path(args.train_dir)

    if not test_dir.exists() or not test_dir.is_dir():
        print(f"éŒ¯èª¤ï¼šæ¸¬è©¦ç›®éŒ„ '{test_dir}' ä¸å­˜åœ¨æˆ–ä¸æ˜¯ä¸€å€‹è³‡æ–™å¤¾ã€‚")
        return
    if not train_dir.exists() or not train_dir.is_dir():
        print(f"éŒ¯èª¤ï¼šè¨“ç·´ç›®éŒ„ '{train_dir}' ä¸å­˜åœ¨æˆ–ä¸æ˜¯ä¸€å€‹è³‡æ–™å¤¾ã€‚")
        return

    # æ­¥é©Ÿ 1: è¨ˆç®—æ¸¬è©¦é›†çš„é›œæ¹Š
    test_hashes = compute_hashes(test_dir)
    if not test_hashes:
        return

    # æ­¥é©Ÿ 2: åœ¨è¨“ç·´é›†ä¸­å°‹æ‰¾é‡è¤‡
    duplicates = find_duplicates(test_hashes, train_dir, similarity_threshold=args.threshold)

    # æ­¥é©Ÿ 3: å ±å‘Šèˆ‡è™•ç†
    if not duplicates:
        print("\nğŸ‰ æ­å–œï¼è¨“ç·´é›†ä¸­æœªç™¼ç¾èˆ‡æ¸¬è©¦é›†é‡è¤‡æˆ–é«˜åº¦ç›¸ä¼¼çš„åœ–ç‰‡ã€‚")
    else:
        print(f"\nâš ï¸ ç™¼ç¾ {len(duplicates)} å€‹é‡è¤‡/ç›¸ä¼¼çš„åœ–ç‰‡ï¼š")
        files_to_delete = []
        for item in duplicates:
            print(f"  - è¨“ç·´åœ–ç‰‡: {item['train_image']}")
            print(f"    é¡å‹: {item['type']} (å·®ç•°åº¦: {item['distance']})")
            print(f"    å°æ‡‰æ¸¬è©¦åœ–ç‰‡: {', '.join(item['test_images'])}")
            files_to_delete.append(item['train_image'])
        
        if args.delete:
            print("\n--delete æ——æ¨™å·²è¨­å®šï¼Œé–‹å§‹åˆªé™¤é‡è¤‡çš„è¨“ç·´åœ–ç‰‡...")
            deleted_count = 0
            for f_path in files_to_delete:
                try:
                    os.remove(f_path)
                    print(f"  å·²åˆªé™¤: {f_path}")
                    deleted_count += 1
                except OSError as e:
                    print(f"  åˆªé™¤å¤±æ•—: {f_path} ({e})")
            print(f"\nå…±åˆªé™¤äº† {deleted_count} å€‹æª”æ¡ˆã€‚")
        else:
            print("\næç¤ºï¼šè‹¥è¦è‡ªå‹•åˆªé™¤é€™äº›é‡è¤‡æª”æ¡ˆï¼Œè«‹åœ¨åŸ·è¡ŒæŒ‡ä»¤æ™‚åŠ ä¸Š --delete æ——æ¨™ã€‚")

if __name__ == "__main__":
    main()