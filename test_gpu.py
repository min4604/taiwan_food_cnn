#!/usr/bin/env python3
"""
GPU æ¸¬è©¦è…³æœ¬ - æª¢æŸ¥ GPU æ˜¯å¦å¯ç”¨
"""

import torch
import sys

def test_gpu():
    """æ¸¬è©¦ GPU å¯ç”¨æ€§"""
    print("=" * 60)
    print("ğŸ” GPU æª¢æ¸¬æ¸¬è©¦")
    print("=" * 60)
    
    # åŸºæœ¬è³‡è¨Š
    print(f"ğŸ Python ç‰ˆæœ¬: {sys.version}")
    print(f"ğŸ”¥ PyTorch ç‰ˆæœ¬: {torch.__version__}")
    
    # CUDA æª¢æ¸¬
    print(f"\nğŸ’» CUDA è³‡è¨Š:")
    print(f"   CUDA å¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"   CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        print(f"   cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        print(f"   GPU æ•¸é‡: {torch.cuda.device_count()}")
        
        # åˆ—å‡ºæ‰€æœ‰ GPU
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            memory_gb = props.total_memory / 1024**3
            print(f"   GPU {i}: {props.name}")
            print(f"           è¨˜æ†¶é«”: {memory_gb:.1f} GB")
            print(f"           é‹ç®—èƒ½åŠ›: {props.major}.{props.minor}")
        
        # æ¸¬è©¦åŸºæœ¬ GPU é‹ç®—
        print(f"\nğŸ§ª GPU é‹ç®—æ¸¬è©¦:")
        try:
            device = torch.device('cuda')
            x = torch.randn(1000, 1000).to(device)
            y = torch.randn(1000, 1000).to(device)
            z = torch.mm(x, y)
            print(f"   âœ… åŸºæœ¬çŸ©é™£é‹ç®—: æˆåŠŸ")
            print(f"   ğŸ“Š çµæœå½¢ç‹€: {z.shape}")
            print(f"   ğŸ’¾ GPU è¨˜æ†¶é«”ä½¿ç”¨: {torch.cuda.memory_allocated()/1024**2:.1f} MB")
        except Exception as e:
            print(f"   âŒ GPU é‹ç®—å¤±æ•—: {e}")
    else:
        print(f"   âŒ GPU ä¸å¯ç”¨")
        print(f"\nğŸ’¡ å¯èƒ½çš„è§£æ±ºæ–¹æ¡ˆ:")
        print(f"   1. ç¢ºèªæ‚¨æœ‰ NVIDIA GPU")
        print(f"   2. å®‰è£ NVIDIA é©…å‹•ç¨‹å¼")
        print(f"   3. å®‰è£ CUDA Toolkit")
        print(f"   4. é‡æ–°å®‰è£ PyTorch (CUDA ç‰ˆæœ¬)")
        print(f"      pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118")
    
    # å…¶ä»–åŠ é€Ÿé¸é …
    print(f"\nğŸš€ å…¶ä»–åŠ é€Ÿé¸é …:")
    
    # DirectML (Windows)
    try:
        import torch_directml
        if torch_directml.is_available():
            print(f"   âœ… DirectML (Windows NPU/GPU): å¯ç”¨")
            print(f"      è¨­å‚™: {torch_directml.device()}")
        else:
            print(f"   âŒ DirectML: ä¸å¯ç”¨")
    except ImportError:
        print(f"   â¡ï¸  DirectML: æœªå®‰è£")
        print(f"      å®‰è£: pip install torch-directml")
    
    # MPS (macOS)
    if hasattr(torch.backends, 'mps'):
        if torch.backends.mps.is_available():
            print(f"   âœ… Apple MPS: å¯ç”¨")
        else:
            print(f"   âŒ Apple MPS: ä¸å¯ç”¨")
    
    print("=" * 60)

if __name__ == "__main__":
    test_gpu()