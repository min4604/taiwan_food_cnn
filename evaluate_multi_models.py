#!/usr/bin/env python3
"""
å¤šæ¨¡å‹é›†æˆè©•ä¼°è…³æœ¬
è‡ªå‹•å¾ models ç›®éŒ„æŠ“å–å¤šå€‹æ¨¡å‹ï¼Œä¸¦ä½¿ç”¨é›†æˆå­¸ç¿’é€²è¡Œé æ¸¬
"""

import torch
import torch.nn as nn
from pytorch_model import get_model
from pytorch_data_loader import TaiwanFoodDataset
from torch.utils.data import DataLoader
from torchvision import transforms
from tqdm import tqdm
import os
import csv
import time
from collections import Counter
import platform
import subprocess
import numpy as np

def detect_available_devices():
    """æª¢æ¸¬å¯ç”¨çš„è¨ˆç®—è£ç½®ï¼ˆNPU/GPU/CPUï¼‰"""
    print("\nğŸ” æª¢æ¸¬å¯ç”¨çš„è¨ˆç®—è£ç½®")
    print("=" * 60)
    
    devices = []
    device_info = []
    
    # æª¢æ¸¬ AMD Ryzen AI NPU
    amd_npu_available = False
    onnx_dml_available = False
    
    try:
        # æ–¹å¼ 1: æª¢æ¸¬ DirectML
        try:
            import torch_directml
            if torch_directml.is_available():
                amd_npu_available = True
                devices.append(('dml', 'AMD NPU (DirectML)'))
                device_info.append("ğŸš€ AMD Ryzen AI NPU å¯ç”¨ (DirectML)")
                device_info.append("   æ”¯æ´: Ryzen AI 7040/8040/9HX ç³»åˆ—")
        except ImportError:
            pass
        
        # æ–¹å¼ 2: æª¢æ¸¬ ONNX Runtime - æ¨è–¦ç”¨æ–¼ NPU åŠ é€Ÿ
        try:
            import onnxruntime as ort
            providers = ort.get_available_providers()
            if 'DmlExecutionProvider' in providers:
                onnx_dml_available = True
                devices.append(('onnx_dml', 'AMD NPU (ONNX Runtime) - æ¨è–¦'))
                device_info.append("âœ… ONNX Runtime DirectML å¯ç”¨ - NPU åŠ é€Ÿæ¨è–¦")
                device_info.append("   æ”¯æ´: AMD Ryzen AI NPU ç¡¬é«”åŠ é€Ÿ")
        except ImportError:
            device_info.append("âš ï¸  ONNX Runtime æœªå®‰è£")
            device_info.append("   å»ºè­°å®‰è£: pip install onnxruntime-directml")
        
        # æ–¹å¼ 3: æª¢æ¸¬ AMD Ryzen AI è™•ç†å™¨
        if platform.system() == 'Windows':
            try:
                result = subprocess.run(['wmic', 'cpu', 'get', 'name'], 
                                      capture_output=True, text=True, timeout=5)
                if 'AMD Ryzen' in result.stdout and 'AI' in result.stdout:
                    device_info.append("ğŸ’» æª¢æ¸¬åˆ° AMD Ryzen AI è™•ç†å™¨")
                    if not (amd_npu_available or onnx_dml_available):
                        device_info.append("   âš ï¸  NPU å¯èƒ½å¯ç”¨ä½†æœªå•Ÿç”¨")
                        device_info.append("   å»ºè­°åŸ·è¡Œ: install_npu.bat")
            except:
                pass
    except Exception as e:
        pass
    
    # æª¢æ¸¬ Intel NPU (DirectML)
    try:
        import torch_directml
        if torch_directml.is_available() and not amd_npu_available:
            devices.append(('dml', 'Intel NPU (DirectML)'))
            device_info.append("ğŸš€ Intel NPU å¯ç”¨ (DirectML)")
    except ImportError:
        pass
    
    # æª¢æ¸¬å‚³çµ± NPU æ”¯æ´ï¼ˆè¯ç‚ºæ˜‡é¨°ç­‰ï¼‰
    try:
        if hasattr(torch, 'npu') and torch.npu.is_available():
            npu_count = torch.npu.device_count()
            devices.append(('npu', f'è¯ç‚ºæ˜‡é¨° NPU ({npu_count}å€‹)'))
            device_info.append(f"ğŸš€ è¯ç‚ºæ˜‡é¨° NPU å¯ç”¨: {npu_count} å€‹è£ç½®")
            for i in range(npu_count):
                try:
                    npu_name = torch.npu.get_device_name(i)
                    device_info.append(f"   NPU {i}: {npu_name}")
                except:
                    device_info.append(f"   NPU {i}: æœªçŸ¥å‹è™Ÿ")
    except Exception as e:
        pass
    
    # æª¢æ¸¬ Apple MPS (Neural Engine)
    try:
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            devices.append(('mps', 'Apple Neural Engine (MPS)'))
            device_info.append("ğŸ Apple Neural Engine (MPS) å¯ç”¨")
    except:
        pass
    
    # æª¢æ¸¬ CUDA GPU
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            devices.append((f'cuda:{i}', f'GPU {i}: {gpu_name} ({memory:.1f}GB)'))
            device_info.append(f"âœ… GPU {i}: {gpu_name} ({memory:.1f} GB)")
    
    # CPU å§‹çµ‚å¯ç”¨
    devices.append(('cpu', 'CPU'))
    device_info.append("ğŸ’» CPU å¯ç”¨")
    
    # é¡¯ç¤ºæª¢æ¸¬çµæœ
    for info in device_info:
        print(info)
    
    if not devices[:-1]:  # é™¤äº†CPUä»¥å¤–æ²’æœ‰å…¶ä»–è¨­å‚™
        print("\nâš ï¸  æ²’æœ‰æª¢æ¸¬åˆ° NPU æˆ– GPUï¼Œå°‡ä½¿ç”¨ CPU")
        print("ğŸ’¡ æç¤º: å¯ä»¥å®‰è£ NPU æ”¯æ´ä»¥åŠ é€Ÿæ¨ç†")
    
    print("=" * 60)
    return devices

def get_all_models_from_directory(models_dir='models', max_models=None, min_models=2):
    """å¾ç›®éŒ„ä¸­ç²å–æ‰€æœ‰æ¨¡å‹æª”æ¡ˆ
    
    Args:
        models_dir: æ¨¡å‹ç›®éŒ„è·¯å¾‘
        max_models: æœ€å¤šä½¿ç”¨çš„æ¨¡å‹æ•¸é‡ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰æ¨¡å‹
        min_models: æœ€å°‘éœ€è¦çš„æ¨¡å‹æ•¸é‡
    
    Returns:
        list: æ¨¡å‹æª”æ¡ˆè·¯å¾‘åˆ—è¡¨
    """
    if not os.path.exists(models_dir):
        print(f"âŒ æ¨¡å‹ç›®éŒ„ä¸å­˜åœ¨: {models_dir}")
        return []
    
    model_files = [f for f in os.listdir(models_dir) if f.endswith('.pth')]
    
    if not model_files:
        print(f"âŒ åœ¨ {models_dir} ä¸­æ²’æœ‰æ‰¾åˆ°ä»»ä½• .pth æ¨¡å‹æª”æ¡ˆ")
        return []
    
    if len(model_files) < min_models:
        print(f"âš ï¸  åªæ‰¾åˆ° {len(model_files)} å€‹æ¨¡å‹ï¼Œå°‘æ–¼æœ€å°‘éœ€æ±‚ {min_models} å€‹")
        print(f"   å»ºè­°è‡³å°‘è¨“ç·´ {min_models} å€‹ä¸åŒçš„æ¨¡å‹ä»¥ç²å¾—æ›´å¥½çš„é›†æˆæ•ˆæœ")
    
    # æŒ‰ä¿®æ”¹æ™‚é–“æ’åºï¼ˆæœ€æ–°çš„å„ªå…ˆï¼‰
    model_files = sorted(model_files, 
                        key=lambda x: os.path.getmtime(os.path.join(models_dir, x)), 
                        reverse=True)
    
    if max_models:
        model_files = model_files[:max_models]
    
    model_paths = [os.path.join(models_dir, f) for f in model_files]
    
    print(f"\nğŸ“ åœ¨ {models_dir} ä¸­æ‰¾åˆ° {len(model_paths)} å€‹æ¨¡å‹:")
    print("=" * 60)
    for i, (path, filename) in enumerate(zip(model_paths, model_files), 1):
        file_time = os.path.getmtime(path)
        time_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(file_time))
        file_size = os.path.getsize(path) / 1024 / 1024  # MB
        print(f"   {i}. {filename}")
        print(f"      ä¿®æ”¹æ™‚é–“: {time_str}, å¤§å°: {file_size:.1f} MB")
    print("=" * 60)
    
    return model_paths

def convert_model_to_onnx(model, model_name, input_shape=(1, 3, 224, 224), output_path=None):
    """å°‡ PyTorch æ¨¡å‹è½‰æ›ç‚º ONNX æ ¼å¼
    
    Args:
        model: PyTorch æ¨¡å‹
        model_name: æ¨¡å‹åç¨±
        input_shape: è¼¸å…¥å¼µé‡å½¢ç‹€
        output_path: ONNX æª”æ¡ˆè¼¸å‡ºè·¯å¾‘
    
    Returns:
        onnx_path: ONNX æª”æ¡ˆè·¯å¾‘
    """
    if output_path is None:
        output_path = f"temp_onnx_{model_name}.onnx"
    
    try:
        # ç¢ºä¿æ¨¡å‹åœ¨ CPU ä¸Š
        model = model.cpu()
        model.eval()
        
        # å‰µå»ºè™›æ“¬è¼¸å…¥
        dummy_input = torch.randn(*input_shape)
        
        # å°å‡ºç‚º ONNX
        torch.onnx.export(
            model,
            dummy_input,
            output_path,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            },
            opset_version=13,
            do_constant_folding=True
        )
        
        return output_path
    
    except Exception as e:
        print(f"   âš ï¸  ONNX è½‰æ›å¤±æ•—: {e}")
        return None

def create_onnx_session(onnx_path, use_dml=True):
    """å‰µå»º ONNX Runtime æ¨ç†æœƒè©±
    
    Args:
        onnx_path: ONNX æ¨¡å‹è·¯å¾‘
        use_dml: æ˜¯å¦ä½¿ç”¨ DirectML åŸ·è¡Œæä¾›è€…
    
    Returns:
        session: ONNX Runtime æ¨ç†æœƒè©±
    """
    try:
        import onnxruntime as ort
        
        # è¨­ç½®åŸ·è¡Œæä¾›è€…
        providers = []
        if use_dml:
            providers.append('DmlExecutionProvider')
        providers.append('CPUExecutionProvider')
        
        # å‰µå»ºæœƒè©±é¸é …
        sess_options = ort.SessionOptions()
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        
        # å‰µå»ºæ¨ç†æœƒè©±
        session = ort.InferenceSession(
            onnx_path,
            sess_options=sess_options,
            providers=providers
        )
        
        return session
    
    except Exception as e:
        print(f"   âš ï¸  å‰µå»º ONNX æœƒè©±å¤±æ•—: {e}")
        return None

def ensemble_predict_onnx(onnx_sessions, images, strategy='weighted_average'):
    """ä½¿ç”¨ ONNX Runtime é€²è¡Œå¤šæ¨¡å‹é›†æˆé æ¸¬ï¼ˆNPU åŠ é€Ÿï¼‰
    
    Args:
        onnx_sessions: ONNX æ¨ç†æœƒè©±åˆ—è¡¨ [(session, weight, name), ...]
        images: è¼¸å…¥åœ–ç‰‡å¼µé‡ (PyTorch)
        strategy: é›†æˆç­–ç•¥
    
    Returns:
        predictions: é æ¸¬é¡åˆ¥
        confidences: é æ¸¬ä¿¡å¿ƒåº¦
        details: è©³ç´°è³‡è¨Š
    """
    all_outputs = []
    all_predictions = []
    all_confidences = []
    successful_models = []
    
    # è½‰æ› PyTorch å¼µé‡ç‚º NumPy
    images_np = images.cpu().numpy() if images.is_cuda else images.numpy()
    
    # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é æ¸¬
    for session, weight, name in onnx_sessions:
        try:
            # ONNX Runtime æ¨ç†ï¼ˆä½¿ç”¨ NPU åŠ é€Ÿï¼‰
            input_name = session.get_inputs()[0].name
            outputs = session.run(None, {input_name: images_np})[0]
            
            # è½‰æ›å› PyTorch å¼µé‡é€²è¡Œå¾Œè™•ç†
            outputs_torch = torch.from_numpy(outputs).float()
            probs = torch.softmax(outputs_torch, dim=1)
            max_probs, predicted = probs.max(1)
            
            all_outputs.append(outputs_torch * weight)
            all_predictions.append(predicted)
            all_confidences.append(max_probs)
            successful_models.append(name)
            
        except Exception as e:
            # éœé»˜è·³éå¤±æ•—çš„æ¨¡å‹
            continue
    
    # ç¢ºä¿è‡³å°‘æœ‰ä¸€å€‹æ¨¡å‹æˆåŠŸ
    if not all_outputs:
        raise RuntimeError("æ‰€æœ‰æ¨¡å‹éƒ½ç„¡æ³•é€²è¡Œé æ¸¬")
    
    # å–®æ¨¡å‹æƒ…æ³
    if len(all_outputs) == 1:
        probs = torch.softmax(all_outputs[0], dim=1)
        max_probs, predictions = probs.max(1)
        details = {
            'individual_predictions': [all_predictions[0].numpy()],
            'individual_confidences': [all_confidences[0].numpy()],
            'model_names': successful_models
        }
        return predictions, max_probs, details
    
    # é›†æˆç­–ç•¥
    if strategy == 'weighted_average':
        ensemble_outputs = all_outputs[0]
        for output in all_outputs[1:]:
            ensemble_outputs = ensemble_outputs + output
        probs = torch.softmax(ensemble_outputs, dim=1)
        max_probs, predictions = probs.max(1)
        
    elif strategy == 'voting':
        predictions_stack = torch.stack(all_predictions)
        predictions = torch.mode(predictions_stack, dim=0)[0]
        max_probs = torch.stack(all_confidences).mean(0)
        
    elif strategy == 'max_confidence':
        all_confidences_stack = torch.stack(all_confidences)
        max_conf_indices = all_confidences_stack.argmax(0)
        
        predictions = torch.zeros_like(all_predictions[0])
        max_probs = torch.zeros_like(all_confidences[0])
        
        for i in range(len(predictions)):
            best_model_idx = max_conf_indices[i]
            predictions[i] = all_predictions[best_model_idx][i]
            max_probs[i] = all_confidences[best_model_idx][i]
    
    # è¿”å›è©³ç´°è³‡è¨Š
    details = {
        'individual_predictions': [p.numpy() for p in all_predictions],
        'individual_confidences': [c.numpy() for c in all_confidences],
        'model_names': successful_models
    }
    
    return predictions, max_probs, details

def detect_model_architecture(model_path):
    """å¾æ¨¡å‹æª”æ¡ˆåç¨±ä¸­æª¢æ¸¬æ¨¡å‹æ¶æ§‹"""
    filename = os.path.basename(model_path).lower()
    
    if 'efficientnet_b3' in filename or 'efficientnet' in filename:
        return 'efficientnet_b3'
    elif 'convnext_tiny' in filename or 'convnext' in filename:
        return 'convnext_tiny'
    elif 'regnet_y' in filename or 'regnet' in filename:
        return 'regnet_y'
    elif 'vit' in filename or 'vision_transformer' in filename:
        return 'vit'
    elif 'resnet50' in filename or 'resnet' in filename:
        return 'resnet50'
    else:
        # å˜—è©¦å¾æ¨¡å‹å…§å®¹æª¢æ¸¬
        try:
            state_dict = torch.load(model_path, map_location='cpu')
            keys = list(state_dict.keys())
            
            if any('features' in key for key in keys):
                if any('block' in key for key in keys):
                    return 'efficientnet_b3'
                elif any('stages' in key for key in keys):
                    return 'convnext_tiny'
            elif any('layer' in key for key in keys):
                return 'resnet50'
            elif any('blocks' in key for key in keys):
                return 'vit'
        except:
            pass
        
        print(f"âš ï¸  ç„¡æ³•å¾æª”æ¡ˆåç¨±æª¢æ¸¬æ¨¡å‹æ¶æ§‹: {filename}")
        print("   ä½¿ç”¨é è¨­æ¶æ§‹: ResNet50")
        return 'resnet50'

def ensemble_predict(models_info, images, device, strategy='weighted_average'):
    """å¤šæ¨¡å‹é›†æˆé æ¸¬
    
    Args:
        models_info: æ¨¡å‹è³‡è¨Šåˆ—è¡¨ [(model, weight, name), ...]
        images: è¼¸å…¥åœ–ç‰‡å¼µé‡
        device: è¨ˆç®—è¨­å‚™
        strategy: é›†æˆç­–ç•¥ ('weighted_average', 'voting', 'max_confidence')
    
    Returns:
        predictions: é æ¸¬é¡åˆ¥
        confidences: é æ¸¬ä¿¡å¿ƒåº¦
        details: æ¯å€‹æ¨¡å‹çš„é æ¸¬è©³æƒ…
    """
    all_outputs = []
    all_predictions = []
    all_confidences = []
    all_probs = []
    successful_models = []  # è¨˜éŒ„æˆåŠŸçš„æ¨¡å‹
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºDirectMLè¨­å‚™
    is_directml = 'privateuseone' in str(device).lower() or 'dml' in str(device).lower()
    
    # å¦‚æœæ˜¯DirectMLï¼Œå°‡åœ–ç‰‡ç§»åˆ°CPUé€²è¡Œæ¨ç†ï¼ˆé¿å…DirectMLå¼µé‡æ“ä½œå•é¡Œï¼‰
    if is_directml:
        images_for_inference = images.cpu()
    else:
        images_for_inference = images
    
    # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é æ¸¬
    for model, weight, name in models_info:
        with torch.no_grad():
            try:
                # DirectML: åœ¨CPUä¸Šæ¨ç†
                if is_directml:
                    # ç¢ºä¿æ¨¡å‹åœ¨CPUä¸Š
                    model_cpu = model.cpu() if hasattr(model, 'cpu') else model
                    outputs = model_cpu(images_for_inference)
                else:
                    outputs = model(images_for_inference)
                
                probs = torch.softmax(outputs, dim=1)
                max_probs, predicted = probs.max(1)
                
                all_outputs.append(outputs * weight)
                all_predictions.append(predicted)
                all_confidences.append(max_probs)
                all_probs.append(probs)
                successful_models.append(name)
                
            except Exception as e:
                # éœé»˜è·³éå¤±æ•—çš„æ¨¡å‹
                continue
    
    # ç¢ºä¿è‡³å°‘æœ‰ä¸€å€‹æ¨¡å‹æˆåŠŸé æ¸¬
    if not all_outputs:
        raise RuntimeError("æ‰€æœ‰æ¨¡å‹éƒ½ç„¡æ³•é€²è¡Œé æ¸¬")
    
    # å¦‚æœåªæœ‰ä¸€å€‹æ¨¡å‹æˆåŠŸï¼Œç›´æ¥ä½¿ç”¨å®ƒçš„çµæœ
    if len(all_outputs) == 1:
        probs = torch.softmax(all_outputs[0], dim=1)
        max_probs, predictions = probs.max(1)
        details = {
            'individual_predictions': [all_predictions[0].cpu().numpy() if all_predictions[0].is_cuda else all_predictions[0].numpy()],
            'individual_confidences': [all_confidences[0].cpu().numpy() if all_confidences[0].is_cuda else all_confidences[0].numpy()],
            'model_names': successful_models
        }
        return predictions, max_probs, details
    
    # æ‰€æœ‰é›†æˆè¨ˆç®—åœ¨CPUä¸Šé€²è¡Œï¼ˆå·²ç¶“æ˜¯CPUå¼µé‡ï¼‰
    try:
        if strategy == 'weighted_average':
            # ç­–ç•¥1: åŠ æ¬Šå¹³å‡æ‰€æœ‰æ¨¡å‹çš„è¼¸å‡º
            ensemble_outputs = all_outputs[0]
            for output in all_outputs[1:]:
                ensemble_outputs = ensemble_outputs + output
            probs = torch.softmax(ensemble_outputs, dim=1)
            max_probs, predictions = probs.max(1)
            
        elif strategy == 'voting':
            # ç­–ç•¥2: æŠ•ç¥¨æ³•ï¼ˆæ¯å€‹æ¨¡å‹ä¸€ç¥¨ï¼‰
            predictions_stack = torch.stack(all_predictions)
            predictions = torch.mode(predictions_stack, dim=0)[0]
            max_probs = torch.stack(all_confidences).mean(0)
            
        elif strategy == 'max_confidence':
            # ç­–ç•¥3: é¸æ“‡æœ€é«˜ä¿¡å¿ƒåº¦çš„é æ¸¬
            all_confidences_stack = torch.stack(all_confidences)
            max_conf_indices = all_confidences_stack.argmax(0)
            
            predictions = torch.zeros_like(all_predictions[0])
            max_probs = torch.zeros_like(all_confidences[0])
            
            for i in range(len(predictions)):
                best_model_idx = max_conf_indices[i]
                predictions[i] = all_predictions[best_model_idx][i]
                max_probs[i] = all_confidences[best_model_idx][i]
    
    except Exception as e:
        print(f"\nâŒ é›†æˆç­–ç•¥ '{strategy}' åŸ·è¡Œå¤±æ•—: {e}")
        print(f"   æˆåŠŸçš„æ¨¡å‹æ•¸: {len(successful_models)}")
        print(f"   è¼¸å‡ºå¼µé‡æ•¸: {len(all_outputs)}")
        if all_outputs:
            print(f"   è¼¸å‡ºå¼µé‡å½¢ç‹€: {all_outputs[0].shape}")
        raise
    
    # è¿”å›è©³ç´°è³‡è¨Š
    details = {
        'individual_predictions': [p.cpu().numpy() if p.is_cuda else p.numpy() for p in all_predictions],
        'individual_confidences': [c.cpu().numpy() if c.is_cuda else c.numpy() for c in all_confidences],
        'model_names': successful_models
    }
    
    return predictions, max_probs, details

def evaluate_multi_models(model_paths, test_csv, test_img_dir, num_classes=101, 
                          batch_size=32, img_size=224, device_str='cpu', 
                          strategy='weighted_average', use_onnx_npu=False):
    """ä½¿ç”¨å¤šå€‹æ¨¡å‹é€²è¡Œé›†æˆè©•ä¼°
    
    Args:
        model_paths: æ¨¡å‹æª”æ¡ˆè·¯å¾‘åˆ—è¡¨
        test_csv: æ¸¬è©¦é›†CSVæª”æ¡ˆ
        test_img_dir: æ¸¬è©¦é›†åœ–ç‰‡ç›®éŒ„
        num_classes: é¡åˆ¥æ•¸é‡
        batch_size: æ‰¹æ¬¡å¤§å°
        img_size: åœ–ç‰‡å¤§å°
        device_str: è¨ˆç®—è¨­å‚™
        strategy: é›†æˆç­–ç•¥
        use_onnx_npu: æ˜¯å¦ä½¿ç”¨ ONNX Runtime DirectML é€²è¡Œ NPU åŠ é€Ÿ
    """
    print(f"\nğŸ¯ å¤šæ¨¡å‹é›†æˆè©•ä¼°æ¨¡å¼")
    print(f"ğŸ“Š ä½¿ç”¨ {len(model_paths)} å€‹æ¨¡å‹é€²è¡Œé›†æˆé æ¸¬")
    print(f"ğŸ² é›†æˆç­–ç•¥: {strategy}")
    if use_onnx_npu:
        print(f"ğŸš€ NPU åŠ é€Ÿ: ONNX Runtime DirectML")
    print("=" * 60)
    
    # è¨­å®šè£ç½®
    try:
        # è™•ç†ç‰¹æ®Šè¨­å‚™é¡å‹
        if isinstance(device_str, str):
            if device_str.startswith('cuda'):
                device = torch.device(device_str)
            elif device_str == 'mps':
                device = torch.device('mps')
            elif device_str.startswith('npu'):
                device = torch.device(device_str)
            elif device_str == 'cpu':
                device = torch.device('cpu')
            else:
                # DirectML æˆ–å…¶ä»–ç‰¹æ®Šè¨­å‚™
                try:
                    import torch_directml
                    device = torch_directml.device()
                except:
                    device = torch.device('cpu')
        else:
            # å¦‚æœå·²ç¶“æ˜¯è¨­å‚™ç‰©ä»¶
            device = device_str
    except Exception as e:
        print(f"âš ï¸  è¨­å‚™åˆå§‹åŒ–å¤±æ•—: {e}ï¼Œä½¿ç”¨ CPU")
        device = torch.device('cpu')
    
    print(f"ğŸ’» ä½¿ç”¨è£ç½®: {device}")
    
    # é¡¯ç¤ºè¨­å‚™è©³ç´°è³‡è¨Š
    if str(device).startswith('cuda'):
        gpu_id = int(str(device).split(':')[1]) if ':' in str(device) else 0
        print(f"ğŸš€ GPU: {torch.cuda.get_device_name(gpu_id)}")
        print(f"ğŸ’¾ GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(gpu_id).total_memory / 1024**3:.1f} GB")
    elif 'dml' in str(device).lower() or 'privateuseone' in str(device).lower():
        print(f"ğŸš€ ä½¿ç”¨ NPU åŠ é€Ÿ (DirectML)")
        print(f"ğŸ’¡ é©åˆ AMD Ryzen AI / Intel Arc ç³»åˆ—è™•ç†å™¨")
        print(f"ğŸ“ æ³¨æ„: ç”±æ–¼ DirectML å¼µé‡å…¼å®¹æ€§å•é¡Œï¼Œæ¨¡å‹æ¨ç†åœ¨ CPU ä¸Šé€²è¡Œ")
        print(f"   æœªä¾†ç‰ˆæœ¬å°‡å„ªåŒ–ä»¥å……åˆ†åˆ©ç”¨ NPU ç¡¬é«”åŠ é€Ÿ")
    elif str(device) == 'mps':
        print(f"ğŸ ä½¿ç”¨ Apple Neural Engine (MPS)")
    elif str(device).startswith('npu'):
        print(f"ğŸš€ ä½¿ç”¨è¯ç‚ºæ˜‡é¨° NPU")
    
    # è¼‰å…¥æ‰€æœ‰æ¨¡å‹
    models_info = []
    onnx_sessions = []  # ç”¨æ–¼ONNX Runtimeæ¨¡å¼
    
    print(f"\nğŸ“¦ é–‹å§‹è¼‰å…¥ {len(model_paths)} å€‹æ¨¡å‹...")
    if use_onnx_npu:
        print("ğŸš€ æ¨¡å¼: ONNX Runtime DirectML (NPUåŠ é€Ÿ)")
    print("=" * 60)
    
    for i, model_path in enumerate(model_paths, 1):
        model_name = os.path.basename(model_path)
        print(f"\nğŸ“¦ [{i}/{len(model_paths)}] è¼‰å…¥: {model_name}")
        
        # æª¢æ¸¬æ¨¡å‹æ¶æ§‹
        architecture = detect_model_architecture(model_path)
        print(f"   ğŸ—ï¸  æ¶æ§‹: {architecture}")
        
        # å»ºç«‹æ¨¡å‹
        try:
            model = get_model(architecture, num_classes=num_classes, dropout_rate=0.3)
            
            # è¼‰å…¥æ¬Šé‡åˆ°CPU
            state_dict = torch.load(model_path, map_location='cpu', weights_only=False)
            model.load_state_dict(state_dict)
            model = model.cpu()
            model.eval()
            
            # è¨ˆç®—æ¨¡å‹æ¬Šé‡
            weight = 1.0 / len(model_paths)
            
            # å¦‚æœä½¿ç”¨ONNX Runtime NPUåŠ é€Ÿ
            if use_onnx_npu:
                try:
                    # è½‰æ›ç‚ºONNX
                    print(f"   ğŸ”„ è½‰æ›ç‚º ONNX æ ¼å¼...")
                    onnx_path = convert_model_to_onnx(
                        model, 
                        model_name.replace('.pth', ''),
                        input_shape=(batch_size, 3, img_size, img_size)
                    )
                    
                    if onnx_path:
                        # å‰µå»º ONNX Runtime æœƒè©±
                        session = create_onnx_session(onnx_path, use_dml=True)
                        if session:
                            # æª¢æŸ¥åŸ·è¡Œæä¾›è€…
                            providers = session.get_providers()
                            if 'DmlExecutionProvider' in providers:
                                print(f"   ï¿½ ONNX Runtime å·²å•Ÿç”¨ DirectML (NPUåŠ é€Ÿ)")
                            else:
                                print(f"   ğŸ’» ONNX Runtime ä½¿ç”¨ CPU")
                            
                            onnx_sessions.append((session, weight, model_name))
                            print(f"   âœ… ONNX è½‰æ›æˆåŠŸ (æ¬Šé‡: {weight:.4f})")
                        else:
                            print(f"   âš ï¸  ONNX æœƒè©±å‰µå»ºå¤±æ•—ï¼Œè·³éæ­¤æ¨¡å‹")
                    else:
                        print(f"   âš ï¸  ONNX è½‰æ›å¤±æ•—ï¼Œè·³éæ­¤æ¨¡å‹")
                        
                except Exception as e:
                    print(f"   âŒ ONNX è™•ç†å¤±æ•—: {e}")
                    print(f"   âš ï¸  è·³éæ­¤æ¨¡å‹")
                    continue
            else:
                # PyTorch æ¨¡å¼
                is_directml_device = 'privateuseone' in str(device).lower() or 'dml' in str(device).lower()
                
                if is_directml_device:
                    # DirectML: æ¨¡å‹ä¿æŒåœ¨CPUä¸Š
                    print(f"   ğŸ“ æ¨¡å‹è¼‰å…¥åˆ° CPU (DirectML æ¨¡å¼)")
                else:
                    # å…¶ä»–è¨­å‚™: ç§»å‹•åˆ°æŒ‡å®šè¨­å‚™
                    model = model.to(device)
                
                models_info.append((model, weight, model_name))
                print(f"   âœ… è¼‰å…¥æˆåŠŸ (æ¬Šé‡: {weight:.4f})")
            
        except Exception as e:
            print(f"   âŒ è¼‰å…¥å¤±æ•—: {e}")
            print(f"   âš ï¸  è·³éæ­¤æ¨¡å‹")
            continue
    
    # æª¢æŸ¥æ˜¯å¦æœ‰æˆåŠŸè¼‰å…¥çš„æ¨¡å‹
    if use_onnx_npu:
        if not onnx_sessions:
            print("\nâŒ æ²’æœ‰æˆåŠŸè¼‰å…¥ä»»ä½• ONNX æ¨¡å‹")
            return
        if len(onnx_sessions) < 2:
            print(f"\nâš ï¸  åªæˆåŠŸè¼‰å…¥ {len(onnx_sessions)} å€‹æ¨¡å‹")
            print("   é›†æˆæ•ˆæœå¯èƒ½æœ‰é™ï¼Œå»ºè­°ä½¿ç”¨è‡³å°‘2å€‹æ¨¡å‹")
        print(f"\nâœ… æˆåŠŸè¼‰å…¥ {len(onnx_sessions)} å€‹ ONNX æ¨¡å‹ï¼ˆNPUåŠ é€Ÿï¼‰")
    else:
        if not models_info:
            print("\nâŒ æ²’æœ‰æˆåŠŸè¼‰å…¥ä»»ä½•æ¨¡å‹")
            return
        if len(models_info) < 2:
            print(f"\nâš ï¸  åªæˆåŠŸè¼‰å…¥ {len(models_info)} å€‹æ¨¡å‹")
            print("   é›†æˆæ•ˆæœå¯èƒ½æœ‰é™ï¼Œå»ºè­°ä½¿ç”¨è‡³å°‘2å€‹æ¨¡å‹")
        print(f"\nâœ… æˆåŠŸè¼‰å…¥ {len(models_info)} å€‹æ¨¡å‹")
    print("=" * 60)
    
    # è³‡æ–™è½‰æ›
    test_transform = transforms.Compose([
        transforms.Resize((img_size, img_size)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    # å»ºç«‹æ¸¬è©¦é›† DataLoader
    print("\nğŸ“Š è¼‰å…¥æ¸¬è©¦é›†è³‡æ–™...")
    test_dataset = TaiwanFoodDataset(test_csv, test_img_dir, test_transform, is_test=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
    
    print(f"   æ¸¬è©¦é›†å¤§å°: {len(test_dataset)} å¼µåœ–ç‰‡")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    print("=" * 60)
    
    # åŸ·è¡Œé›†æˆé æ¸¬
    print(f"\nğŸ” é–‹å§‹å¤šæ¨¡å‹é›†æˆè©•ä¼°...")
    print(f"   ç­–ç•¥: {strategy}")
    
    all_predictions = []
    all_confidences = []
    all_details = []
    
    start_time = time.time()
    
    # è¿½è¹¤å¤±æ•—çš„æ¨¡å‹ï¼ˆåªåœ¨ç¬¬ä¸€æ‰¹é¡¯ç¤ºï¼‰
    first_batch_failed_models = set()
    first_batch_processed = False
    
    with torch.no_grad():
        test_pbar = tqdm(test_loader, desc="é›†æˆé æ¸¬ä¸­", ncols=100)
        for batch_idx, (images, _) in enumerate(test_pbar):
            
            # æ ¹æ“šæ¨¡å¼é¸æ“‡é æ¸¬æ–¹æ³•
            try:
                if use_onnx_npu:
                    # ONNX Runtime DirectML æ¨¡å¼ï¼ˆNPUåŠ é€Ÿï¼‰
                    predictions, confidences, details = ensemble_predict_onnx(
                        onnx_sessions, images, strategy
                    )
                    
                    # ç¬¬ä¸€æ‰¹æ™‚é¡¯ç¤ºä¿¡æ¯
                    if not first_batch_processed:
                        print(f"\nâœ… ä½¿ç”¨ {len(details['model_names'])} å€‹æ¨¡å‹é€²è¡Œ NPU åŠ é€Ÿæ¨ç†\n")
                        first_batch_processed = True
                        
                else:
                    # PyTorch æ¨¡å¼
                    is_directml = 'privateuseone' in str(device).lower() or 'dml' in str(device).lower()
                    
                    if not is_directml:
                        images = images.to(device)
                    
                    predictions, confidences, details = ensemble_predict(
                        models_info, images, device, strategy
                    )
                    
                    # ç¬¬ä¸€æ‰¹æ™‚æª¢æŸ¥å“ªäº›æ¨¡å‹å¤±æ•—äº†
                    if not first_batch_processed:
                        all_model_names = [name for _, _, name in models_info]
                        successful_models = details['model_names']
                        failed_models = set(all_model_names) - set(successful_models)
                        
                        if failed_models:
                            print(f"\nâš ï¸  ä»¥ä¸‹æ¨¡å‹ç„¡æ³•åœ¨DirectMLä¸Šé‹è¡Œï¼Œå·²è·³é:")
                            for model_name in failed_models:
                                print(f"   - {model_name}")
                            print(f"\nâœ… ä½¿ç”¨ {len(successful_models)} å€‹æ¨¡å‹ç¹¼çºŒé›†æˆé æ¸¬\n")
                        
                        first_batch_processed = True
                
            except Exception as e:
                print(f"\nâŒ é›†æˆé æ¸¬å¤±æ•—: {type(e).__name__}")
                print(f"   éŒ¯èª¤ä¿¡æ¯: {str(e)}")
                import traceback
                traceback.print_exc()
                raise
            
            all_predictions.extend(predictions.cpu().numpy())
            all_confidences.extend(confidences.cpu().numpy())
            all_details.append(details)
            
            # æ›´æ–°é€²åº¦æ¢
            avg_conf = sum(all_confidences) / len(all_confidences)
            test_pbar.set_postfix({
                'å·²è™•ç†': len(all_predictions),
                'å¹³å‡ä¿¡å¿ƒ': f'{avg_conf:.3f}'
            })
    
    elapsed_time = time.time() - start_time
    
    # å„²å­˜é æ¸¬çµæœ
    results_file = f"test_predictions_ensemble_{strategy}.csv"
    print(f"\nğŸ’¾ å„²å­˜é æ¸¬çµæœ...")
    
    with open(results_file, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Id', 'Category', 'Confidence'])
        for i, (pred, conf) in enumerate(zip(all_predictions, all_confidences)):
            writer.writerow([i, int(pred), f'{conf:.6f}'])
    
    # çµ±è¨ˆçµæœ
    print("\n" + "=" * 60)
    print("ğŸ“Š é›†æˆè©•ä¼°çµæœçµ±è¨ˆ")
    print("=" * 60)
    print(f"âœ… è©•ä¼°å®Œæˆï¼")
    print(f"ğŸ“ çµæœæª”æ¡ˆ: {results_file}")
    print(f"ğŸ“Š è™•ç†åœ–ç‰‡: {len(all_predictions)} å¼µ")
    print(f"â±ï¸  ç¸½è€—æ™‚: {elapsed_time:.2f} ç§’")
    print(f"âš¡ é€Ÿåº¦: {len(all_predictions)/elapsed_time:.2f} å¼µ/ç§’")
    
    avg_confidence = sum(all_confidences) / len(all_confidences)
    print(f"\nğŸ“ˆ å¹³å‡ä¿¡å¿ƒåº¦: {avg_confidence:.4f}")
    print(f"ğŸ“ˆ æœ€é«˜ä¿¡å¿ƒåº¦: {max(all_confidences):.4f}")
    print(f"ğŸ“‰ æœ€ä½ä¿¡å¿ƒåº¦: {min(all_confidences):.4f}")
    
    # é¡¯ç¤ºé æ¸¬é¡åˆ¥åˆ†ä½ˆ
    pred_counts = Counter(all_predictions)
    print(f"\nğŸ“Š é æ¸¬é¡åˆ¥åˆ†ä½ˆ (å‰10å):")
    for class_id, count in pred_counts.most_common(10):
        percentage = count / len(all_predictions) * 100
        print(f"   é¡åˆ¥ {class_id:3d}: {count:4d} å¼µ ({percentage:5.2f}%)")
    
    # é¡¯ç¤ºä¿¡å¿ƒåº¦åˆ†ä½ˆ
    conf_ranges = [(0, 0.5), (0.5, 0.7), (0.7, 0.85), (0.85, 0.95), (0.95, 1.0)]
    print(f"\nğŸ“Š ä¿¡å¿ƒåº¦åˆ†ä½ˆ:")
    for low, high in conf_ranges:
        count = sum(1 for c in all_confidences if low <= c < high)
        percentage = count / len(all_confidences) * 100
        print(f"   {low:.2f} ~ {high:.2f}: {count:4d} å¼µ ({percentage:5.2f}%)")
    
    # æ¨™è¨˜ä½ä¿¡å¿ƒåº¦é æ¸¬
    low_conf_threshold = 0.5
    low_conf_predictions = [(i, pred, conf) for i, (pred, conf) in enumerate(zip(all_predictions, all_confidences)) 
                            if conf < low_conf_threshold]
    
    if low_conf_predictions:
        print(f"\nâš ï¸  ä¿¡å¿ƒåº¦ä½æ–¼ {low_conf_threshold} çš„é æ¸¬: {len(low_conf_predictions)} å¼µ ({len(low_conf_predictions)/len(all_predictions)*100:.1f}%)")
        print(f"   å»ºè­°äººå·¥æª¢æŸ¥é€™äº›é æ¸¬")
        
        # å„²å­˜ä½ä¿¡å¿ƒåº¦é æ¸¬æ¸…å–®
        low_conf_file = f"low_confidence_predictions_{strategy}.csv"
        with open(low_conf_file, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['ImageId', 'PredictedClass', 'Confidence'])
            for img_id, pred, conf in low_conf_predictions[:20]:  # åªé¡¯ç¤ºå‰20å€‹
                writer.writerow([img_id, int(pred), f'{conf:.6f}'])
        print(f"   è©³ç´°æ¸…å–®å·²å„²å­˜è‡³: {low_conf_file}")
    else:
        print(f"\nâœ… å¤ªæ£’äº†ï¼æ‰€æœ‰é æ¸¬çš„ä¿¡å¿ƒåº¦éƒ½é«˜æ–¼ {low_conf_threshold}")
    
    print("=" * 60)
    
    return all_predictions, all_confidences

def main():
    """ä¸»ç¨‹å¼"""
    print("=" * 60)
    print("ğŸ¯ å¤šæ¨¡å‹é›†æˆè©•ä¼°å·¥å…·")
    print("=" * 60)
    
    # ç²å–æ‰€æœ‰æ¨¡å‹
    model_paths = get_all_models_from_directory(models_dir='models', max_models=None, min_models=2)
    
    if len(model_paths) < 2:
        print("\nâŒ éœ€è¦è‡³å°‘2å€‹æ¨¡å‹æ‰èƒ½é€²è¡Œé›†æˆè©•ä¼°")
        print("ğŸ’¡ è«‹å…ˆè¨“ç·´å¤šå€‹ä¸åŒçš„æ¨¡å‹")
        return
    
    # é¸æ“‡é›†æˆç­–ç•¥
    print("\nğŸ² è«‹é¸æ“‡é›†æˆç­–ç•¥:")
    print("=" * 60)
    print("1. åŠ æ¬Šå¹³å‡ (Weighted Average) - æ¨è–¦")
    print("   æ‰€æœ‰æ¨¡å‹çš„è¼¸å‡ºåŠ æ¬Šå¹³å‡ï¼Œå¹³è¡¡å„æ¨¡å‹æ„è¦‹")
    print("\n2. æŠ•ç¥¨æ³• (Voting)")
    print("   æ¯å€‹æ¨¡å‹æŠ•ä¸€ç¥¨ï¼Œå¤šæ•¸æ±ºå®šæœ€çµ‚é æ¸¬")
    print("\n3. æœ€é«˜ä¿¡å¿ƒåº¦ (Max Confidence)")
    print("   é¸æ“‡ä¿¡å¿ƒåº¦æœ€é«˜çš„æ¨¡å‹çš„é æ¸¬çµæœ")
    print("=" * 60)
    
    strategy_map = {
        '1': 'weighted_average',
        '2': 'voting',
        '3': 'max_confidence'
    }
    
    while True:
        try:
            choice = input("\nè«‹é¸æ“‡ç­–ç•¥ (1-3) [é è¨­=1]: ").strip()
            if choice == '':
                choice = '1'
            
            if choice in strategy_map:
                strategy = strategy_map[choice]
                print(f"âœ… é¸æ“‡ç­–ç•¥: {strategy}")
                break
            else:
                print("âš ï¸  è«‹è¼¸å…¥ 1ã€2 æˆ– 3")
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç¨‹å¼çµæŸ")
            return
    
    # æª¢æ¸¬ä¸¦é¸æ“‡è¨ˆç®—è¨­å‚™
    available_devices = detect_available_devices()
    
    print("\nğŸ’» è«‹é¸æ“‡è¨ˆç®—è¨­å‚™:")
    print("=" * 60)
    
    for i, (device_id, device_name) in enumerate(available_devices, 1):
        prefix = "ğŸš€" if 'npu' in device_id.lower() or 'dml' in device_id.lower() or 'mps' in device_id.lower() else \
                 "âš¡" if 'cuda' in device_id else "ğŸ’»"
        suffix = " - æ¨è–¦" if i == 1 and device_id != 'cpu' else ""
        print(f"{i}. {prefix} {device_name}{suffix}")
    
    print("=" * 60)
    
    while True:
        try:
            choice = input(f"\nè«‹é¸æ“‡è¨­å‚™ (1-{len(available_devices)}) [é è¨­=1]: ").strip()
            if choice == '':
                choice = '1'
            
            idx = int(choice) - 1
            if 0 <= idx < len(available_devices):
                device_str, device_name = available_devices[idx]
                print(f"âœ… é¸æ“‡è¨­å‚™: {device_name}")
                
                # ç‰¹æ®Šè™•ç† DirectML è¨­å‚™
                if device_str == 'dml':
                    try:
                        import torch_directml
                        device = torch_directml.device()
                        device_str = str(device)  # è½‰æ›ç‚ºå­—ä¸²è¡¨ç¤º
                        print(f"   è¨­å‚™ç‰©ä»¶: {device_str}")
                    except ImportError:
                        print("âš ï¸  torch_directml æœªå®‰è£ï¼Œæ”¹ç”¨ CPU")
                        device_str = 'cpu'
                    except Exception as e:
                        print(f"âš ï¸  DirectML åˆå§‹åŒ–å¤±æ•—: {e}ï¼Œæ”¹ç”¨ CPU")
                        device_str = 'cpu'
                elif device_str == 'onnx_dml':
                    # ONNX Runtime æ¨¡å¼ä¸‹ï¼Œå¯¦éš›è¨“ç·´é‚„æ˜¯ç”¨ CPUï¼Œä½†æ¨ç†æœƒç”¨ NPU
                    print("   æ³¨æ„: ONNX Runtime NPU æ¨¡å¼")
                    print("   è¨“ç·´ä½¿ç”¨ CPUï¼Œæ¨ç†é€é ONNX Runtime åŠ é€Ÿ")
                    device_str = 'cpu'
                
                break
            else:
                print(f"âš ï¸  è«‹è¼¸å…¥ 1-{len(available_devices)}")
        except ValueError:
            print("âš ï¸  è«‹è¼¸å…¥æœ‰æ•ˆæ•¸å­—")
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç¨‹å¼çµæŸ")
            return
    
    # å¦‚æœé¸æ“‡äº† ONNX DML è¨­å‚™ï¼Œè©¢å•æ˜¯å¦ä½¿ç”¨ NPU åŠ é€Ÿ
    use_onnx_npu = False
    if device_str == 'onnx_dml':
        print("\nğŸš€ AMD Ryzen AI NPU å„ªåŒ–é¸é …")
        print("=" * 60)
        print("æª¢æ¸¬åˆ° ONNX Runtime DirectML å¯ç”¨")
        print("æ˜¯å¦å•Ÿç”¨ NPU ç¡¬é«”åŠ é€Ÿï¼Ÿ")
        print("1. âœ… æ˜¯ - ä½¿ç”¨ ONNX Runtime DirectML (æ¨è–¦ï¼Œå……åˆ†åˆ©ç”¨ NPU)")
        print("2. âŒ å¦ - ä½¿ç”¨ PyTorch CPU æ¨¡å¼")
        print("=" * 60)
        
        while True:
            try:
                npu_choice = input("\nè«‹é¸æ“‡ (1-2) [é è¨­=1]: ").strip()
                if npu_choice == '' or npu_choice == '1':
                    use_onnx_npu = True
                    print("âœ… å·²å•Ÿç”¨ ONNX Runtime NPU åŠ é€Ÿ")
                    print("ğŸ’¡ æ¨¡å‹å°‡è½‰æ›ç‚º ONNX æ ¼å¼ä¸¦ä½¿ç”¨ DirectML åŸ·è¡Œ")
                    device_str = 'cpu'  # ONNX Runtime æœƒè™•ç†è¨­å‚™ï¼ŒPyTorch ç«¯ä½¿ç”¨CPU
                    break
                elif npu_choice == '2':
                    use_onnx_npu = False
                    print("âœ… ä½¿ç”¨ PyTorch CPU æ¨¡å¼")
                    device_str = 'cpu'
                    break
                else:
                    print("âš ï¸  è«‹è¼¸å…¥ 1 æˆ– 2")
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç¨‹å¼çµæŸ")
                return
    
    # é–‹å§‹è©•ä¼°
    print("\n" + "=" * 60)
    print("ğŸš€ é–‹å§‹å¤šæ¨¡å‹é›†æˆè©•ä¼°")
    print("=" * 60)
    
    evaluate_multi_models(
        model_paths=model_paths,
        test_csv='archive/tw_food_101/tw_food_101_test_list.csv',
        test_img_dir='archive/tw_food_101/test',
        num_classes=101,
        batch_size=32,
        img_size=224,
        device_str=device_str,
        strategy=strategy,
        use_onnx_npu=use_onnx_npu
    )
    
    print("\n" + "=" * 60)
    print("ğŸ‰ è©•ä¼°å®Œæˆï¼")
    print("=" * 60)

if __name__ == '__main__':
    main()