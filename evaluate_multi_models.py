#!/usr/bin/env python3
"""
å¤šæ¨¡å‹é›†æˆè©•ä¼°è…³æœ¬
è‡ªå‹•å¾ models ç›®éŒ„æŠ“å–å¤šå€‹æ¨¡å‹ï¼Œä¸¦ä½¿ç”¨é›†æˆå­¸ç¿’é€²è¡Œé æ¸¬
"""

import torch
import torch.nn as nn
from pytorch_model import get_model
from pytorch_data_loader import TaiwanFoodDataset
from torch.utils.data import DataLoader
from torchvision import transforms
from tqdm import tqdm
import os
import csv
import time
from collections import Counter
import platform
import subprocess
import numpy as np

def detect_available_devices():
    """æª¢æ¸¬å¯ç”¨çš„è¨ˆç®—è£ç½®ï¼ˆNPU/GPU/CPUï¼‰"""
    print("\nğŸ” æª¢æ¸¬å¯ç”¨çš„è¨ˆç®—è£ç½®")
    print("=" * 60)
    
    devices = []
    device_info = []
    
    # æª¢æ¸¬ ONNX Runtime DirectML - NPU åŠ é€Ÿæ¨è–¦æ–¹å¼
    onnx_dml_available = False
    
    try:
        import onnxruntime as ort
        providers = ort.get_available_providers()
        if 'DmlExecutionProvider' in providers:
            onnx_dml_available = True
            devices.append(('onnx_dml', 'AMD Ryzen AI NPU (ONNX Runtime DirectML)'))
            device_info.append("âœ… ONNX Runtime DirectML å¯ç”¨")
            device_info.append("   æ”¯æ´: AMD Ryzen AI NPU ç¡¬é«”åŠ é€Ÿ")
            device_info.append("   ç³»åˆ—: Ryzen AI 7040/8040/9HX")
    except ImportError:
        device_info.append("âš ï¸  ONNX Runtime æœªå®‰è£")
        device_info.append("   å»ºè­°å®‰è£: pip install onnxruntime-directml")
    
    # æª¢æ¸¬ AMD Ryzen AI è™•ç†å™¨
    if platform.system() == 'Windows':
        try:
            result = subprocess.run(['wmic', 'cpu', 'get', 'name'], 
                                  capture_output=True, text=True, timeout=5)
            if 'AMD Ryzen' in result.stdout and 'AI' in result.stdout:
                device_info.append("ğŸ’» æª¢æ¸¬åˆ° AMD Ryzen AI è™•ç†å™¨")
                if not onnx_dml_available:
                    device_info.append("   âš ï¸  NPU å¯èƒ½å¯ç”¨ä½†æœªå•Ÿç”¨")
                    device_info.append("   è«‹å®‰è£: pip install onnxruntime-directml")
        except:
            pass
    
    # æª¢æ¸¬ Apple MPS (Neural Engine)
    try:
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            devices.append(('mps', 'Apple Neural Engine (MPS)'))
            device_info.append("ğŸ Apple Neural Engine (MPS) å¯ç”¨")
    except:
        pass
    
    # æª¢æ¸¬ CUDA GPU
    if torch.cuda.is_available():
        gpu_count = torch.cuda.device_count()
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            devices.append((f'cuda:{i}', f'GPU {i}: {gpu_name} ({memory:.1f}GB)'))
            device_info.append(f"âœ… GPU {i}: {gpu_name} ({memory:.1f} GB)")
    
    # CPU å§‹çµ‚å¯ç”¨
    devices.append(('cpu', 'CPU'))
    device_info.append("ğŸ’» CPU å¯ç”¨")
    
    # é¡¯ç¤ºæª¢æ¸¬çµæœ
    for info in device_info:
        print(info)
    
    if not devices[:-1]:  # é™¤äº†CPUä»¥å¤–æ²’æœ‰å…¶ä»–è¨­å‚™
        print("\nâš ï¸  æ²’æœ‰æª¢æ¸¬åˆ° NPU æˆ– GPUï¼Œå°‡ä½¿ç”¨ CPU")
        print("ğŸ’¡ æç¤º: å¯ä»¥å®‰è£ NPU æ”¯æ´ä»¥åŠ é€Ÿæ¨ç†")
    
    print("=" * 60)
    return devices

def get_all_models_from_directory(models_dir='models', max_models=None, min_models=2):
    """å¾ç›®éŒ„ä¸­ç²å–æ‰€æœ‰æ¨¡å‹æª”æ¡ˆ
    
    Args:
        models_dir: æ¨¡å‹ç›®éŒ„è·¯å¾‘
        max_models: æœ€å¤šä½¿ç”¨çš„æ¨¡å‹æ•¸é‡ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰æ¨¡å‹
        min_models: æœ€å°‘éœ€è¦çš„æ¨¡å‹æ•¸é‡
    
    Returns:
        list: æ¨¡å‹æª”æ¡ˆè·¯å¾‘åˆ—è¡¨
    """
    if not os.path.exists(models_dir):
        print(f"âŒ æ¨¡å‹ç›®éŒ„ä¸å­˜åœ¨: {models_dir}")
        return []
    
    model_files = [f for f in os.listdir(models_dir) if f.endswith('.pth')]
    
    if not model_files:
        print(f"âŒ åœ¨ {models_dir} ä¸­æ²’æœ‰æ‰¾åˆ°ä»»ä½• .pth æ¨¡å‹æª”æ¡ˆ")
        return []
    
    if len(model_files) < min_models:
        print(f"âš ï¸  åªæ‰¾åˆ° {len(model_files)} å€‹æ¨¡å‹ï¼Œå°‘æ–¼æœ€å°‘éœ€æ±‚ {min_models} å€‹")
        print(f"   å»ºè­°è‡³å°‘è¨“ç·´ {min_models} å€‹ä¸åŒçš„æ¨¡å‹ä»¥ç²å¾—æ›´å¥½çš„é›†æˆæ•ˆæœ")
    
    # æŒ‰ä¿®æ”¹æ™‚é–“æ’åºï¼ˆæœ€æ–°çš„å„ªå…ˆï¼‰
    model_files = sorted(model_files, 
                        key=lambda x: os.path.getmtime(os.path.join(models_dir, x)), 
                        reverse=True)
    
    if max_models:
        model_files = model_files[:max_models]
    
    model_paths = [os.path.join(models_dir, f) for f in model_files]
    
    print(f"\nğŸ“ åœ¨ {models_dir} ä¸­æ‰¾åˆ° {len(model_paths)} å€‹æ¨¡å‹:")
    print("=" * 60)
    for i, (path, filename) in enumerate(zip(model_paths, model_files), 1):
        file_time = os.path.getmtime(path)
        time_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(file_time))
        file_size = os.path.getsize(path) / 1024 / 1024  # MB
        print(f"   {i}. {filename}")
        print(f"      ä¿®æ”¹æ™‚é–“: {time_str}, å¤§å°: {file_size:.1f} MB")
    print("=" * 60)
    
    return model_paths

def convert_model_to_onnx(model, model_name, input_shape=(1, 3, 224, 224), output_path=None):
    """å°‡ PyTorch æ¨¡å‹è½‰æ›ç‚º ONNX æ ¼å¼
    
    Args:
        model: PyTorch æ¨¡å‹
        model_name: æ¨¡å‹åç¨±
        input_shape: è¼¸å…¥å¼µé‡å½¢ç‹€
        output_path: ONNX æª”æ¡ˆè¼¸å‡ºè·¯å¾‘
    
    Returns:
        onnx_path: ONNX æª”æ¡ˆè·¯å¾‘
    """
    if output_path is None:
        output_path = f"temp_onnx_{model_name}.onnx"
    
    try:
        # ç¢ºä¿æ¨¡å‹åœ¨ CPU ä¸Š
        model = model.cpu()
        model.eval()
        
        # å‰µå»ºè™›æ“¬è¼¸å…¥
        dummy_input = torch.randn(*input_shape)
        
        # å°å‡ºç‚º ONNX
        torch.onnx.export(
            model,
            dummy_input,
            output_path,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={
                'input': {0: 'batch_size'},
                'output': {0: 'batch_size'}
            },
            opset_version=13,
            do_constant_folding=True
        )
        
        return output_path
    
    except Exception as e:
        print(f"   âš ï¸  ONNX è½‰æ›å¤±æ•—: {e}")
        return None

def warmup_onnx_session(session, input_shape):
    """é ç†± ONNX æœƒè©±ä»¥å„ªåŒ–é¦–æ¬¡æ¨ç†æ€§èƒ½
    
    Args:
        session: ONNX Runtime æœƒè©±
        input_shape: è¼¸å…¥å½¢ç‹€ (batch_size, channels, height, width)
    """
    try:
        dummy_input = np.random.randn(*input_shape).astype(np.float32)
        input_name = session.get_inputs()[0].name
        # åŸ·è¡Œå¹¾æ¬¡é ç†±æ¨ç†
        for _ in range(3):
            _ = session.run(None, {input_name: dummy_input})
    except:
        pass  # é ç†±å¤±æ•—ä¸å½±éŸ¿æ­£å¸¸ä½¿ç”¨

def create_onnx_session(onnx_path, use_dml=True, enable_profiling=False):
    """å‰µå»ºå„ªåŒ–çš„ ONNX Runtime æ¨ç†æœƒè©±ï¼ˆNPU åŠ é€Ÿå„ªåŒ–ï¼‰
    
    Args:
        onnx_path: ONNX æ¨¡å‹è·¯å¾‘
        use_dml: æ˜¯å¦ä½¿ç”¨ DirectML åŸ·è¡Œæä¾›è€…
    
    Returns:
        session: ONNX Runtime æ¨ç†æœƒè©±
    """
    try:
        import onnxruntime as ort
        
        # å‰µå»ºæœƒè©±é¸é … - å„ªåŒ–é…ç½®
        sess_options = ort.SessionOptions()
        
        # åœ–å„ªåŒ–ç­‰ç´š - ä½¿ç”¨æœ€é«˜å„ªåŒ–ï¼ˆæ“´å±•å„ªåŒ–ï¼‰
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED
        
        # å•Ÿç”¨è¨˜æ†¶é«”å„ªåŒ–æ¨¡å¼
        sess_options.enable_mem_pattern = True
        sess_options.enable_mem_reuse = True
        sess_options.enable_cpu_mem_arena = True
        
        # ä¸¦è¡ŒåŸ·è¡Œå„ªåŒ–
        sess_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL
        sess_options.inter_op_num_threads = 2  # æ“ä½œé–“ä¸¦è¡Œ
        sess_options.intra_op_num_threads = 4  # æ“ä½œå…§ä¸¦è¡Œ
        
        # æ€§èƒ½åˆ†æï¼ˆå¯é¸ï¼‰
        if enable_profiling:
            sess_options.enable_profiling = True
        
        # è¨­ç½®åŸ·è¡Œæä¾›è€… - DirectML å„ªåŒ–é…ç½®
        providers = []
        provider_options = []
        
        if use_dml:
            # DirectML æä¾›è€…é…ç½®ï¼ˆé‡å° AMD Ryzen AI NPU å„ªåŒ–ï¼‰
            dml_options = {
                'device_id': 0,  # ä½¿ç”¨ç¬¬ä¸€å€‹ NPU è¨­å‚™
                'disable_metacommands': False,  # å•Ÿç”¨ metacommands åŠ é€Ÿ
                'enable_dynamic_graph_fusion': True,  # å•Ÿç”¨å‹•æ…‹åœ–èåˆ
            }
            providers.append('DmlExecutionProvider')
            provider_options.append(dml_options)
        
        # CPU ä½œç‚ºå›é€€
        providers.append('CPUExecutionProvider')
        provider_options.append({})
        
        # å‰µå»ºæ¨ç†æœƒè©±
        session = ort.InferenceSession(
            onnx_path,
            sess_options=sess_options,
            providers=providers,
            provider_options=provider_options
        )
        
        return session
    
    except Exception as e:
        print(f"   âš ï¸  å‰µå»º ONNX æœƒè©±å¤±æ•—: {e}")
        return None

def ensemble_predict_onnx(onnx_sessions, images, strategy='weighted_average'):
    """ä½¿ç”¨ ONNX Runtime é€²è¡Œå¤šæ¨¡å‹é›†æˆé æ¸¬ï¼ˆNPU åŠ é€Ÿï¼‰
    
    Args:
        onnx_sessions: ONNX æ¨ç†æœƒè©±åˆ—è¡¨ [(session, weight, name), ...]
        images: è¼¸å…¥åœ–ç‰‡å¼µé‡ (PyTorch)
        strategy: é›†æˆç­–ç•¥
    
    Returns:
        predictions: é æ¸¬é¡åˆ¥
        confidences: é æ¸¬ä¿¡å¿ƒåº¦
        details: è©³ç´°è³‡è¨Š
    """
    all_outputs = []
    all_predictions = []
    all_confidences = []
    successful_models = []
    
    # è½‰æ› PyTorch å¼µé‡ç‚º NumPyï¼ˆé€£çºŒè¨˜æ†¶é«”å¸ƒå±€ï¼‰
    if images.is_cuda:
        images_np = images.cpu().numpy()
    else:
        images_np = images.numpy()
    
    # ç¢ºä¿é€£çºŒè¨˜æ†¶é«”å¸ƒå±€ï¼ˆå„ªåŒ–æ€§èƒ½ï¼‰
    if not images_np.flags['C_CONTIGUOUS']:
        images_np = np.ascontiguousarray(images_np)
    
    # ç¢ºä¿æ­£ç¢ºçš„æ•¸æ“šé¡å‹
    images_np = images_np.astype(np.float32)
    
    # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é æ¸¬ï¼ˆä¸¦è¡Œæ¨ç†å„ªåŒ–ï¼‰
    for session, weight, name in onnx_sessions:
        try:
            # ONNX Runtime æ¨ç†ï¼ˆä½¿ç”¨ NPU åŠ é€Ÿï¼‰
            input_name = session.get_inputs()[0].name
            outputs = session.run(None, {input_name: images_np})[0]
            
            # è½‰æ›å› PyTorch å¼µé‡é€²è¡Œå¾Œè™•ç†
            outputs_torch = torch.from_numpy(outputs).float()
            probs = torch.softmax(outputs_torch, dim=1)
            max_probs, predicted = probs.max(1)
            
            all_outputs.append(outputs_torch * weight)
            all_predictions.append(predicted)
            all_confidences.append(max_probs)
            successful_models.append(name)
            
        except Exception as e:
            # éœé»˜è·³éå¤±æ•—çš„æ¨¡å‹
            continue
    
    # ç¢ºä¿è‡³å°‘æœ‰ä¸€å€‹æ¨¡å‹æˆåŠŸ
    if not all_outputs:
        raise RuntimeError("æ‰€æœ‰æ¨¡å‹éƒ½ç„¡æ³•é€²è¡Œé æ¸¬")
    
    # å–®æ¨¡å‹æƒ…æ³
    if len(all_outputs) == 1:
        probs = torch.softmax(all_outputs[0], dim=1)
        max_probs, predictions = probs.max(1)
        details = {
            'individual_predictions': [all_predictions[0].numpy()],
            'individual_confidences': [all_confidences[0].numpy()],
            'model_names': successful_models
        }
        return predictions, max_probs, details
    
    # é›†æˆç­–ç•¥
    if strategy == 'weighted_average':
        ensemble_outputs = all_outputs[0]
        for output in all_outputs[1:]:
            ensemble_outputs = ensemble_outputs + output
        probs = torch.softmax(ensemble_outputs, dim=1)
        max_probs, predictions = probs.max(1)
        
    elif strategy == 'voting':
        predictions_stack = torch.stack(all_predictions)
        predictions = torch.mode(predictions_stack, dim=0)[0]
        max_probs = torch.stack(all_confidences).mean(0)
        
    elif strategy == 'max_confidence':
        all_confidences_stack = torch.stack(all_confidences)
        max_conf_indices = all_confidences_stack.argmax(0)
        
        predictions = torch.zeros_like(all_predictions[0])
        max_probs = torch.zeros_like(all_confidences[0])
        
        for i in range(len(predictions)):
            best_model_idx = max_conf_indices[i]
            predictions[i] = all_predictions[best_model_idx][i]
            max_probs[i] = all_confidences[best_model_idx][i]
    
    # è¿”å›è©³ç´°è³‡è¨Š
    details = {
        'individual_predictions': [p.numpy() for p in all_predictions],
        'individual_confidences': [c.numpy() for c in all_confidences],
        'model_names': successful_models
    }
    
    return predictions, max_probs, details

def detect_model_architecture(model_path):
    """å¾æ¨¡å‹æª”æ¡ˆåç¨±ä¸­æª¢æ¸¬æ¨¡å‹æ¶æ§‹"""
    filename = os.path.basename(model_path).lower()
    
    if 'efficientnet_b3' in filename or 'efficientnet' in filename:
        return 'efficientnet_b3'
    elif 'convnext_tiny' in filename or 'convnext' in filename:
        return 'convnext_tiny'
    elif 'regnet_y' in filename or 'regnet' in filename:
        return 'regnet_y'
    elif 'vit' in filename or 'vision_transformer' in filename:
        return 'vit'
    elif 'resnet50' in filename or 'resnet' in filename:
        return 'resnet50'
    else:
        # å˜—è©¦å¾æ¨¡å‹å…§å®¹æª¢æ¸¬
        try:
            state_dict = torch.load(model_path, map_location='cpu')
            keys = list(state_dict.keys())
            
            if any('features' in key for key in keys):
                if any('block' in key for key in keys):
                    return 'efficientnet_b3'
                elif any('stages' in key for key in keys):
                    return 'convnext_tiny'
            elif any('layer' in key for key in keys):
                return 'resnet50'
            elif any('blocks' in key for key in keys):
                return 'vit'
        except:
            pass
        
        print(f"âš ï¸  ç„¡æ³•å¾æª”æ¡ˆåç¨±æª¢æ¸¬æ¨¡å‹æ¶æ§‹: {filename}")
        print("   ä½¿ç”¨é è¨­æ¶æ§‹: ResNet50")
        return 'resnet50'

def ensemble_predict(models_info, images, device, strategy='weighted_average'):
    """å¤šæ¨¡å‹é›†æˆé æ¸¬
    
    Args:
        models_info: æ¨¡å‹è³‡è¨Šåˆ—è¡¨ [(model, weight, name), ...]
        images: è¼¸å…¥åœ–ç‰‡å¼µé‡
        device: è¨ˆç®—è¨­å‚™
        strategy: é›†æˆç­–ç•¥ ('weighted_average', 'voting', 'max_confidence')
    
    Returns:
        predictions: é æ¸¬é¡åˆ¥
        confidences: é æ¸¬ä¿¡å¿ƒåº¦
        details: æ¯å€‹æ¨¡å‹çš„é æ¸¬è©³æƒ…
    """
    all_outputs = []
    all_predictions = []
    all_confidences = []
    all_probs = []
    successful_models = []  # è¨˜éŒ„æˆåŠŸçš„æ¨¡å‹
    
    # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é æ¸¬
    for model, weight, name in models_info:
        with torch.no_grad():
            try:
                outputs = model(images.to(device))
                probs = torch.softmax(outputs, dim=1)
                max_probs, predicted = probs.max(1)
                
                all_outputs.append(outputs * weight)
                all_predictions.append(predicted)
                all_confidences.append(max_probs)
                all_probs.append(probs)
                successful_models.append(name)
                
            except Exception as e:
                # éœé»˜è·³éå¤±æ•—çš„æ¨¡å‹
                continue
    
    # ç¢ºä¿è‡³å°‘æœ‰ä¸€å€‹æ¨¡å‹æˆåŠŸé æ¸¬
    if not all_outputs:
        raise RuntimeError("æ‰€æœ‰æ¨¡å‹éƒ½ç„¡æ³•é€²è¡Œé æ¸¬")
    
    # å¦‚æœåªæœ‰ä¸€å€‹æ¨¡å‹æˆåŠŸï¼Œç›´æ¥ä½¿ç”¨å®ƒçš„çµæœ
    if len(all_outputs) == 1:
        probs = torch.softmax(all_outputs[0], dim=1)
        max_probs, predictions = probs.max(1)
        details = {
            'individual_predictions': [all_predictions[0].cpu().numpy() if all_predictions[0].is_cuda else all_predictions[0].numpy()],
            'individual_confidences': [all_confidences[0].cpu().numpy() if all_confidences[0].is_cuda else all_confidences[0].numpy()],
            'model_names': successful_models
        }
        return predictions, max_probs, details
    
    # æ‰€æœ‰é›†æˆè¨ˆç®—åœ¨CPUä¸Šé€²è¡Œï¼ˆå·²ç¶“æ˜¯CPUå¼µé‡ï¼‰
    try:
        if strategy == 'weighted_average':
            # ç­–ç•¥1: åŠ æ¬Šå¹³å‡æ‰€æœ‰æ¨¡å‹çš„è¼¸å‡º
            ensemble_outputs = all_outputs[0]
            for output in all_outputs[1:]:
                ensemble_outputs = ensemble_outputs + output
            probs = torch.softmax(ensemble_outputs, dim=1)
            max_probs, predictions = probs.max(1)
            
        elif strategy == 'voting':
            # ç­–ç•¥2: æŠ•ç¥¨æ³•ï¼ˆæ¯å€‹æ¨¡å‹ä¸€ç¥¨ï¼‰
            predictions_stack = torch.stack(all_predictions)
            predictions = torch.mode(predictions_stack, dim=0)[0]
            max_probs = torch.stack(all_confidences).mean(0)
            
        elif strategy == 'max_confidence':
            # ç­–ç•¥3: é¸æ“‡æœ€é«˜ä¿¡å¿ƒåº¦çš„é æ¸¬
            all_confidences_stack = torch.stack(all_confidences)
            max_conf_indices = all_confidences_stack.argmax(0)
            
            predictions = torch.zeros_like(all_predictions[0])
            max_probs = torch.zeros_like(all_confidences[0])
            
            for i in range(len(predictions)):
                best_model_idx = max_conf_indices[i]
                predictions[i] = all_predictions[best_model_idx][i]
                max_probs[i] = all_confidences[best_model_idx][i]
    
    except Exception as e:
        print(f"\nâŒ é›†æˆç­–ç•¥ '{strategy}' åŸ·è¡Œå¤±æ•—: {e}")
        print(f"   æˆåŠŸçš„æ¨¡å‹æ•¸: {len(successful_models)}")
        print(f"   è¼¸å‡ºå¼µé‡æ•¸: {len(all_outputs)}")
        if all_outputs:
            print(f"   è¼¸å‡ºå¼µé‡å½¢ç‹€: {all_outputs[0].shape}")
        raise
    
    # è¿”å›è©³ç´°è³‡è¨Š
    details = {
        'individual_predictions': [p.cpu().numpy() if p.is_cuda else p.numpy() for p in all_predictions],
        'individual_confidences': [c.cpu().numpy() if c.is_cuda else c.numpy() for c in all_confidences],
        'model_names': successful_models
    }
    
    return predictions, max_probs, details

def evaluate_multi_models(model_paths, test_csv, test_img_dir, num_classes=101, 
                          batch_size=32, img_size=224, device_str='cpu', 
                          strategy='weighted_average', use_onnx_npu=False):
    """ä½¿ç”¨å¤šå€‹æ¨¡å‹é€²è¡Œé›†æˆè©•ä¼°
    
    Args:
        model_paths: æ¨¡å‹æª”æ¡ˆè·¯å¾‘åˆ—è¡¨
        test_csv: æ¸¬è©¦é›†CSVæª”æ¡ˆ
        test_img_dir: æ¸¬è©¦é›†åœ–ç‰‡ç›®éŒ„
        num_classes: é¡åˆ¥æ•¸é‡
        batch_size: æ‰¹æ¬¡å¤§å°
        img_size: åœ–ç‰‡å¤§å°
        device_str: è¨ˆç®—è¨­å‚™
        strategy: é›†æˆç­–ç•¥
        use_onnx_npu: æ˜¯å¦ä½¿ç”¨ ONNX Runtime DirectML é€²è¡Œ NPU åŠ é€Ÿ
    """
    print(f"\nğŸ¯ å¤šæ¨¡å‹é›†æˆè©•ä¼°æ¨¡å¼")
    print(f"ğŸ“Š ä½¿ç”¨ {len(model_paths)} å€‹æ¨¡å‹é€²è¡Œé›†æˆé æ¸¬")
    print(f"ğŸ² é›†æˆç­–ç•¥: {strategy}")
    if use_onnx_npu:
        print(f"ğŸš€ NPU åŠ é€Ÿ: ONNX Runtime DirectML")
    print("=" * 60)
    
    # è¨­å®šè£ç½®
    # æ³¨æ„ï¼šç•¶ use_onnx_npu=True æ™‚ï¼ŒPyTorch æ¨¡å‹ä½¿ç”¨ CPUï¼Œæ¨ç†ç”± ONNX Runtime åœ¨ NPU ä¸ŠåŸ·è¡Œ
    if use_onnx_npu:
        device = torch.device('cpu')
        print(f"ğŸ’» PyTorch ä½¿ç”¨è£ç½®: CPU (æ¨¡å‹è¼‰å…¥ç”¨)")
        print(f"ğŸš€ ONNX Runtime å°‡ä½¿ç”¨: NPU (DirectML æ¨ç†)")
    else:
        # PyTorch æ¨¡å¼
        try:
            if isinstance(device_str, str):
                if device_str.startswith('cuda'):
                    device = torch.device(device_str)
                elif device_str == 'mps':
                    device = torch.device('mps')
                elif device_str.startswith('npu'):
                    device = torch.device(device_str)
                else:
                    device = torch.device('cpu')
            else:
                device = device_str
        except Exception as e:
            print(f"âš ï¸  è¨­å‚™åˆå§‹åŒ–å¤±æ•—: {e}ï¼Œä½¿ç”¨ CPU")
            device = torch.device('cpu')
        
        print(f"ğŸ’» ä½¿ç”¨è£ç½®: {device}")
    
    # é¡¯ç¤ºè¨­å‚™è©³ç´°è³‡è¨Š
    if not use_onnx_npu:
        if str(device).startswith('cuda'):
            gpu_id = int(str(device).split(':')[1]) if ':' in str(device) else 0
            print(f"ğŸš€ GPU: {torch.cuda.get_device_name(gpu_id)}")
            print(f"ğŸ’¾ GPU è¨˜æ†¶é«”: {torch.cuda.get_device_properties(gpu_id).total_memory / 1024**3:.1f} GB")
        elif str(device) == 'mps':
            print(f"ğŸ ä½¿ç”¨ Apple Neural Engine (MPS)")
    print("=" * 60)
    
    # è¼‰å…¥æ‰€æœ‰æ¨¡å‹
    models_info = []
    onnx_sessions = []  # ç”¨æ–¼ONNX Runtimeæ¨¡å¼
    
    print(f"\nğŸ“¦ é–‹å§‹è¼‰å…¥ {len(model_paths)} å€‹æ¨¡å‹...")
    if use_onnx_npu:
        print("ğŸš€ æ¨¡å¼: ONNX Runtime DirectML (NPUåŠ é€Ÿ)")
    print("=" * 60)
    
    for i, model_path in enumerate(model_paths, 1):
        model_name = os.path.basename(model_path)
        print(f"\nğŸ“¦ [{i}/{len(model_paths)}] è¼‰å…¥: {model_name}")
        
        # æª¢æ¸¬æ¨¡å‹æ¶æ§‹
        architecture = detect_model_architecture(model_path)
        print(f"   ğŸ—ï¸  æ¶æ§‹: {architecture}")
        
        # å»ºç«‹æ¨¡å‹
        try:
            model = get_model(architecture, num_classes=num_classes, dropout_rate=0.3)
            
            # è¼‰å…¥æ¬Šé‡åˆ°CPU
            state_dict = torch.load(model_path, map_location='cpu', weights_only=False)
            model.load_state_dict(state_dict)
            model = model.cpu()
            model.eval()
            
            # è¨ˆç®—æ¨¡å‹æ¬Šé‡
            weight = 1.0 / len(model_paths)
            
            # å¦‚æœä½¿ç”¨ONNX Runtime NPUåŠ é€Ÿ
            if use_onnx_npu:
                try:
                    # è½‰æ›ç‚ºONNX
                    print(f"   ğŸ”„ è½‰æ›ç‚º ONNX æ ¼å¼...")
                    onnx_path = convert_model_to_onnx(
                        model, 
                        model_name.replace('.pth', ''),
                        input_shape=(batch_size, 3, img_size, img_size)
                    )
                    
                    if onnx_path:
                        # å‰µå»º ONNX Runtime æœƒè©±
                        session = create_onnx_session(onnx_path, use_dml=True)
                        if session:
                            # æª¢æŸ¥åŸ·è¡Œæä¾›è€…
                            providers = session.get_providers()
                            if 'DmlExecutionProvider' in providers:
                                print(f"   ï¿½ ONNX Runtime å·²å•Ÿç”¨ DirectML (NPUåŠ é€Ÿ)")
                            else:
                                print(f"   ğŸ’» ONNX Runtime ä½¿ç”¨ CPU")
                            
                            onnx_sessions.append((session, weight, model_name))
                            print(f"   âœ… ONNX è½‰æ›æˆåŠŸ (æ¬Šé‡: {weight:.4f})")
                        else:
                            print(f"   âš ï¸  ONNX æœƒè©±å‰µå»ºå¤±æ•—ï¼Œè·³éæ­¤æ¨¡å‹")
                    else:
                        print(f"   âš ï¸  ONNX è½‰æ›å¤±æ•—ï¼Œè·³éæ­¤æ¨¡å‹")
                        
                except Exception as e:
                    print(f"   âŒ ONNX è™•ç†å¤±æ•—: {e}")
                    print(f"   âš ï¸  è·³éæ­¤æ¨¡å‹")
                    continue
            else:
                # PyTorch æ¨¡å¼ï¼ˆé NPUï¼‰
                model = model.to(device)
                models_info.append((model, weight, model_name))
                print(f"   âœ… è¼‰å…¥æˆåŠŸ (æ¬Šé‡: {weight:.4f})")
            
        except Exception as e:
            print(f"   âŒ è¼‰å…¥å¤±æ•—: {e}")
            print(f"   âš ï¸  è·³éæ­¤æ¨¡å‹")
            continue
    
    # æª¢æŸ¥æ˜¯å¦æœ‰æˆåŠŸè¼‰å…¥çš„æ¨¡å‹
    if use_onnx_npu:
        if not onnx_sessions:
            print("\nâŒ æ²’æœ‰æˆåŠŸè¼‰å…¥ä»»ä½• ONNX æ¨¡å‹")
            return
        if len(onnx_sessions) < 2:
            print(f"\nâš ï¸  åªæˆåŠŸè¼‰å…¥ {len(onnx_sessions)} å€‹æ¨¡å‹")
            print("   é›†æˆæ•ˆæœå¯èƒ½æœ‰é™ï¼Œå»ºè­°ä½¿ç”¨è‡³å°‘2å€‹æ¨¡å‹")
        print(f"\nâœ… æˆåŠŸè¼‰å…¥ {len(onnx_sessions)} å€‹ ONNX æ¨¡å‹ï¼ˆNPUåŠ é€Ÿï¼‰")
    else:
        if not models_info:
            print("\nâŒ æ²’æœ‰æˆåŠŸè¼‰å…¥ä»»ä½•æ¨¡å‹")
            return
        if len(models_info) < 2:
            print(f"\nâš ï¸  åªæˆåŠŸè¼‰å…¥ {len(models_info)} å€‹æ¨¡å‹")
            print("   é›†æˆæ•ˆæœå¯èƒ½æœ‰é™ï¼Œå»ºè­°ä½¿ç”¨è‡³å°‘2å€‹æ¨¡å‹")
        print(f"\nâœ… æˆåŠŸè¼‰å…¥ {len(models_info)} å€‹æ¨¡å‹")
    print("=" * 60)
    
    # è³‡æ–™è½‰æ›
    test_transform = transforms.Compose([
        transforms.Resize((img_size, img_size)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    # NPU æ‰¹æ¬¡å¤§å°å„ªåŒ–å»ºè­°
    if use_onnx_npu:
        # NPU é€šå¸¸åœ¨è¼ƒå¤§æ‰¹æ¬¡ä¸‹æ€§èƒ½æ›´å¥½
        original_batch_size = batch_size
        if batch_size < 32:
            batch_size = 32
            print(f"\nğŸ’¡ NPU å„ªåŒ–: æ‰¹æ¬¡å¤§å°å¾ {original_batch_size} èª¿æ•´ç‚º {batch_size}")
            print(f"   è¼ƒå¤§æ‰¹æ¬¡èƒ½æ›´å¥½åˆ©ç”¨ NPU ä¸¦è¡Œè¨ˆç®—èƒ½åŠ›")
    
    # å»ºç«‹æ¸¬è©¦é›† DataLoader
    print("\nğŸ“Š è¼‰å…¥æ¸¬è©¦é›†è³‡æ–™...")
    test_dataset = TaiwanFoodDataset(test_csv, test_img_dir, test_transform, is_test=True)
    
    # NPU å„ªåŒ–ï¼šä½¿ç”¨ pin_memory åŠ é€Ÿæ•¸æ“šå‚³è¼¸
    pin_memory = use_onnx_npu
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=0,
        pin_memory=pin_memory
    )
    
    print(f"   æ¸¬è©¦é›†å¤§å°: {len(test_dataset)} å¼µåœ–ç‰‡")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    if use_onnx_npu:
        print(f"   NPU å„ªåŒ–: å·²å•Ÿç”¨è¨˜æ†¶é«”å›ºå®š (pin_memory)")
    print("=" * 60)
    
    # åŸ·è¡Œé›†æˆé æ¸¬
    print(f"\nğŸ” é–‹å§‹å¤šæ¨¡å‹é›†æˆè©•ä¼°...")
    print(f"   ç­–ç•¥: {strategy}")
    
    all_predictions = []
    all_confidences = []
    all_details = []
    
    start_time = time.time()
    
    # è¿½è¹¤å¤±æ•—çš„æ¨¡å‹ï¼ˆåªåœ¨ç¬¬ä¸€æ‰¹é¡¯ç¤ºï¼‰
    first_batch_failed_models = set()
    first_batch_processed = False
    
    with torch.no_grad():
        test_pbar = tqdm(test_loader, desc="é›†æˆé æ¸¬ä¸­", ncols=100)
        for batch_idx, (images, _) in enumerate(test_pbar):
            
            # æ ¹æ“šæ¨¡å¼é¸æ“‡é æ¸¬æ–¹æ³•
            try:
                if use_onnx_npu:
                    # ONNX Runtime DirectML æ¨¡å¼ï¼ˆNPUåŠ é€Ÿï¼‰
                    predictions, confidences, details = ensemble_predict_onnx(
                        onnx_sessions, images, strategy
                    )
                    
                    # ç¬¬ä¸€æ‰¹æ™‚é¡¯ç¤ºä¿¡æ¯
                    if not first_batch_processed:
                        print(f"\nâœ… ä½¿ç”¨ {len(details['model_names'])} å€‹æ¨¡å‹é€²è¡Œ NPU åŠ é€Ÿæ¨ç†\n")
                        first_batch_processed = True
                        
                else:
                    # PyTorch æ¨¡å¼
                    is_directml = 'privateuseone' in str(device).lower() or 'dml' in str(device).lower()
                    
                    if not is_directml:
                        images = images.to(device)
                    
                    predictions, confidences, details = ensemble_predict(
                        models_info, images, device, strategy
                    )
                    
                    # ç¬¬ä¸€æ‰¹æ™‚æª¢æŸ¥å“ªäº›æ¨¡å‹å¤±æ•—äº†
                    if not first_batch_processed:
                        all_model_names = [name for _, _, name in models_info]
                        successful_models = details['model_names']
                        failed_models = set(all_model_names) - set(successful_models)
                        
                        if failed_models:
                            print(f"\nâš ï¸  ä»¥ä¸‹æ¨¡å‹ç„¡æ³•åœ¨DirectMLä¸Šé‹è¡Œï¼Œå·²è·³é:")
                            for model_name in failed_models:
                                print(f"   - {model_name}")
                            print(f"\nâœ… ä½¿ç”¨ {len(successful_models)} å€‹æ¨¡å‹ç¹¼çºŒé›†æˆé æ¸¬\n")
                        
                        first_batch_processed = True
                
            except Exception as e:
                print(f"\nâŒ é›†æˆé æ¸¬å¤±æ•—: {type(e).__name__}")
                print(f"   éŒ¯èª¤ä¿¡æ¯: {str(e)}")
                import traceback
                traceback.print_exc()
                raise
            
            all_predictions.extend(predictions.cpu().numpy())
            all_confidences.extend(confidences.cpu().numpy())
            all_details.append(details)
            
            # æ›´æ–°é€²åº¦æ¢
            avg_conf = sum(all_confidences) / len(all_confidences)
            test_pbar.set_postfix({
                'å·²è™•ç†': len(all_predictions),
                'å¹³å‡ä¿¡å¿ƒ': f'{avg_conf:.3f}'
            })
    
    elapsed_time = time.time() - start_time
    
    # å„²å­˜é æ¸¬çµæœ
    results_file = f"test_predictions_ensemble_{strategy}.csv"
    print(f"\nğŸ’¾ å„²å­˜é æ¸¬çµæœ...")
    
    with open(results_file, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Id', 'Category', 'Confidence'])
        for i, (pred, conf) in enumerate(zip(all_predictions, all_confidences)):
            writer.writerow([i, int(pred), f'{conf:.6f}'])
    
    # çµ±è¨ˆçµæœ
    print("\n" + "=" * 60)
    print("ğŸ“Š é›†æˆè©•ä¼°çµæœçµ±è¨ˆ")
    print("=" * 60)
    print(f"âœ… è©•ä¼°å®Œæˆï¼")
    print(f"ğŸ“ çµæœæª”æ¡ˆ: {results_file}")
    print(f"ğŸ“Š è™•ç†åœ–ç‰‡: {len(all_predictions)} å¼µ")
    print(f"â±ï¸  ç¸½è€—æ™‚: {elapsed_time:.2f} ç§’")
    print(f"âš¡ é€Ÿåº¦: {len(all_predictions)/elapsed_time:.2f} å¼µ/ç§’")
    
    avg_confidence = sum(all_confidences) / len(all_confidences)
    print(f"\nğŸ“ˆ å¹³å‡ä¿¡å¿ƒåº¦: {avg_confidence:.4f}")
    print(f"ğŸ“ˆ æœ€é«˜ä¿¡å¿ƒåº¦: {max(all_confidences):.4f}")
    print(f"ğŸ“‰ æœ€ä½ä¿¡å¿ƒåº¦: {min(all_confidences):.4f}")
    
    # é¡¯ç¤ºé æ¸¬é¡åˆ¥åˆ†ä½ˆ
    pred_counts = Counter(all_predictions)
    print(f"\nğŸ“Š é æ¸¬é¡åˆ¥åˆ†ä½ˆ (å‰10å):")
    for class_id, count in pred_counts.most_common(10):
        percentage = count / len(all_predictions) * 100
        print(f"   é¡åˆ¥ {class_id:3d}: {count:4d} å¼µ ({percentage:5.2f}%)")
    
    # é¡¯ç¤ºä¿¡å¿ƒåº¦åˆ†ä½ˆ
    conf_ranges = [(0, 0.5), (0.5, 0.7), (0.7, 0.85), (0.85, 0.95), (0.95, 1.0)]
    print(f"\nğŸ“Š ä¿¡å¿ƒåº¦åˆ†ä½ˆ:")
    for low, high in conf_ranges:
        count = sum(1 for c in all_confidences if low <= c < high)
        percentage = count / len(all_confidences) * 100
        print(f"   {low:.2f} ~ {high:.2f}: {count:4d} å¼µ ({percentage:5.2f}%)")
    
    # æ¨™è¨˜ä½ä¿¡å¿ƒåº¦é æ¸¬
    low_conf_threshold = 0.5
    low_conf_predictions = [(i, pred, conf) for i, (pred, conf) in enumerate(zip(all_predictions, all_confidences)) 
                            if conf < low_conf_threshold]
    
    if low_conf_predictions:
        print(f"\nâš ï¸  ä¿¡å¿ƒåº¦ä½æ–¼ {low_conf_threshold} çš„é æ¸¬: {len(low_conf_predictions)} å¼µ ({len(low_conf_predictions)/len(all_predictions)*100:.1f}%)")
        print(f"   å»ºè­°äººå·¥æª¢æŸ¥é€™äº›é æ¸¬")
        
        # å„²å­˜ä½ä¿¡å¿ƒåº¦é æ¸¬æ¸…å–®
        low_conf_file = f"low_confidence_predictions_{strategy}.csv"
        with open(low_conf_file, 'w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(['ImageId', 'PredictedClass', 'Confidence'])
            for img_id, pred, conf in low_conf_predictions[:20]:  # åªé¡¯ç¤ºå‰20å€‹
                writer.writerow([img_id, int(pred), f'{conf:.6f}'])
        print(f"   è©³ç´°æ¸…å–®å·²å„²å­˜è‡³: {low_conf_file}")
    else:
        print(f"\nâœ… å¤ªæ£’äº†ï¼æ‰€æœ‰é æ¸¬çš„ä¿¡å¿ƒåº¦éƒ½é«˜æ–¼ {low_conf_threshold}")
    
    print("=" * 60)
    
    return all_predictions, all_confidences

def main():
    """ä¸»ç¨‹å¼"""
    print("=" * 60)
    print("ğŸ¯ å¤šæ¨¡å‹é›†æˆè©•ä¼°å·¥å…·")
    print("=" * 60)
    
    # ç²å–æ‰€æœ‰æ¨¡å‹
    model_paths = get_all_models_from_directory(models_dir='models', max_models=None, min_models=2)
    
    if len(model_paths) < 2:
        print("\nâŒ éœ€è¦è‡³å°‘2å€‹æ¨¡å‹æ‰èƒ½é€²è¡Œé›†æˆè©•ä¼°")
        print("ğŸ’¡ è«‹å…ˆè¨“ç·´å¤šå€‹ä¸åŒçš„æ¨¡å‹")
        return
    
    # é¸æ“‡é›†æˆç­–ç•¥
    print("\nğŸ² è«‹é¸æ“‡é›†æˆç­–ç•¥:")
    print("=" * 60)
    print("1. åŠ æ¬Šå¹³å‡ (Weighted Average) - æ¨è–¦")
    print("   æ‰€æœ‰æ¨¡å‹çš„è¼¸å‡ºåŠ æ¬Šå¹³å‡ï¼Œå¹³è¡¡å„æ¨¡å‹æ„è¦‹")
    print("\n2. æŠ•ç¥¨æ³• (Voting)")
    print("   æ¯å€‹æ¨¡å‹æŠ•ä¸€ç¥¨ï¼Œå¤šæ•¸æ±ºå®šæœ€çµ‚é æ¸¬")
    print("\n3. æœ€é«˜ä¿¡å¿ƒåº¦ (Max Confidence)")
    print("   é¸æ“‡ä¿¡å¿ƒåº¦æœ€é«˜çš„æ¨¡å‹çš„é æ¸¬çµæœ")
    print("=" * 60)
    
    strategy_map = {
        '1': 'weighted_average',
        '2': 'voting',
        '3': 'max_confidence'
    }
    
    while True:
        try:
            choice = input("\nè«‹é¸æ“‡ç­–ç•¥ (1-3) [é è¨­=1]: ").strip()
            if choice == '':
                choice = '1'
            
            if choice in strategy_map:
                strategy = strategy_map[choice]
                print(f"âœ… é¸æ“‡ç­–ç•¥: {strategy}")
                break
            else:
                print("âš ï¸  è«‹è¼¸å…¥ 1ã€2 æˆ– 3")
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç¨‹å¼çµæŸ")
            return
    
    # æª¢æ¸¬ä¸¦é¸æ“‡è¨ˆç®—è¨­å‚™
    available_devices = detect_available_devices()
    
    print("\nğŸ’» è«‹é¸æ“‡è¨ˆç®—è¨­å‚™:")
    print("=" * 60)
    
    for i, (device_id, device_name) in enumerate(available_devices, 1):
        prefix = "ğŸš€" if 'npu' in device_id.lower() or 'dml' in device_id.lower() or 'mps' in device_id.lower() else \
                 "âš¡" if 'cuda' in device_id else "ğŸ’»"
        suffix = " - æ¨è–¦" if i == 1 and device_id != 'cpu' else ""
        print(f"{i}. {prefix} {device_name}{suffix}")
    
    print("=" * 60)
    
    while True:
        try:
            choice = input(f"\nè«‹é¸æ“‡è¨­å‚™ (1-{len(available_devices)}) [é è¨­=1]: ").strip()
            if choice == '':
                choice = '1'
            
            idx = int(choice) - 1
            if 0 <= idx < len(available_devices):
                device_str, device_name = available_devices[idx]
                print(f"âœ… é¸æ“‡è¨­å‚™: {device_name}")
                break
            else:
                print(f"âš ï¸  è«‹è¼¸å…¥ 1-{len(available_devices)}")
        except ValueError:
            print("âš ï¸  è«‹è¼¸å…¥æœ‰æ•ˆæ•¸å­—")
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç¨‹å¼çµæŸ")
            return
    
    # å¦‚æœé¸æ“‡äº† ONNX DML è¨­å‚™ï¼Œè©¢å•æ˜¯å¦ä½¿ç”¨ NPU åŠ é€Ÿ
    use_onnx_npu = False
    if device_str == 'onnx_dml':
        print("\nğŸš€ AMD Ryzen AI NPU åŠ é€Ÿ")
        print("=" * 60)
        print("æ˜¯å¦å•Ÿç”¨ ONNX Runtime DirectML NPU ç¡¬é«”åŠ é€Ÿï¼Ÿ")
        print("1. âœ… æ˜¯ - ä½¿ç”¨ ONNX Runtime DirectML (æ¨è–¦)")
        print("2. âŒ å¦ - ä½¿ç”¨ PyTorch CPU æ¨¡å¼")
        print("=" * 60)
        
        while True:
            try:
                npu_choice = input("\nè«‹é¸æ“‡ (1-2) [é è¨­=1]: ").strip()
                if npu_choice == '' or npu_choice == '1':
                    use_onnx_npu = True
                    print("âœ… å·²å•Ÿç”¨ ONNX Runtime NPU åŠ é€Ÿ")
                    print("ğŸ’¡ æ¨¡å‹å°‡è½‰æ›ç‚º ONNX æ ¼å¼ä¸¦åœ¨ NPU ä¸ŠåŸ·è¡Œ")
                    device_str = 'cpu'  # PyTorch ç«¯ä½¿ç”¨ CPU è¼‰å…¥æ¨¡å‹
                    break
                elif npu_choice == '2':
                    use_onnx_npu = False
                    print("âœ… ä½¿ç”¨ PyTorch CPU æ¨¡å¼")
                    device_str = 'cpu'
                    break
                else:
                    print("âš ï¸  è«‹è¼¸å…¥ 1 æˆ– 2")
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç¨‹å¼çµæŸ")
                return
    
    # é–‹å§‹è©•ä¼°
    print("\n" + "=" * 60)
    print("ğŸš€ é–‹å§‹å¤šæ¨¡å‹é›†æˆè©•ä¼°")
    print("=" * 60)
    
    evaluate_multi_models(
        model_paths=model_paths,
        test_csv='archive/tw_food_101/tw_food_101_test_list.csv',
        test_img_dir='archive/tw_food_101/test',
        num_classes=101,
        batch_size=32,
        img_size=224,
        device_str=device_str,
        strategy=strategy,
        use_onnx_npu=use_onnx_npu
    )
    
    print("\n" + "=" * 60)
    print("ğŸ‰ è©•ä¼°å®Œæˆï¼")
    print("=" * 60)

if __name__ == '__main__':
    main()