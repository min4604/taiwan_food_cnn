"""
PyTorch GPU é©…å‹•å•é¡Œè¨ºæ–·å·¥å…·
æ­¤è…³æœ¬æª¢æ¸¬ GPU é©…å‹•ã€å…¼å®¹æ€§å’Œæ€§èƒ½å•é¡Œï¼Œç‰¹åˆ¥æ˜¯ CUDA èˆ‡ PyTorch çš„é…åˆå•é¡Œ
"""

import torch
import torch.nn as nn
import torch.optim as optim
import time
import sys
import os

def print_header(title):
    print("\n" + "=" * 70)
    print(title)
    print("=" * 70)

def print_section(title):
    print("\n" + "-" * 70)
    print(title)
    print("-" * 70)

def check_cuda_availability():
    print_header("ğŸ“‹ åŸºæœ¬ CUDA ä¿¡æ¯")
    
    # æª¢æŸ¥ CUDA æ˜¯å¦å¯ç”¨
    cuda_available = torch.cuda.is_available()
    print(f"CUDA æ˜¯å¦å¯ç”¨: {'âœ…' if cuda_available else 'âŒ'} {cuda_available}")
    
    if not cuda_available:
        print("\nâŒ ç„¡æ³•ä½¿ç”¨ CUDAï¼")
        print("å¯èƒ½çš„åŸå› :")
        print("1. NVIDIA é©…å‹•æœªå®‰è£æˆ–éæœŸ")
        print("2. CUDA å·¥å…·åŒ…æœªå®‰è£")
        print("3. PyTorch å®‰è£çš„æ˜¯ CPU ç‰ˆæœ¬")
        print("\nè§£æ±ºæ–¹æ¡ˆ:")
        print("1. æ›´æ–° NVIDIA é©…å‹•åˆ°æœ€æ–°ç‰ˆæœ¬")
        print("2. é‡æ–°å®‰è£å¸¶ CUDA æ”¯æŒçš„ PyTorch ç‰ˆæœ¬:")
        print("   pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
        return False

    # CUDA ä¿¡æ¯
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    print(f"cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version() if hasattr(torch.backends.cudnn, 'version') else 'æœªçŸ¥'}")
    
    # GPU ä¿¡æ¯
    device_count = torch.cuda.device_count()
    print(f"å¯ç”¨ GPU æ•¸é‡: {device_count}")
    
    for i in range(device_count):
        props = torch.cuda.get_device_properties(i)
        name = torch.cuda.get_device_name(i)
        mem_gb = props.total_memory / 1024**3
        print(f"\nGPU {i}: {name}")
        print(f"  ç¸½è¨˜æ†¶é«”: {mem_gb:.1f} GB")
        print(f"  CUDA è¨ˆç®—èƒ½åŠ›: {props.major}.{props.minor}")
        print(f"  å¤šè™•ç†å™¨æ•¸é‡: {props.multi_processor_count}")
    
    return True

def verify_cuda_operations():
    print_header("ğŸ§ª CUDA æ“ä½œæ¸¬è©¦")
    
    try:
        # å‰µå»ºæ¸¬è©¦å¼µé‡
        print("å‰µå»º GPU å¼µé‡...", end="")
        x = torch.randn(100, 100, device="cuda")
        print(" âœ…")
        
        # æ¸¬è©¦åŸºæœ¬é‹ç®—
        print("æ¸¬è©¦ GPU é‹ç®—...", end="")
        y = x + 2
        z = y * y
        print(f" âœ… (çµæœåœ¨ {z.device})")
        
        # æ¸¬è©¦ GPU å’Œ CPU å¼µé‡äº’è½‰
        print("æ¸¬è©¦ GPU<->CPU å‚³è¼¸...", end="")
        z_cpu = z.cpu()
        z_gpu = z_cpu.cuda()
        print(f" âœ… (çµæœåœ¨ {z_gpu.device})")
        
        return True
        
    except Exception as e:
        print(f"\nâŒ CUDA æ“ä½œæ¸¬è©¦å¤±æ•—: {e}")
        print("\nå¯èƒ½çš„åŸå› :")
        print("1. CUDA é©…å‹•èˆ‡ PyTorch ç‰ˆæœ¬ä¸å…¼å®¹")
        print("2. GPU è¨˜æ†¶é«”ä¸è¶³")
        print("3. GPU ç¡¬é«”å•é¡Œ")
        return False

def measure_performance_comparison():
    print_header("âš¡ GPU vs CPU æ€§èƒ½å°æ¯”")
    
    sizes = [(1000, 1000), (2000, 2000), (4000, 4000)]
    
    for size in sizes:
        m, n = size
        print(f"\nçŸ©é™£å¤§å°: {m}x{n}")
        
        # CPU æ¸¬è©¦
        try:
            a_cpu = torch.randn(m, n)
            b_cpu = torch.randn(m, n)
            
            # é ç†±
            _ = torch.matmul(a_cpu, b_cpu)
            
            # è¨ˆæ™‚
            start = time.time()
            c_cpu = torch.matmul(a_cpu, b_cpu)
            cpu_time = time.time() - start
            print(f"CPU æ™‚é–“: {cpu_time*1000:.2f} ms")
        except Exception as e:
            print(f"CPU æ¸¬è©¦å¤±æ•—: {e}")
            continue
        
        # GPU æ¸¬è©¦
        try:
            a_gpu = torch.randn(m, n, device="cuda")
            b_gpu = torch.randn(m, n, device="cuda")
            
            # é ç†±
            _ = torch.matmul(a_gpu, b_gpu)
            torch.cuda.synchronize()
            
            # è¨ˆæ™‚
            start = time.time()
            c_gpu = torch.matmul(a_gpu, b_gpu)
            torch.cuda.synchronize()  # ç­‰å¾… GPU å®Œæˆ
            gpu_time = time.time() - start
            print(f"GPU æ™‚é–“: {gpu_time*1000:.2f} ms")
            
            # è¨ˆç®—åŠ é€Ÿæ¯”
            speedup = cpu_time / gpu_time
            print(f"åŠ é€Ÿæ¯”: {speedup:.2f}x")
            
            if speedup < 1:
                print("âš ï¸  è­¦å‘Š: GPU æ¯” CPU æ…¢ï¼å¯èƒ½å­˜åœ¨é©…å‹•å•é¡Œ")
                if m < 2000:
                    print("     (å°çŸ©é™£æ¸¬è©¦å° GPU ä¸åˆ©ï¼Œç¨å¾Œå°‡å˜—è©¦æ›´å¤§çŸ©é™£)")
                else:
                    print("     å³ä½¿å°å¤§çŸ©é™£æ¸¬è©¦ï¼ŒGPU ä»è¼ƒæ…¢ï¼Œé€™è¡¨æ˜å­˜åœ¨åš´é‡å•é¡Œ")
            elif speedup < 5 and m >= 2000:
                print("âš ï¸  è­¦å‘Š: GPU åŠ é€Ÿæ¯”è¼ƒä½ï¼Œå¯èƒ½æœªå……åˆ†åˆ©ç”¨")
            else:
                print("âœ… GPU åŠ é€Ÿæ­£å¸¸")
                
        except Exception as e:
            print(f"GPU æ¸¬è©¦å¤±æ•—: {e}")
    
def test_cnn_training():
    print_header("ğŸ”„ CNN è¨“ç·´æ¸¬è©¦")
    
    # å®šç¾©ç°¡å–® CNN
    class SimpleCNN(nn.Module):
        def __init__(self):
            super(SimpleCNN, self).__init__()
            self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
            self.conv2 = nn.Conv2d(16, 32, 3, padding=1)
            self.pool = nn.MaxPool2d(2)
            self.fc = nn.Linear(32 * 56 * 56, 10)
            self.relu = nn.ReLU()
            
        def forward(self, x):
            x = self.pool(self.relu(self.conv1(x)))
            x = self.pool(self.relu(self.conv2(x)))
            x = x.view(x.size(0), -1)
            return self.fc(x)
    
    batch_size = 8
    
    try:
        # å‰µå»ºæ¨¡å‹
        print("å‰µå»º CNN æ¨¡å‹...")
        model_cpu = SimpleCNN()
        model_gpu = SimpleCNN().cuda()
        
        # å‰µå»ºå„ªåŒ–å™¨
        optimizer_cpu = optim.Adam(model_cpu.parameters(), lr=0.001)
        optimizer_gpu = optim.Adam(model_gpu.parameters(), lr=0.001)
        
        # å‰µå»ºæå¤±å‡½æ•¸
        criterion = nn.CrossEntropyLoss()
        criterion_gpu = nn.CrossEntropyLoss().cuda()
        
        # å‰µå»ºå‡æ•¸æ“š
        print("å‰µå»ºæ¸¬è©¦æ•¸æ“š...")
        inputs_cpu = torch.randn(batch_size, 3, 224, 224)
        targets_cpu = torch.randint(0, 10, (batch_size,))
        inputs_gpu = inputs_cpu.cuda()
        targets_gpu = targets_cpu.cuda()
        
        # CPU è¨“ç·´æ­¥é©Ÿ
        print("\næ¸¬è©¦ CPU è¨“ç·´...")
        model_cpu.train()
        start = time.time()
        
        optimizer_cpu.zero_grad()
        outputs_cpu = model_cpu(inputs_cpu)
        loss_cpu = criterion(outputs_cpu, targets_cpu)
        loss_cpu.backward()
        optimizer_cpu.step()
        
        cpu_time = time.time() - start
        print(f"CPU è¨“ç·´æ­¥é©Ÿ: {cpu_time*1000:.2f} ms")
        print(f"Loss: {loss_cpu.item():.4f}")
        
        # GPU è¨“ç·´æ­¥é©Ÿ
        print("\næ¸¬è©¦ GPU è¨“ç·´...")
        model_gpu.train()
        torch.cuda.synchronize()
        start = time.time()
        
        optimizer_gpu.zero_grad()
        outputs_gpu = model_gpu(inputs_gpu)
        loss_gpu = criterion_gpu(outputs_gpu, targets_gpu)
        loss_gpu.backward()
        optimizer_gpu.step()
        
        torch.cuda.synchronize()
        gpu_time = time.time() - start
        print(f"GPU è¨“ç·´æ­¥é©Ÿ: {gpu_time*1000:.2f} ms")
        print(f"Loss: {loss_gpu.item():.4f}")
        
        # æ¯”è¼ƒ
        speedup = cpu_time / gpu_time
        print(f"\nGPU åŠ é€Ÿæ¯”: {speedup:.2f}x")
        
        if speedup < 1:
            print("âŒ GPU è¨“ç·´æ¯” CPU æ…¢ï¼é€™æ˜¯åš´é‡å•é¡Œ")
            print("   å¯èƒ½çš„åŸå› :")
            print("   1. GPU é©…å‹•å•é¡Œ")
            print("   2. PyTorch å’Œ CUDA ç‰ˆæœ¬ä¸åŒ¹é…")
            print("   3. GPU åŠŸç‡é™åˆ¶æˆ–éç†±")
        elif speedup < 3:
            print("âš ï¸  GPU åŠ é€Ÿæ¯”è¼ƒä½ï¼Œå¯èƒ½æœªå……åˆ†åˆ©ç”¨")
        else:
            print("âœ… GPU è¨“ç·´æ­£å¸¸åŠ é€Ÿ")
        
        return speedup > 1
        
    except Exception as e:
        print(f"\nâŒ CNN è¨“ç·´æ¸¬è©¦å¤±æ•—: {e}")
        return False

def check_cudnn():
    print_header("âš™ï¸ cuDNN é…ç½®æª¢æŸ¥")
    
    try:
        print(f"cuDNN å¯ç”¨: {'âœ…' if torch.backends.cudnn.is_available() else 'âŒ'}")
        print(f"cuDNN å·²å•Ÿç”¨: {'âœ…' if torch.backends.cudnn.enabled else 'âŒ'}")
        print(f"cuDNN benchmark æ¨¡å¼: {'âœ…' if torch.backends.cudnn.benchmark else 'âŒ'}")
        print(f"cuDNN ç¢ºå®šæ€§æ¨¡å¼: {'âœ…' if torch.backends.cudnn.deterministic else 'âŒ'}")
        
        if not torch.backends.cudnn.is_available():
            print("\nâŒ cuDNN ä¸å¯ç”¨ï¼é€™æœƒåš´é‡å½±éŸ¿å·ç©æ“ä½œæ€§èƒ½")
            print("å¯èƒ½çš„åŸå› :")
            print("1. cuDNN æœªå®‰è£")
            print("2. cuDNN èˆ‡ CUDA ç‰ˆæœ¬ä¸åŒ¹é…")
            print("\nè§£æ±ºæ–¹æ¡ˆ:")
            print("é‡æ–°å®‰è£å¸¶ cuDNN æ”¯æŒçš„ PyTorch:")
            print("pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121")
            return False
            
        if not torch.backends.cudnn.enabled:
            print("\nâš ï¸  cuDNN æœªå•Ÿç”¨ï¼å°‡é¡¯è‘—é™ä½æ€§èƒ½")
            print("è§£æ±ºæ–¹æ¡ˆ:")
            print("åœ¨ä»£ç¢¼ä¸­æ·»åŠ : torch.backends.cudnn.enabled = True")
            
        if not torch.backends.cudnn.benchmark:
            print("\nâš ï¸  cuDNN benchmark æ¨¡å¼æœªé–‹å•Ÿ")
            print("å°æ–¼å›ºå®šå°ºå¯¸è¼¸å…¥çš„è¨“ç·´ï¼Œå»ºè­°é–‹å•Ÿ benchmark æ¨¡å¼ä¾†æé«˜æ€§èƒ½")
            print("è§£æ±ºæ–¹æ¡ˆ:")
            print("åœ¨ä»£ç¢¼ä¸­æ·»åŠ : torch.backends.cudnn.benchmark = True")
            
        return torch.backends.cudnn.is_available() and torch.backends.cudnn.enabled
    
    except Exception as e:
        print(f"\nâŒ cuDNN æª¢æŸ¥å¤±æ•—: {e}")
        return False

def check_memory_issues():
    print_header("ğŸ’¾ GPU è¨˜æ†¶é«”æ¸¬è©¦")
    
    try:
        # ç²å– GPU ç¸½è¨˜æ†¶é«”
        total_memory = torch.cuda.get_device_properties(0).total_memory
        total_memory_gb = total_memory / 1024**3
        print(f"GPU ç¸½è¨˜æ†¶é«”: {total_memory_gb:.2f} GB")
        
        # ç•¶å‰è¨˜æ†¶é«”ä½¿ç”¨
        memory_allocated = torch.cuda.memory_allocated(0)
        memory_reserved = torch.cuda.memory_reserved(0)
        print(f"å·²åˆ†é…è¨˜æ†¶é«”: {memory_allocated/1024**2:.2f} MB")
        print(f"å·²ä¿ç•™è¨˜æ†¶é«”: {memory_reserved/1024**2:.2f} MB")
        
        # å˜—è©¦åˆ†é…ä¸¦é‡‹æ”¾è¨˜æ†¶é«”
        print("\næ¸¬è©¦è¨˜æ†¶é«”åˆ†é…...")
        try:
            # å˜—è©¦åˆ†é… 1GB è¨˜æ†¶é«”
            test_size = min(1 * 1024**3, int(total_memory * 0.7))  # åˆ†é… 1GB æˆ– 70% è¨˜æ†¶é«”ï¼ˆå–å°è€…ï¼‰
            test_size_mb = test_size / 1024**2
            print(f"å˜—è©¦åˆ†é… {test_size_mb:.2f} MB è¨˜æ†¶é«”...", end="")
            
            x = torch.empty(test_size // 4, device='cuda')  # 4 bytes per float
            print(" âœ…")
            
            del x
            torch.cuda.empty_cache()
            print("è¨˜æ†¶é«”é‡‹æ”¾æˆåŠŸ âœ…")
            
            # å†æ¬¡æª¢æŸ¥è¨˜æ†¶é«”
            memory_allocated_after = torch.cuda.memory_allocated(0)
            print(f"é‡‹æ”¾å¾Œå·²åˆ†é…è¨˜æ†¶é«”: {memory_allocated_after/1024**2:.2f} MB")
        
        except Exception as e:
            print(f"\nâŒ è¨˜æ†¶é«”åˆ†é…å¤±æ•—: {e}")
            print("å¯èƒ½çš„åŸå› :")
            print("1. GPU è¨˜æ†¶é«”ä¸è¶³")
            print("2. å…¶ä»–ç¨‹åºä½”ç”¨äº† GPU è¨˜æ†¶é«”")
            print("3. CUDA é©…å‹•å•é¡Œ")
            return False
            
        return True
        
    except Exception as e:
        print(f"\nâŒ GPU è¨˜æ†¶é«”æ¸¬è©¦å¤±æ•—: {e}")
        return False

def check_train_pytorch_file():
    print_header("ğŸ“„ train_pytorch.py æª¢æŸ¥")
    
    file_path = "train_pytorch.py"
    if not os.path.exists(file_path):
        print(f"âŒ æ‰¾ä¸åˆ° {file_path}")
        return False
        
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
            
        # æª¢æŸ¥é—œéµ GPU ä»£ç¢¼
        checks = {
            "torch.cuda.is_available()": "CUDA å¯ç”¨æ€§æª¢æŸ¥",
            "device = torch.device": "è¨­å‚™é¸æ“‡ä»£ç¢¼",
            ".to(device)": "å°‡æ¨¡å‹/æ•¸æ“šç§»è‡³è¨­å‚™",
            "torch.backends.cudnn": "cuDNN é…ç½®"
        }
        
        print("æª¢æŸ¥é—œéµ GPU ä»£ç¢¼:")
        for code, desc in checks.items():
            if code in content:
                print(f"âœ… {desc} å­˜åœ¨")
            else:
                print(f"âŒ {desc} ç¼ºå¤±")
        
        # æª¢æŸ¥æ½›åœ¨å•é¡Œ
        issues = {
            ".cuda()": "ç›´æ¥ä½¿ç”¨ .cuda() è€Œé .to(device)",
            "device='cuda'": "ç›´æ¥ç¡¬ç·¨ç¢¼ device='cuda'",
            "torch.device('cpu')": "å¯èƒ½å¼·åˆ¶ä½¿ç”¨ CPU"
        }
        
        found_issues = False
        print("\næª¢æŸ¥æ½›åœ¨å•é¡Œ:")
        for issue, desc in issues.items():
            if issue in content:
                print(f"âš ï¸  {desc}")
                found_issues = True
                
        if not found_issues:
            print("âœ… æœªç™¼ç¾æ˜é¡¯å•é¡Œ")
            
        return True
        
    except Exception as e:
        print(f"âŒ æª¢æŸ¥å¤±æ•—: {e}")
        return False

def fix_train_pytorch():
    print_header("ğŸ”§ ä¿®å¾©å»ºè­°")
    
    print("åŸºæ–¼æ¸¬è©¦çµæœï¼Œé€™è£¡æ˜¯ä¿®å¾© GPU è¨“ç·´çš„å»ºè­°ï¼š\n")
    
    print("1. åœ¨ train_pytorch.py é–‹é ­æ·»åŠ  GPU å¼·åˆ¶æª¢æ¸¬ï¼š")
    print("""
    # å¼·åˆ¶å•Ÿç”¨ CUDA å’Œ cuDNN
    if torch.cuda.is_available():
        torch.backends.cudnn.enabled = True
        torch.backends.cudnn.benchmark = True
    """)
    
    print("\n2. åœ¨ main() å‡½æ•¸ä¸­ä½¿ç”¨æ˜ç¢ºçš„ GPU è¨­ç½®ï¼š")
    print("""
    # ç›´æ¥æŒ‡å®š GPU
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    
    # æ¸¬è©¦ GPU åŠŸèƒ½
    if device.type == 'cuda':
        test_tensor = torch.randn(100, 100).to(device)
        result = test_tensor.sum().item()
        print(f"âœ… GPU æ¸¬è©¦æˆåŠŸ: {test_tensor.device}")
    """)
    
    print("\n3. åœ¨æ•¸æ“šåŠ è¼‰æ™‚æ·»åŠ  pin_memory å’Œ num_workersï¼š")
    print("""
    # æ•¸æ“šåŠ è¼‰å™¨
    pin_memory = True if device.type == 'cuda' else False
    num_workers = 4 if device.type == 'cuda' else 0
    
    train_loader = torch.utils.data.DataLoader(
        dataset, batch_size=batch_size, shuffle=True, 
        pin_memory=pin_memory, num_workers=num_workers
    )
    """)
    
    print("\n4. ç¢ºä¿æ­£ç¢ºå°‡æ¨¡å‹ç§»è‡³ GPUï¼š")
    print("""
    # æ¨¡å‹åˆå§‹åŒ–
    model = get_model(model_architecture, num_classes=num_classes)
    model = model.to(device)  # å…ˆç¢ºä¿ç§»è‡³æ­£ç¢ºè¨­å‚™
    
    # æ‰“å°ç¢ºèªä¿¡æ¯
    print(f"æ¨¡å‹ä½æ–¼: {next(model.parameters()).device}")
    """)
    
    print("\n5. åœ¨è¨“ç·´è¿´åœˆå‰æ·»åŠ  GPU è¨˜æ†¶é«”ä½¿ç”¨ç›£æ§ï¼š")
    print("""
    # é¡¯ç¤º GPU ä½¿ç”¨æƒ…æ³
    if device.type == 'cuda':
        allocated = torch.cuda.memory_allocated() / 1024**2
        reserved = torch.cuda.memory_reserved() / 1024**2
        print(f"GPU è¨˜æ†¶é«”: å·²åˆ†é… {allocated:.1f} MB, å·²ä¿ç•™ {reserved:.1f} MB")
    """)

def check_extensions_loaded():
    print_section("ğŸ” æª¢æŸ¥ PyTorch CUDA æ“´å±•")
    
    try:
        print("æª¢æŸ¥ CUDA ç®—å­æ˜¯å¦èƒ½æ­£ç¢ºåŠ è¼‰...")
        
        # å˜—è©¦ä½¿ç”¨ä¸€äº›æœƒè§¸ç™¼ CUDA æ“´å±•çš„æ“ä½œ
        if torch.cuda.is_available():
            # æª¢æŸ¥ cuDNN å·ç©
            x = torch.randn(2, 3, 10, 10).cuda()
            conv = nn.Conv2d(3, 5, 3).cuda()
            y = conv(x)
            
            # æª¢æŸ¥ CUDA RNN
            rnn = nn.GRU(10, 20, batch_first=True).cuda()
            h = torch.randn(1, 2, 10).cuda()
            y, _ = rnn(h)
            
            print("âœ… CUDA æ“´å±•æ¸¬è©¦é€šé")
            return True
            
    except Exception as e:
        print(f"âŒ CUDA æ“´å±•æ¸¬è©¦å¤±æ•—: {e}")
        print("å¯èƒ½çš„åŸå› :")
        print("1. CUDA å·¥å…·éˆå®‰è£éŒ¯èª¤")
        print("2. PyTorch ç·¨è­¯å•é¡Œ")
        return False

def prepare_batch_file():
    print_header("ğŸ› ï¸ å‰µå»ºä¿®å¾©è…³æœ¬")
    
    try:
        script_content = """@echo off
echo ====================================
echo PyTorch GPU ä¿®å¾©å·¥å…·
echo ====================================
echo.

echo æ­¥é©Ÿ 1: æª¢æŸ¥ NVIDIA é©…å‹•...
nvidia-smi
if %ERRORLEVEL% NEQ 0 (
    echo âŒ NVIDIA é©…å‹•æœªå®‰è£æˆ–æœ‰å•é¡Œ
    echo è«‹å…ˆå®‰è£æˆ–æ›´æ–° NVIDIA é©…å‹•
    pause
    exit /b
)

echo.
echo æ­¥é©Ÿ 2: å¸è¼‰ç¾æœ‰ PyTorch...
pip uninstall -y torch torchvision torchaudio
echo.

echo æ­¥é©Ÿ 3: å®‰è£ CUDA ç‰ˆæœ¬çš„ PyTorch...
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
echo.

echo æ­¥é©Ÿ 4: é©—è­‰å®‰è£...
python -c "import torch; print('CUDA å¯ç”¨:', torch.cuda.is_available()); print('PyTorch ç‰ˆæœ¬:', torch.__version__); print('CUDA ç‰ˆæœ¬:', torch.version.cuda if torch.cuda.is_available() else 'N/A')"
echo.

echo æ­¥é©Ÿ 5: æ¸…ç† PyTorch å¿«å–...
python -c "import torch; torch.cuda.empty_cache() if torch.cuda.is_available() else print('CUDA ä¸å¯ç”¨')"
echo.

echo ä¿®å¾©å®Œæˆï¼è«‹å˜—è©¦é‡æ–°é‹è¡Œ train_pytorch.py
pause
"""
        
        with open("fix_pytorch_gpu.bat", "w") as f:
            f.write(script_content)
        
        print("âœ… ä¿®å¾©æ‰¹è™•ç†æª”æ¡ˆå·²å‰µå»º: fix_pytorch_gpu.bat")
        print("   åŸ·è¡Œæ­¤æª”æ¡ˆå°‡å¸è¼‰ä¸¦é‡æ–°å®‰è£ PyTorch (CUDA ç‰ˆæœ¬)")
    
    except Exception as e:
        print(f"âŒ å‰µå»ºä¿®å¾©è…³æœ¬å¤±æ•—: {e}")

def main():
    print_header("ğŸš€ PyTorch GPU è¨ºæ–·å·¥å…·")
    print("æ­¤å·¥å…·å°‡å…¨é¢æª¢æ¸¬ PyTorch GPU é…ç½®å•é¡Œ\n")
    
    import time
    from datetime import datetime
    
    print(f"åŸ·è¡Œæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Python ç‰ˆæœ¬: {sys.version}")
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    
    # æª¢æŸ¥éšæ®µ
    cuda_ok = check_cuda_availability()
    
    if not cuda_ok:
        print("\nâŒ CUDA ä¸å¯ç”¨ï¼Œç„¡æ³•é€²è¡Œå¾ŒçºŒæ¸¬è©¦")
        prepare_batch_file()
        return

    ops_ok = verify_cuda_operations()
    
    if not ops_ok:
        print("\nâŒ åŸºæœ¬ CUDA æ“ä½œå¤±æ•—ï¼Œç„¡æ³•é€²è¡Œå¾ŒçºŒæ¸¬è©¦")
        prepare_batch_file()
        return
    
    cudnn_ok = check_cudnn()
    measure_performance_comparison()
    memory_ok = check_memory_issues()
    training_ok = test_cnn_training()
    extensions_ok = check_extensions_loaded()
    
    # æª¢æŸ¥ train_pytorch.py
    file_ok = check_train_pytorch_file()
    
    # ç¸½çµ
    print_header("ğŸ“Š è¨ºæ–·ç¸½çµ")
    
    print(f"CUDA åŸºç¤æª¢æŸ¥: {'âœ…' if cuda_ok else 'âŒ'}")
    print(f"CUDA æ“ä½œæ¸¬è©¦: {'âœ…' if ops_ok else 'âŒ'}")
    print(f"cuDNN é…ç½®æª¢æŸ¥: {'âœ…' if cudnn_ok else 'âŒ'}")
    print(f"GPU è¨˜æ†¶é«”æ¸¬è©¦: {'âœ…' if memory_ok else 'âŒ'}")
    print(f"CNN è¨“ç·´æ¸¬è©¦: {'âœ…' if training_ok else 'âŒ'}")
    print(f"CUDA æ“´å±•æ¸¬è©¦: {'âœ…' if extensions_ok else 'âŒ'}")
    
    if file_ok:
        print(f"train_pytorch.py æª¢æŸ¥: âœ…")
    elif file_ok is False:
        print(f"train_pytorch.py æª¢æŸ¥: âš ï¸  æœ‰å•é¡Œ")
    else:
        print(f"train_pytorch.py æª¢æŸ¥: â“ æœªæª¢æŸ¥")
    
    # å»ºè­°ä¿®å¾©æ–¹æ¡ˆ
    if not all([cuda_ok, ops_ok, cudnn_ok, memory_ok, training_ok, extensions_ok]):
        fix_train_pytorch()
        prepare_batch_file()
        
        print("\nâš ï¸  è¨ºæ–·ç™¼ç¾å•é¡Œï¼Œè«‹æŸ¥çœ‹ä¸Šè¿°å ±å‘Šå’Œä¿®å¾©å»ºè­°")
    else:
        print("\nâœ… æ‰€æœ‰æ¸¬è©¦é€šéï¼")
        print("å¦‚æœè¨“ç·´ä»ç„¶ä¸ä½¿ç”¨ GPUï¼Œè«‹æŸ¥çœ‹ä»¥ä¸‹å»ºè­°:")
        fix_train_pytorch()
    
    # å‰µå»ºä¿®å¾©æ‰¹è™•ç†æª”æ¡ˆ
    prepare_batch_file()

if __name__ == "__main__":
    main()