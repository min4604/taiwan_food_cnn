import torch
import torch.nn as nn
import torch.optim as optim
import torch.utils.data
from pytorch_model import get_model, freeze_backbone
from pytorch_data_loader import get_dataloaders
from tqdm import tqdm
import os

def check_gpu():
    """è©³ç´°æª¢æŸ¥ GPU å¯ç”¨æ€§"""
    print("=" * 60)
    print("ğŸ” GPU ç’°å¢ƒæª¢æ¸¬")
    print("=" * 60)
    
    # å˜—è©¦ç›´æ¥æ¸¬è©¦ CUDA æ˜¯å¦å¯ç”¨
    try:
        gpu_available = torch.cuda.is_available()
        # é€²ä¸€æ­¥é©—è­‰ï¼Œå˜—è©¦é€²è¡Œä¸€æ¬¡ CUDA æ“ä½œ
        if gpu_available:
            print("ğŸ“Š æ¸¬è©¦ CUDA åŠŸèƒ½...", end="")
            # ä½¿ç”¨ CUDA äº‹ä»¶æ¸¬è©¦ CUDA å¯¦éš›å¯ç”¨æ€§
            start_event = torch.cuda.Event(enable_timing=True)
            end_event = torch.cuda.Event(enable_timing=True)
            
            # åŸ·è¡ŒçŸ©é™£ä¹˜æ³•æ¸¬è©¦
            start_event.record()
            test_tensor1 = torch.rand(1000, 1000, device='cuda')
            test_tensor2 = torch.rand(1000, 1000, device='cuda')
            result = torch.mm(test_tensor1, test_tensor2)
            torch.cuda.synchronize()  # ç¢ºä¿è¨ˆç®—å®Œæˆ
            end_event.record()
            end_event.synchronize()
            elapsed_time = start_event.elapsed_time(end_event)
            
            print(f" âœ… æˆåŠŸ! è€—æ™‚: {elapsed_time:.2f} ms")
            
            # å¼·åˆ¶å•Ÿç”¨ cuDNN
            torch.backends.cudnn.enabled = True
            torch.backends.cudnn.benchmark = True
            
            torch.cuda.empty_cache()  # æ¸…ç†
            
    except Exception as e:
        print(f"\nâŒ å˜—è©¦ CUDA æ“ä½œæ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        gpu_available = False
    
    if gpu_available:
        device_count = torch.cuda.device_count()
        current_device = torch.cuda.current_device()
        device_name = torch.cuda.get_device_name(current_device)
        
        print(f"âœ… CUDA å¯ç”¨: {torch.version.cuda}")
        print(f"ğŸ¯ GPU æ•¸é‡: {device_count}")
        print(f"ğŸš€ ç•¶å‰ GPU: {device_name}")
        print(f"âœ… cuDNN å•Ÿç”¨: {torch.backends.cudnn.enabled}")
        print(f"âœ… cuDNN Benchmark: {torch.backends.cudnn.benchmark}")
        
        # é¡¯ç¤º GPU è¨˜æ†¶é«”è³‡è¨Š
        if device_count > 0:
            for i in range(device_count):
                props = torch.cuda.get_device_properties(i)
                memory = props.total_memory / 1024**3  # è½‰æ›ç‚º GB
                print(f"   GPU {i}: {props.name} ({memory:.1f} GB)")
                
        print(f"ğŸ”§ PyTorch ç‰ˆæœ¬: {torch.__version__}")
    else:
        print("âŒ CUDA ä¸å¯ç”¨")
        print("ğŸ’¡ å»ºè­°:")
        print("   1. ç¢ºèªæ‚¨çš„é›»è…¦æœ‰ NVIDIA GPU")
        print("   2. å®‰è£ NVIDIA é©…å‹•ç¨‹å¼")
        print("   3. åŸ·è¡Œ install_pytorch_gpu.bat å®‰è£ CUDA ç‰ˆæœ¬çš„ PyTorch")
        print(f"ğŸ”§ ç•¶å‰ PyTorch ç‰ˆæœ¬: {torch.__version__}")
    
    print("=" * 60)
    return gpu_available

def detect_model_architecture_from_file(model_path):
    """å¾æ¨¡å‹æª”æ¡ˆæª¢æ¸¬æ¶æ§‹é¡å‹"""
    filename = os.path.basename(model_path).lower()
    
    if 'efficientnet_b7' in filename:
        return 'efficientnet_b7'
    elif 'efficientnet_b3' in filename:
        return 'efficientnet_b3'
    elif 'convnext_tiny' in filename:
        return 'convnext_tiny'
    elif 'regnet_y' in filename:
        return 'regnet_y'
    elif 'vit' in filename:
        return 'vit'
    elif 'resnet50' in filename:
        return 'resnet50'
    else:
        # å˜—è©¦å¾æ¨¡å‹å…§å®¹æª¢æ¸¬
        try:
            state_dict = torch.load(model_path, map_location='cpu')
            keys = list(state_dict.keys())
            
            # æ ¹æ“š state_dict çš„éµå€¼æª¢æ¸¬æ¶æ§‹
            if any('features' in key for key in keys):
                if any('block' in key for key in keys):
                    return 'efficientnet_b3'  # EfficientNet ç‰¹å¾µ
                elif any('stages' in key for key in keys):
                    return 'convnext_tiny'    # ConvNeXt ç‰¹å¾µ
                else:
                    return 'efficientnet_b3'  # é è¨­ç‚º EfficientNet
            elif any('layer' in key for key in keys):
                return 'resnet50'             # ResNet ç‰¹å¾µ
            elif any('blocks' in key for key in keys):
                return 'vit'                  # ViT ç‰¹å¾µ
            else:
                return 'resnet50'             # é è¨­ç‚º ResNet50
        except:
            return 'resnet50'                 # é è¨­ç‚º ResNet50

def choose_model_architecture():
    """é¸æ“‡è¦ä½¿ç”¨çš„æ¨¡å‹æ¶æ§‹"""
    print("\n" + "=" * 60)
    print("ğŸ—ï¸  é¸æ“‡æ¨¡å‹æ¶æ§‹")
    print("=" * 60)
    
    models = {
        '1': ('resnet50', 'ResNet50 (åŸºç¤æ¨¡å‹ï¼Œé€Ÿåº¦å¿«)', '~23M', '~2GB', 'å¿«'),
        '2': ('efficientnet_b3', 'EfficientNet-B3 (æ¨è–¦ï¼Œæ•ˆèƒ½ä½³)', '~12M', '~3GB', 'ä¸­ç­‰'),
        '3': ('efficientnet_b7', 'EfficientNet-B7 (æœ€å¼·æ€§èƒ½ï¼Œéœ€å¤§é¡¯å­˜)', '~66M', '~8GB+', 'æ…¢'),
        '4': ('convnext_tiny', 'ConvNeXt-Tiny (ç¾ä»£æ¶æ§‹ï¼Œç²¾åº¦é«˜)', '~28M', '~3GB', 'ä¸­ç­‰'),
        '5': ('regnet_y', 'RegNet-Y (é«˜æ•ˆç¶²è·¯ï¼Œå¹³è¡¡æ€§å¥½)', '~4M', '~2GB', 'å¿«'),
        '6': ('vit', 'Vision Transformer (æ³¨æ„åŠ›æ©Ÿåˆ¶ï¼Œéœ€è¼ƒå¤§è³‡æ–™é›†)', '~86M', '~4GB', 'æ…¢')
    }
    
    for key, (model_name, description, params, memory, speed) in models.items():
        print(f"{key}. {description}")
        print(f"   åƒæ•¸é‡: {params} | é¡¯å­˜éœ€æ±‚: {memory} | é€Ÿåº¦: {speed}")
        if key == '3':  # EfficientNet-B7 è­¦å‘Š
            print("   âš ï¸  å»ºè­°: RTX 4060 8GB å¯èƒ½éœ€è¦æ¸›å°æ‰¹æ¬¡å¤§å°")
        elif key == '6':  # ViT è­¦å‘Š
            print("   ğŸ’¡ é©åˆ: å¤§å‹è³‡æ–™é›†ï¼Œéœ€è¦é•·æ™‚é–“è¨“ç·´")
    
    print("\nğŸ’¡ æç¤º: æ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ ImageNet é è¨“ç·´æ¬Šé‡")
    
    while True:
        try:
            choice = input("\nè«‹é¸æ“‡æ¨¡å‹æ¶æ§‹ (1-6) [é è¨­=2]: ").strip()
            if choice == '':
                choice = '2'
            if choice in models:
                selected_model, description, params, memory, speed = models[choice]
                print(f"âœ… é¸æ“‡äº†: {description}")
                
                # B7 ç‰¹æ®Šæé†’
                if choice == '3':
                    print("\nğŸ”¥ EfficientNet-B7 æ³¨æ„äº‹é …:")
                    print("   â€¢ éœ€è¦ 8GB+ é¡¯å­˜")
                    print("   â€¢ å»ºè­°æ‰¹æ¬¡å¤§å°: 8-16")
                    print("   â€¢ è¨“ç·´æ™‚é–“è¼ƒé•·ï¼Œä½†ç²¾åº¦æ›´é«˜")
                    confirm = input("   ç¢ºå®šä½¿ç”¨ B7 æ¨¡å‹? (y/n) [y]: ").lower()
                    if confirm == 'n':
                        continue
                
                return selected_model
            else:
                print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œè«‹è¼¸å…¥ 1-6")
        except KeyboardInterrupt:
            print("\nğŸš« å–æ¶ˆé¸æ“‡ï¼Œé€€å‡ºç¨‹å¼")
            exit(0)

def choose_training_strategy():
    """é¸æ“‡è¨“ç·´ç­–ç•¥"""
    print("\n" + "=" * 60)
    print("ğŸ¯ é¸æ“‡è¨“ç·´ç­–ç•¥")
    print("=" * 60)
    
    strategies = {
        '1': ('fine_tune', 'å¾®èª¿è¨“ç·´ (æ¨è–¦)', 'å…ˆå‡çµé è¨“ç·´å±¤è¨“ç·´åˆ†é¡å™¨ï¼Œå¾Œè§£å‡å…¨æ¨¡å‹å¾®èª¿'),
        '2': ('full_train', 'å…¨æ¨¡å‹è¨“ç·´', 'å¾é ­è¨“ç·´æ‰€æœ‰å±¤ï¼ˆéœ€è¦æ›´å¤šæ™‚é–“å’Œæ•¸æ“šï¼‰'),
        '3': ('freeze_train', 'å‡çµéª¨å¹¹è¨“ç·´', 'åªè¨“ç·´åˆ†é¡å™¨ï¼Œé è¨“ç·´å±¤ä¿æŒä¸è®Šï¼ˆé©åˆå°æ•¸æ“šé›†ï¼‰')
    }
    
    for key, (strategy, name, description) in strategies.items():
        print(f"{key}. {name}")
        print(f"   {description}")
    
    print("\nğŸ’¡ æ¨è–¦:")
    print("   - è³‡æ–™é›†è¼ƒå° (<5000å¼µ) â†’ é¸æ“‡ 3 (å‡çµéª¨å¹¹)")
    print("   - è³‡æ–™é›†ä¸­ç­‰ (5000-20000å¼µ) â†’ é¸æ“‡ 1 (å¾®èª¿è¨“ç·´)")
    print("   - è³‡æ–™é›†è¼ƒå¤§ (>20000å¼µ) â†’ é¸æ“‡ 2 (å…¨æ¨¡å‹è¨“ç·´)")
    
    while True:
        try:
            choice = input("\nè«‹é¸æ“‡è¨“ç·´ç­–ç•¥ (1-3) [é è¨­=1]: ").strip()
            if choice == '':
                choice = '1'
            
            if choice in strategies:
                selected_strategy, name, _ = strategies[choice]
                print(f"âœ… é¸æ“‡äº†: {name}")
                return selected_strategy
            else:
                print("âŒ ç„¡æ•ˆé¸æ“‡ï¼Œè«‹è¼¸å…¥ 1-3")
        except KeyboardInterrupt:
            print("\nğŸš« å–æ¶ˆé¸æ“‡ï¼Œé€€å‡ºç¨‹å¼")
            exit(0)

def choose_model_to_continue():
    """é¸æ“‡è¦ç¹¼çºŒè¨“ç·´çš„æ¨¡å‹ï¼Œä¸¦è‡ªå‹•æª¢æ¸¬æ¶æ§‹"""
    print("\n" + "=" * 60)
    print("ğŸ¯ é¸æ“‡è¨“ç·´æ¨¡å¼")
    print("=" * 60)
    
    # æª¢æŸ¥æ˜¯å¦æœ‰å·²ä¿å­˜çš„æ¨¡å‹
    models_dir = 'models'
    if not os.path.exists(models_dir):
        os.makedirs(models_dir)
        print("ğŸ“ å»ºç«‹ models ç›®éŒ„")
        model_architecture = choose_model_architecture()
        return None, 0, model_architecture
    
    # ç²å–æ‰€æœ‰ .pth æ¨¡å‹æª”æ¡ˆ
    model_files = [f for f in os.listdir(models_dir) if f.endswith('.pth')]
    
    if not model_files:
        print("ğŸ“‹ æ²’æœ‰æ‰¾åˆ°å·²ä¿å­˜çš„æ¨¡å‹ï¼Œå°‡å¾é ­é–‹å§‹è¨“ç·´")
        model_architecture = choose_model_architecture()
        return None, 0, model_architecture
    
    # é¡¯ç¤ºé¸é …
    print("è«‹é¸æ“‡è¨“ç·´æ¨¡å¼:")
    print("0. ğŸ†• å¾é ­é–‹å§‹è¨“ç·´ (æ–°æ¨¡å‹)")
    
    # é¡¯ç¤ºå¯ç”¨çš„æ¨¡å‹æª”æ¡ˆ
    for i, model_file in enumerate(model_files, 1):
        # å˜—è©¦å¾æª”åä¸­æå– epoch è³‡è¨Š
        if 'epoch' in model_file:
            print(f"{i}. ğŸ”„ ç¹¼çºŒè¨“ç·´: {model_file}")
        else:
            print(f"{i}. ğŸ”„ ç¹¼çºŒè¨“ç·´: {model_file}")
    
    print("-" * 60)
    
    # ç²å–ç”¨æˆ¶é¸æ“‡
    while True:
        try:
            choice = input(f"è«‹è¼¸å…¥é¸æ“‡ (0-{len(model_files)}): ").strip()
            choice = int(choice)
            
            if choice == 0:
                print("âœ… é¸æ“‡å¾é ­é–‹å§‹è¨“ç·´")
                # è®“ç”¨æˆ¶é¸æ“‡æ¶æ§‹
                model_architecture = choose_model_architecture()
                return None, 0, model_architecture
            elif 1 <= choice <= len(model_files):
                selected_model = model_files[choice - 1]
                print(f"âœ… é¸æ“‡ç¹¼çºŒè¨“ç·´: {selected_model}")
                
                # æª¢æ¸¬æ¨¡å‹æ¶æ§‹
                model_path = os.path.join(models_dir, selected_model)
                model_architecture = detect_model_architecture_from_file(model_path)
                print(f"ğŸ—ï¸  æª¢æ¸¬åˆ°æ¨¡å‹æ¶æ§‹: {model_architecture}")
                
                # å˜—è©¦å¾æª”åä¸­æå– epoch æ•¸
                start_epoch = 0
                try:
                    if 'epoch' in selected_model:
                        # å‡è¨­æª”åæ ¼å¼ç‚º taiwan_food_resnet50_epoch10.pth
                        epoch_part = selected_model.split('epoch')[1].split('.')[0]
                        start_epoch = int(epoch_part)
                        print(f"ğŸ“Š æª¢æ¸¬åˆ°å¾ç¬¬ {start_epoch} å€‹ epoch ç¹¼çºŒè¨“ç·´")
                    else:
                        print("âš ï¸  ç„¡æ³•å¾æª”åæª¢æ¸¬ epochï¼Œå°‡å¾ epoch 0 é–‹å§‹è¨ˆæ•¸")
                except:
                    print("âš ï¸  ç„¡æ³•è§£æ epoch è³‡è¨Šï¼Œå°‡å¾ epoch 0 é–‹å§‹è¨ˆæ•¸")
                
                return os.path.join(models_dir, selected_model), start_epoch, model_architecture
            else:
                print(f"âŒ ç„¡æ•ˆé¸æ“‡ï¼Œè«‹è¼¸å…¥ 0-{len(model_files)} ä¹‹é–“çš„æ•¸å­—")
        except ValueError:
            print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆçš„æ•¸å­—")
        except KeyboardInterrupt:
            print("\nğŸš« å–æ¶ˆé¸æ“‡ï¼Œé€€å‡ºç¨‹å¼")
            exit(0)

def main():
    # è³‡æ–™è·¯å¾‘
    train_csv = 'archive/tw_food_101/tw_food_101_train.csv'
    # æ³¨æ„ï¼šæ¸¬è©¦é›†ä¸åƒèˆ‡è¨“ç·´ï¼Œåƒ…ç”¨æ–¼æœ€çµ‚è©•ä¼°
    test_csv = 'archive/tw_food_101/tw_food_101_test_list.csv'  
    train_img_dir = 'archive/tw_food_101/train'
    test_img_dir = 'archive/tw_food_101/test'
    num_classes = 101
    total_epochs = 50
    lr = 1e-3
    img_size = 224

    # ç¢ºä¿ CUDA å¯ç”¨æ€§
    print("\n" + "=" * 60)
    print("ğŸ”§ è¨­å‚™é…ç½®é©—è­‰")
    print("=" * 60)
    
    # å¼·åˆ¶é‡æ–°æª¢æ¸¬ CUDA ä¸¦ç›´æ¥æŒ‡å®šè¨­å‚™
    if torch.cuda.is_available():
        # ç›´æ¥å¼·åˆ¶ä½¿ç”¨ CUDAï¼Œä¸ç¶“é check_gpu()
        device = torch.device('cuda:0')
        print(f"âœ… å¼·åˆ¶è¨­ç½®è¨­å‚™ç‚º CUDA: {device}")
        
        # é¡¯ç¤º GPU ä¿¡æ¯
        gpu_count = torch.cuda.device_count()
        current_gpu = torch.cuda.current_device()
        gpu_name = torch.cuda.get_device_name(current_gpu)
        print(f"ï¿½ GPU æ•¸é‡: {gpu_count}")
        print(f"ğŸ“ ç•¶å‰ GPU: {current_gpu} - {gpu_name}")
        print(f"ğŸ“ CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        
        # å¼·åˆ¶åŸ·è¡Œä¸€æ¬¡ CUDA æ“ä½œ
        try:
            print("â³ åŸ·è¡Œ CUDA æ¸¬è©¦...")
            x = torch.randn(100, 100, device=device)
            y = torch.randn(100, 100, device=device)
            z = torch.matmul(x, y)
            result = z.sum().item()
            print(f"âœ… CUDA æ¸¬è©¦æˆåŠŸ! çµæœ: {result:.4f}")
            print(f"âœ… å¼µé‡è¨­å‚™: {z.device}")
            
            # æ¸…ç†è¨˜æ†¶é«”
            del x, y, z
            torch.cuda.empty_cache()
        except Exception as e:
            print(f"âŒ GPU æ¸¬è©¦å¤±æ•—: {str(e)}")
            print("âš ï¸  ç„¡æ³•ä½¿ç”¨ GPUï¼Œé™ç´šä½¿ç”¨ CPU")
            device = torch.device('cpu')
    else:
        device = torch.device('cpu')
        print("âš ï¸  ç„¡æ³•ä½¿ç”¨ GPUï¼Œè¨­å‚™è¨­ç½®ç‚º CPU")
    
    print("\nğŸ“ æœ€çµ‚è¨­å‚™è¨­ç½®: " + str(device) + f" ({device.type.upper()})")
    
    # è¨­ç½® CUDA æ€§èƒ½å„ªåŒ–
    if device.type == 'cuda':
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        print(f"âœ… cuDNN benchmark æ¨¡å¼å·²å•Ÿç”¨")
    
    print("=" * 60 + "\n")

    # é¸æ“‡è¦ç¹¼çºŒè¨“ç·´çš„æ¨¡å‹ï¼ˆé€™æœƒè‡ªå‹•æª¢æ¸¬æ¶æ§‹ï¼‰
    model_path, start_epoch, model_architecture = choose_model_to_continue()
    remaining_epochs = total_epochs - start_epoch
    
    # æ ¹æ“šæ¨¡å‹æ¶æ§‹èª¿æ•´æ‰¹æ¬¡å¤§å°
    if device.type == 'cuda':
        if model_architecture == 'efficientnet_b7':
            batch_size = 8   # B7 éœ€è¦æ›´å°æ‰¹æ¬¡
            print(f"ğŸ”¥ EfficientNet-B7 æª¢æ¸¬åˆ°ï¼Œèª¿æ•´æ‰¹æ¬¡å¤§å°ç‚º {batch_size}")
        elif model_architecture == 'vit':
            batch_size = 16  # ViT ä¹Ÿéœ€è¦è¼ƒå°æ‰¹æ¬¡
            print(f"ğŸ¤– ViT æª¢æ¸¬åˆ°ï¼Œèª¿æ•´æ‰¹æ¬¡å¤§å°ç‚º {batch_size}")
        else:
            batch_size = 32  # å…¶ä»–æ¨¡å‹ä½¿ç”¨æ¨™æº–æ‰¹æ¬¡
            print(f"ğŸš€ ä½¿ç”¨ GPU è¨“ç·´ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
    else:
        batch_size = 8   # CPU ä½¿ç”¨å°æ‰¹æ¬¡
        print(f"ğŸ’» ä½¿ç”¨ CPU è¨“ç·´ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
    
    # é¸æ“‡è¨“ç·´ç­–ç•¥ï¼ˆåªåœ¨æ–°è¨“ç·´æ™‚è©¢å•ï¼‰
    if model_path is None:
        training_strategy = choose_training_strategy()
    else:
        # ç¹¼çºŒè¨“ç·´æ™‚ï¼Œé è¨­ä½¿ç”¨å¾®èª¿ç­–ç•¥
        training_strategy = 'fine_tune'
        print(f"ğŸ”„ ç¹¼çºŒè¨“ç·´æ¨¡å¼ï¼Œä½¿ç”¨å¾®èª¿ç­–ç•¥")

    # èª¿æ•´ DataLoader åƒæ•¸ï¼Œç¢ºä¿æ•¸æ“šåŠ è¼‰é«˜æ•ˆ
    num_workers = 0
    pin_memory = False
    
    if device.type == 'cuda':
        num_workers = 4  # å¤šç·šç¨‹åŠ è¼‰æ•¸æ“š
        pin_memory = True  # æ•¸æ“šç›´æ¥åŠ è¼‰åˆ°å›ºå®šè¨˜æ†¶é«”ï¼ŒåŠ é€Ÿ GPU å‚³è¼¸
        print(f"âœ… æ•¸æ“šåŠ è¼‰å„ªåŒ–å·²å•Ÿç”¨: {num_workers} å·¥ä½œç·šç¨‹, pin_memory={pin_memory}")
    
    # DataLoader - åªä½¿ç”¨è¨“ç·´é›†é€²è¡Œè¨“ç·´å’Œé©—è­‰
    # æ¸¬è©¦é›†ä¸åƒèˆ‡ä»»ä½•è¨“ç·´éç¨‹
    train_loader, val_loader, _ = get_dataloaders(
        train_csv, test_csv, train_img_dir, test_img_dir, 
        batch_size=batch_size, 
        img_size=img_size,
        num_workers=num_workers,
        pin_memory=pin_memory
    )
    
    print("ğŸ“Š è³‡æ–™é›†è³‡è¨Š:")
    print(f"   è¨“ç·´é›†å¤§å°: {len(train_loader.dataset)} (ç”¨æ–¼è¨“ç·´)")
    print(f"   é©—è­‰é›†å¤§å°: {len(val_loader.dataset)} (å¾è¨“ç·´é›†åˆ†å‰²ï¼Œç”¨æ–¼é©—è­‰)")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   âš ï¸  æ¸¬è©¦é›†: ä¸åƒèˆ‡è¨“ç·´éç¨‹ï¼Œåƒ…ä¾›æœ€çµ‚è©•ä¼°ä½¿ç”¨")
    print()

    # æ ¹æ“šé¸æ“‡å‰µå»ºæ¨¡å‹ï¼ˆä½¿ç”¨é è¨“ç·´æ¬Šé‡ï¼‰
    print(f"ğŸ—ï¸  å‰µå»ºæ¨¡å‹: {model_architecture}")
    print(f"ğŸ“¦ è¼‰å…¥ ImageNet é è¨“ç·´æ¬Šé‡...")
    
    # å¼·åˆ¶æŒ‡å®š CPU å…ˆå‰µå»ºæ¨¡å‹ï¼Œç„¶å¾Œå†ç§»åˆ° GPU
    model = get_model(model_architecture, num_classes=num_classes, dropout_rate=0.3, pretrained=True)
    print(f"ğŸ“Œ æ¨¡å‹åˆå§‹åŒ–åœ¨: {next(model.parameters()).device}")
    
    # æ˜ç¢ºç§»è‡³ç›®æ¨™è¨­å‚™
    model = model.to(device)
    print(f"ğŸ“Œ æ¨¡å‹å·²ç§»è‡³: {next(model.parameters()).device}")
    
    # ç¢ºä¿æå¤±å‡½æ•¸ä¹Ÿåœ¨ç›¸åŒè¨­å‚™ä¸Š
    criterion = nn.CrossEntropyLoss().to(device)
    
    # è¼‰å…¥å·²å­˜åœ¨çš„æ¨¡å‹ï¼ˆå¦‚æœé¸æ“‡äº†ï¼‰
    if model_path:
        try:
            model.load_state_dict(torch.load(model_path, map_location=device))
            print(f"âœ… æˆåŠŸè¼‰å…¥æ¨¡å‹: {model_path}")
            print(f"ğŸ”„ å¾ epoch {start_epoch + 1} é–‹å§‹ç¹¼çºŒè¨“ç·´")
        except Exception as e:
            print(f"âŒ è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}")
            print("ğŸ†• å°‡å¾é ­é–‹å§‹è¨“ç·´")
            start_epoch = 0
            remaining_epochs = total_epochs
    else:
        print("ğŸ†• å¾é ­é–‹å§‹è¨“ç·´æ–°æ¨¡å‹")
    
    # æ ¹æ“šè¨“ç·´ç­–ç•¥è¨­å®šæ¨¡å‹
    print("\n" + "=" * 60)
    print(f"ğŸ¯ è¨“ç·´ç­–ç•¥: {training_strategy}")
    print("=" * 60)
    
    if training_strategy == 'freeze_train':
        # å‡çµéª¨å¹¹ï¼Œåªè¨“ç·´åˆ†é¡å™¨
        model = freeze_backbone(model, freeze=True)
        optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)
        print(f"ğŸ“š è¨“ç·´éšæ®µ: åªè¨“ç·´åˆ†é¡å™¨å±¤")
        
    elif training_strategy == 'fine_tune':
        # å¾®èª¿è¨“ç·´ï¼šåˆ†å…©éšæ®µ
        # éšæ®µ1: å‡çµéª¨å¹¹è¨“ç·´åˆ†é¡å™¨ (å‰ 20% epochs)
        # éšæ®µ2: è§£å‡å…¨æ¨¡å‹å¾®èª¿ (å¾Œ 80% epochs)
        freeze_epochs = max(5, total_epochs // 5)  # è‡³å°‘5å€‹epochç”¨æ–¼é è¨“ç·´åˆ†é¡å™¨
        
        if start_epoch < freeze_epochs:
            # éšæ®µ1: å‡çµéª¨å¹¹
            model = freeze_backbone(model, freeze=True)
            optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)
            print(f"ğŸ“š éšæ®µ1: å‡çµéª¨å¹¹è¨“ç·´åˆ†é¡å™¨ (epoch 1-{freeze_epochs})")
            print(f"ğŸ“š éšæ®µ2: å°‡åœ¨ epoch {freeze_epochs + 1} å¾Œè§£å‡å…¨æ¨¡å‹å¾®èª¿")
        else:
            # å·²ç¶“éäº†å‡çµéšæ®µï¼Œç›´æ¥å¾®èª¿
            model = freeze_backbone(model, freeze=False)
            optimizer = optim.Adam(model.parameters(), lr=lr * 0.1)  # ä½¿ç”¨è¼ƒå°å­¸ç¿’ç‡
            print(f"ğŸ“š éšæ®µ2: å…¨æ¨¡å‹å¾®èª¿ï¼ˆä½¿ç”¨è¼ƒå°å­¸ç¿’ç‡ï¼‰")
        
        fine_tune_strategy = True
        fine_tune_epoch_threshold = freeze_epochs
        
    else:  # full_train
        # å…¨æ¨¡å‹è¨“ç·´
        model = freeze_backbone(model, freeze=False)
        optimizer = optim.Adam(model.parameters(), lr=lr)
        print(f"ğŸ“š è¨“ç·´éšæ®µ: å…¨æ¨¡å‹è¨“ç·´")
        fine_tune_strategy = False
        fine_tune_epoch_threshold = 0
        
    print(f"ğŸ“ˆ è¨“ç·´è¨ˆç•«: å¾ epoch {start_epoch + 1} åˆ° epoch {total_epochs} (å…± {remaining_epochs} å€‹ epochs)")
    print("=" * 60)
    
    # é©—è­‰æ¨¡å‹å’Œæ•¸æ“šåœ¨æ­£ç¢ºçš„è¨­å‚™ä¸Š
    print("\nğŸ” æœ€çµ‚è¨­å‚™æª¢æŸ¥:")
    print(f"   æ¨¡å‹è¨­å‚™: {next(model.parameters()).device}")
    print(f"   ç›®æ¨™è¨­å‚™: {device}")
    if next(model.parameters()).device.type != device.type:
        print(f"   âš ï¸  è­¦å‘Š: æ¨¡å‹è¨­å‚™ä¸åŒ¹é…ï¼é‡æ–°ç§»å‹•æ¨¡å‹åˆ° {device}")
        model = model.to(device)
        print(f"   âœ… æ¨¡å‹å·²ç§»è‡³: {next(model.parameters()).device}")
    else:
        print(f"   âœ… æ¨¡å‹è¨­å‚™é…ç½®æ­£ç¢º")
    print()

    # Training loop
    for epoch in range(start_epoch, total_epochs):
        # å¾®èª¿ç­–ç•¥ï¼šåœ¨æŒ‡å®š epoch å¾Œè§£å‡ä¸¦èª¿æ•´å­¸ç¿’ç‡
        if training_strategy == 'fine_tune' and epoch == fine_tune_epoch_threshold:
            print("\n" + "=" * 60)
            print(f"ğŸ”“ è§£å‡æ¨¡å‹ï¼Œé–‹å§‹å…¨æ¨¡å‹å¾®èª¿ (epoch {epoch + 1})")
            print("=" * 60)
            model = freeze_backbone(model, freeze=False)
            # é‡æ–°å‰µå»ºå„ªåŒ–å™¨ï¼Œä½¿ç”¨è¼ƒå°å­¸ç¿’ç‡
            optimizer = optim.Adam(model.parameters(), lr=lr * 0.1)
            print(f"ğŸ“‰ èª¿æ•´å­¸ç¿’ç‡: {lr} â†’ {lr * 0.1}")
            print()
        
        # åœ¨æ¯å€‹ epoch é–‹å§‹æ™‚é©—è­‰è¨­å‚™ï¼ˆç¬¬ä¸€å€‹ epochï¼‰
        if epoch == start_epoch:
            print(f"ğŸ¯ Epoch {epoch+1} è¨­å‚™é©—è­‰:")
            print(f"   æ¨¡å‹åœ¨: {next(model.parameters()).device}")
        
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        # ç¬¬ä¸€å€‹ batch çš„è©³ç´°æª¢æŸ¥
        first_batch = True
        
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{total_epochs}", ncols=100)
        for batch_idx, (images, labels) in enumerate(pbar):
            images, labels = images.to(device), labels.to(device)
            
            # ç¬¬ä¸€å€‹ batch æ™‚æª¢æŸ¥æ•¸æ“šè¨­å‚™
            if first_batch and epoch == start_epoch:
                print(f"   æ•¸æ“šæ‰¹æ¬¡åœ¨: {images.device}")
                print(f"   æ¨™ç±¤åœ¨: {labels.device}")
                if device.type == 'cuda':
                    print(f"   GPU è¨˜æ†¶é«”å·²ä½¿ç”¨: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB")
                print()
                first_batch = False
            
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * images.size(0)
            _, predicted = outputs.max(1)
            correct += predicted.eq(labels).sum().item()
            total += labels.size(0)
            pbar.set_postfix({
                'loss': f'{running_loss/total:.4f}',
                'acc': f'{correct/total:.4f}'
            })
        print(f"è¨“ç·´ Loss: {running_loss/total:.4f} | è¨“ç·´ Acc: {correct/total:.4f}")

        # é©—è­‰
        model.eval()
        val_loss = 0.0
        val_correct = 0
        val_total = 0
        with torch.no_grad():
            val_pbar = tqdm(val_loader, desc="é©—è­‰ä¸­", ncols=80, leave=False)
            for images, labels in val_pbar:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                loss = criterion(outputs, labels)
                val_loss += loss.item() * images.size(0)
                _, predicted = outputs.max(1)
                val_correct += predicted.eq(labels).sum().item()
                val_total += labels.size(0)
                val_pbar.set_postfix({
                    'val_loss': f'{val_loss/val_total:.4f}',
                    'val_acc': f'{val_correct/val_total:.4f}'
                })
        
        print(f"é©—è­‰ Loss: {val_loss/val_total:.4f} | é©—è­‰ Acc: {val_correct/val_total:.4f}")
        
        # é¡¯ç¤º GPU è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³ï¼ˆå¦‚æœä½¿ç”¨ GPUï¼‰
        if device.type == 'cuda':
            memory_allocated = torch.cuda.memory_allocated(0) / 1024**2
            memory_reserved = torch.cuda.memory_reserved(0) / 1024**2
            print(f"ğŸ® GPU è¨˜æ†¶é«”: å·²åˆ†é… {memory_allocated:.2f} MB | å·²ä¿ç•™ {memory_reserved:.2f} MB")
        
        print("-" * 60)

        # ä¿å­˜æ¨¡å‹
        os.makedirs('models', exist_ok=True)
        model_filename = f'taiwan_food_{model_architecture}_epoch{epoch+1}.pth'
        torch.save(model.state_dict(), f'models/{model_filename}')
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: models/{model_filename}")
        print()

if __name__ == '__main__':
    main()
