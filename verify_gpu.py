"""
å¿«é€Ÿé©—è­‰ PyTorch GPU ç’°å¢ƒæ˜¯å¦æ­£ç¢ºé…ç½®
"""

import torch
import torch.nn as nn
import time
import os

def test_gpu():
    print("=" * 60)
    print("ğŸ” PyTorch GPU ç’°å¢ƒæ¸¬è©¦")
    print("=" * 60)
    
    # æª¢æŸ¥ CUDA æ˜¯å¦å¯ç”¨
    if not torch.cuda.is_available():
        print("âŒ CUDA ä¸å¯ç”¨")
        print("ğŸ’¡ å»ºè­°å®‰è£ GPU ç‰ˆæœ¬çš„ PyTorch:")
        print("pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
        return False
        
    # é¡¯ç¤º CUDA è³‡è¨Š
    print(f"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"âœ… CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    print(f"âœ… cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
    
    # é¡¯ç¤º GPU è³‡è¨Š
    device_count = torch.cuda.device_count()
    print(f"âœ… æ‰¾åˆ° {device_count} å€‹ GPU")
    
    for i in range(device_count):
        props = torch.cuda.get_device_properties(i)
        print(f"   GPU {i}: {props.name}")
        print(f"      è¨˜æ†¶é«”: {props.total_memory / 1024**3:.1f} GB")
        print(f"      è¨ˆç®—èƒ½åŠ›: {props.major}.{props.minor}")
    
    # é€²è¡Œä¸€æ¬¡ç°¡å–®çš„ GPU è¨ˆç®—æ¸¬è©¦
    print("\nğŸ“Š é€²è¡Œ GPU è¨ˆç®—æ¸¬è©¦...")
    device = torch.device('cuda')
    
    # 1. æ¸¬è©¦åŸºæœ¬æ“ä½œ
    try:
        # å»ºç«‹æ¸¬è©¦å¼µé‡
        x = torch.rand(1000, 1000, device=device)
        y = torch.rand(1000, 1000, device=device)
        
        # ç¢ºä¿ GPU é‹ç®—åŒæ­¥
        torch.cuda.synchronize()
        start_time = time.time()
        
        # çŸ©é™£ä¹˜æ³•æ¸¬è©¦
        z = torch.matmul(x, y)
        
        # ç¢ºä¿é‹ç®—å®Œæˆ
        torch.cuda.synchronize()
        gpu_time = time.time() - start_time
        
        print(f"âœ… GPU çŸ©é™£ä¹˜æ³•æ¸¬è©¦é€šéï¼Œè€—æ™‚: {gpu_time*1000:.2f} ms")
        print(f"   çµæœå¼µé‡ä½æ–¼: {z.device}")
        
        # æ¯”è¼ƒ CPU çš„é€Ÿåº¦
        x_cpu = x.cpu()
        y_cpu = y.cpu()
        start_time = time.time()
        z_cpu = torch.matmul(x_cpu, y_cpu)
        cpu_time = time.time() - start_time
        
        print(f"âœ… CPU çŸ©é™£ä¹˜æ³•æ¸¬è©¦ï¼Œè€—æ™‚: {cpu_time*1000:.2f} ms")
        print(f"âœ… GPU åŠ é€Ÿæ¯”: {cpu_time/gpu_time:.1f}x")
        
        if cpu_time / gpu_time < 1:
            print("âš ï¸  è­¦å‘Š: GPU é‹ç®—é€Ÿåº¦æ…¢æ–¼ CPUï¼Œå¯èƒ½æœ‰å•é¡Œï¼")
        
    except Exception as e:
        print(f"âŒ GPU çŸ©é™£é‹ç®—æ¸¬è©¦å¤±æ•—: {e}")
        return False
    
    # 2. æ¸¬è©¦ CNN æ¨¡å‹
    print("\nğŸ“Š é€²è¡Œ CNN æ¨¡å‹æ¸¬è©¦...")
    try:
        # å»ºç«‹ä¸€å€‹ç°¡å–®çš„ CNN æ¨¡å‹
        class SimpleCNN(nn.Module):
            def __init__(self):
                super(SimpleCNN, self).__init__()
                self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)
                self.relu = nn.ReLU()
                self.maxpool = nn.MaxPool2d(kernel_size=2)
                self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)
                self.fc = nn.Linear(32 * 56 * 56, 10)
            
            def forward(self, x):
                x = self.relu(self.conv1(x))
                x = self.maxpool(x)
                x = self.relu(self.conv2(x))
                x = self.maxpool(x)
                x = x.view(x.size(0), -1)
                x = self.fc(x)
                return x
        
        # å°‡æ¨¡å‹ç§»åˆ° GPU
        model = SimpleCNN().to(device)
        
        # æº–å‚™è¼¸å…¥è³‡æ–™
        batch_size = 16
        input_tensor = torch.rand(batch_size, 3, 224, 224, device=device)
        
        # æ¸¬è©¦å‰å‘å‚³æ’­
        torch.cuda.synchronize()
        start_time = time.time()
        
        with torch.no_grad():
            output = model(input_tensor)
        
        torch.cuda.synchronize()
        forward_time = time.time() - start_time
        
        print(f"âœ… CNN å‰å‘å‚³æ’­æ¸¬è©¦é€šéï¼Œè€—æ™‚: {forward_time*1000:.2f} ms")
        print(f"   è¼¸å…¥å½¢ç‹€: {input_tensor.shape}")
        print(f"   è¼¸å‡ºå½¢ç‹€: {output.shape}")
        print(f"   æ¨¡å‹ä½æ–¼: {next(model.parameters()).device}")
        
    except Exception as e:
        print(f"âŒ CNN æ¨¡å‹æ¸¬è©¦å¤±æ•—: {e}")
        return False
    
    # 3. æ¸¬è©¦è¨“ç·´è¿´åœˆ
    print("\nğŸ“Š é€²è¡Œè¨“ç·´è¿´åœˆæ¸¬è©¦...")
    try:
        criterion = nn.CrossEntropyLoss().to(device)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        
        # æ¨¡æ“¬è¨“ç·´
        model.train()
        torch.cuda.synchronize()
        start_time = time.time()
        
        # å‰å‘å‚³æ’­
        output = model(input_tensor)
        
        # è¨ˆç®—æå¤±
        target = torch.randint(0, 10, (batch_size,), device=device)
        loss = criterion(output, target)
        
        # åå‘å‚³æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        torch.cuda.synchronize()
        train_time = time.time() - start_time
        
        print(f"âœ… è¨“ç·´è¿´åœˆæ¸¬è©¦é€šéï¼Œè€—æ™‚: {train_time*1000:.2f} ms")
        print(f"   æå¤±å€¼: {loss.item():.4f}")
        print(f"   æå¤±ä½æ–¼: {loss.device}")
        
    except Exception as e:
        print(f"âŒ è¨“ç·´è¿´åœˆæ¸¬è©¦å¤±æ•—: {e}")
        return False
    
    # 4. GPU è¨˜æ†¶é«”æ¸¬è©¦
    print("\nğŸ“Š æª¢æŸ¥ GPU è¨˜æ†¶é«”ä½¿ç”¨...")
    try:
        memory_allocated = torch.cuda.memory_allocated() / (1024**2)
        memory_reserved = torch.cuda.memory_reserved() / (1024**2)
        total_memory = torch.cuda.get_device_properties(0).total_memory / (1024**2)
        
        print(f"âœ… GPU è¨˜æ†¶é«”å·²åˆ†é…: {memory_allocated:.1f} MB")
        print(f"âœ… GPU è¨˜æ†¶é«”å·²ä¿ç•™: {memory_reserved:.1f} MB")
        print(f"âœ… GPU ç¸½è¨˜æ†¶é«”: {total_memory:.1f} MB")
        print(f"âœ… ä½¿ç”¨ç‡: {memory_allocated/total_memory*100:.1f}%")
        
        # é‡‹æ”¾è¨˜æ†¶é«”
        del model, input_tensor, output, x, y, z
        torch.cuda.empty_cache()
        
        memory_after = torch.cuda.memory_allocated() / (1024**2)
        print(f"âœ… é‡‹æ”¾å¾Œè¨˜æ†¶é«”ä½¿ç”¨: {memory_after:.1f} MB")
        
    except Exception as e:
        print(f"âŒ è¨˜æ†¶é«”æ¸¬è©¦å¤±æ•—: {e}")
    
    print("\n" + "=" * 60)
    print("âœ… GPU æ¸¬è©¦å…¨éƒ¨é€šéï¼")
    print("=" * 60)
    return True

def fix_gpu_issues():
    """å˜—è©¦ä¿®å¾©å¸¸è¦‹çš„ GPU å•é¡Œ"""
    print("\nğŸ”§ å˜—è©¦ä¿®å¾© GPU å•é¡Œ...")
    
    # æª¢æŸ¥æ˜¯å¦å®‰è£äº† GPU ç‰ˆæœ¬çš„ PyTorch
    if not torch.cuda.is_available():
        print("\nâš ï¸ æ‚¨å¯èƒ½æœªå®‰è£ GPU ç‰ˆæœ¬çš„ PyTorch")
        print("\nåŸ·è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£ GPU ç‰ˆæœ¬ï¼š")
        print("pip uninstall torch torchvision torchaudio")
        print("pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
        return
    
    # ç¢ºèª CUDA_VISIBLE_DEVICES ç’°å¢ƒè®Šé‡
    cuda_visible = os.environ.get('CUDA_VISIBLE_DEVICES', 'not set')
    print(f"CUDA_VISIBLE_DEVICES = {cuda_visible}")
    if cuda_visible == '':
        print("âš ï¸ CUDA_VISIBLE_DEVICES ç‚ºç©ºï¼Œè¨­ç½®ç‚º 0")
        os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    
    # å˜—è©¦è¨­ç½® cudnn benchmark
    torch.backends.cudnn.benchmark = True
    print("âœ… å·²å•Ÿç”¨ cuDNN benchmark æ¨¡å¼")
    
    # èª¿æ•´è¨˜æ†¶é«”åˆ†é…
    torch.cuda.empty_cache()
    print("âœ… å·²æ¸…ç† CUDA ç·©å­˜")
    
    print("\nğŸ’¡ å»ºè­°ï¼š")
    print("1. ç¢ºèªæ‚¨çš„ PyTorch ç‰ˆæœ¬æ”¯æ´æ‚¨çš„ CUDA ç‰ˆæœ¬")
    print("2. æ›´æ–° GPU é©…å‹•ç¨‹å¼åˆ°æœ€æ–°ç‰ˆæœ¬")
    print("3. é‡å•Ÿé›»è…¦å¾Œå†è©¦")
    print("4. ä½¿ç”¨è¼ƒå°æ‰¹æ¬¡å¤§å° (batch_size)")
    print("5. æª¢æŸ¥ nvidia-smi æŒ‡ä»¤æ˜¯å¦æ­£å¸¸é‹è¡Œ")
    print("\né‹è¡Œä»¥ä¸‹æŒ‡ä»¤æª¢æŸ¥ GPU ä½¿ç”¨æƒ…æ³ï¼š")
    print("nvidia-smi")

if __name__ == "__main__":
    test_result = test_gpu()
    
    if not test_result:
        fix_gpu_issues()
    else:
        print("\nğŸ‰ æ‚¨çš„ç’°å¢ƒå·²ç¶“å¯ä»¥ä½¿ç”¨ GPU è¨“ç·´ï¼")
        print("ğŸ’¡ æ‚¨å¯ä»¥åŸ·è¡Œï¼špython train_pytorch.py")
        print("ğŸ’¡ è¨˜å¾—ç›£æ§ GPU ä½¿ç”¨æƒ…æ³ï¼šnvidia-smi -l 1")