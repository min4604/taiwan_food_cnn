import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
from PIL import Image
import pandas as pd
import os

class TaiwanFoodDataset(Dataset):
    def __init__(self, csv_path, img_dir, transform=None, is_test=False):
        self.data = pd.read_csv(csv_path, header=None)
        self.img_dir = img_dir
        self.transform = transform
        self.is_test = is_test
        
        # å»ºç«‹ç›®éŒ„åç¨±æ˜ å°„ä»¥è™•ç†å‘½åä¸ä¸€è‡´å•é¡Œ
        self.dir_mapping = {
            'deep_fried_chicken_cutlets': 'deep-fried_chicken_cutlets',
            'fried_spanish_mackerel_thick_soup': 'fried-spanish_mackerel_thick_soup',
            'hakka_stir_fried': 'hakka_stir-fried',
            'kung_pao_chicken': 'kung-pao_chicken',
            'rice_with_soy_stewed_pork': 'rice_with_soy-stewed_pork',
            'steam_fried_bun': 'steam-fried_bun',
            'stir_fried_calamari_broth': 'stir-fried_calamari_broth',
            'stir_fried_duck_meat_broth': 'stir-fried_duck_meat_broth',
            'stir_fried_loofah_with_clam': 'stir-fried_loofah_with_clam',
            'stir_fried_pork_intestine_with_ginger': 'stir-fried_pork_intestine_with_ginger',
            'three_cup_chicken': 'three-cup_chicken',
            'tube_shaped_migao': 'tube-shaped_migao',
        }
        
    def __len__(self):
        return len(self.data)
        
    def __getitem__(self, idx):
        if self.is_test:
            # æ¸¬è©¦é›†æ ¼å¼: [index, image_path]
            img_path = self.data.iloc[idx, 1]
            # æ¸¬è©¦é›†çš„åœ–ç‰‡ç›´æ¥åœ¨ test ç›®éŒ„ä¸‹
            img_filename = os.path.basename(img_path)
            img_name = os.path.join(self.img_dir, img_filename)
            # æ¸¬è©¦é›†æ²’æœ‰æ¨™ç±¤ï¼Œè¿”å› -1
            label = -1
        else:
            # è¨“ç·´é›†æ ¼å¼: [index, class_id, image_path]
            img_path = self.data.iloc[idx, 2]  # train/category/filename.jpg
            # ç§»é™¤é–‹é ­çš„ "train/" å‰ç¶´ï¼Œä¿ç•™é¡åˆ¥ç›®éŒ„çµæ§‹
            if img_path.startswith('train/'):
                relative_path = img_path[6:]  # ç§»é™¤ "train/" å‰ç¶´
            else:
                relative_path = img_path
            
            # åˆ†é›¢ç›®éŒ„åç¨±å’Œæª”æ¡ˆåç¨±
            path_parts = relative_path.split('/')
            if len(path_parts) >= 2:
                category_name = path_parts[0]
                filename = path_parts[1]
                
                # æª¢æŸ¥æ˜¯å¦éœ€è¦æ˜ å°„ç›®éŒ„åç¨±
                if category_name in self.dir_mapping:
                    category_name = self.dir_mapping[category_name]
                
                # é‡æ–°çµ„åˆè·¯å¾‘
                relative_path = os.path.join(category_name, filename)
            
            # æ­£è¦åŒ–è·¯å¾‘åˆ†éš”ç¬¦è™Ÿï¼Œç¢ºä¿åœ¨ Windows ä¸‹æ­£ç¢º
            relative_path = relative_path.replace('/', os.sep).replace('\\', os.sep)
            # çµ„åˆå®Œæ•´è·¯å¾‘ï¼šbase_dir + category + filename
            img_name = os.path.join(self.img_dir, relative_path)
            label = int(self.data.iloc[idx, 1])
            
        try:
            image = Image.open(img_name).convert('RGB')
        except FileNotFoundError:
            print(f"æ‰¾ä¸åˆ°æª”æ¡ˆ: {img_name}")
            print(f"åŸå§‹è·¯å¾‘: {img_path if not self.is_test else self.data.iloc[idx, 1]}")
            raise
        
        if self.transform:
            image = self.transform(image)
            
        return image, label

def get_dataloaders(train_csv, test_csv, train_img_dir, test_img_dir, batch_size=32, img_size=224, val_split=0.2, num_workers=0, pin_memory=False):
    """
    å»ºç«‹è¨“ç·´ã€é©—è­‰å’Œæ¸¬è©¦çš„ DataLoader
    
    é‡è¦ï¼šæ¸¬è©¦é›†ä¸åƒèˆ‡è¨“ç·´éç¨‹ï¼
    - è¨“ç·´é›†ï¼šç”¨æ–¼æ¨¡å‹å­¸ç¿’
    - é©—è­‰é›†ï¼šå¾è¨“ç·´é›†åˆ†å‰²å‡ºä¾†ï¼Œç”¨æ–¼èª¿åƒå’Œæ—©åœ
    - æ¸¬è©¦é›†ï¼šå®Œå…¨ç¨ç«‹ï¼Œåƒ…ç”¨æ–¼æœ€çµ‚æ¨¡å‹è©•ä¼°
    
    åƒæ•¸ï¼š
    - num_workersï¼šè³‡æ–™è¼‰å…¥çš„å·¥ä½œåŸ·è¡Œç·’æ•¸ï¼Œå»ºè­° GPU è¨“ç·´æ™‚è¨­ç‚º 4
    - pin_memoryï¼šæ˜¯å¦ä½¿ç”¨å›ºå®šè¨˜æ†¶é«”ï¼Œå»ºè­° GPU è¨“ç·´æ™‚è¨­ç‚º True
    """
    # å¢å¼·çš„è¨“ç·´è³‡æ–™å¢å¼·
    train_transform = transforms.Compose([
        transforms.RandomResizedCrop(img_size, scale=(0.8, 1.0)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.ColorJitter(brightness=0.2, contrast=0.2),
        transforms.RandomRotation(10),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    # é©—è­‰/æ¸¬è©¦è³‡æ–™ä¸å¢å¼·
    val_transform = transforms.Compose([
        transforms.Resize((img_size, img_size)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    # å»ºç«‹å…©å€‹ç›¸åŒçš„è³‡æ–™é›†ï¼Œä¸€å€‹ç”¨æ–¼è¨“ç·´ï¼ˆæœ‰å¢å¼·ï¼‰ï¼Œä¸€å€‹ç”¨æ–¼é©—è­‰ï¼ˆç„¡å¢å¼·ï¼‰
    train_dataset_aug = TaiwanFoodDataset(train_csv, train_img_dir, train_transform, is_test=False)
    train_dataset_no_aug = TaiwanFoodDataset(train_csv, train_img_dir, val_transform, is_test=False)
    
    # åˆ†å‰²è¨“ç·´é›†ç‚ºè¨“ç·´/é©—è­‰é›†
    dataset_size = len(train_dataset_aug)
    train_size = int((1 - val_split) * dataset_size)
    val_size = dataset_size - train_size
    
    # ä½¿ç”¨ç›¸åŒçš„éš¨æ©Ÿç¨®å­ç¢ºä¿åˆ†å‰²ä¸€è‡´
    torch.manual_seed(42)
    train_indices, val_indices = torch.utils.data.random_split(range(dataset_size), [train_size, val_size])
    
    # å»ºç«‹å­é›†
    train_subset = torch.utils.data.Subset(train_dataset_aug, train_indices.indices)
    val_subset = torch.utils.data.Subset(train_dataset_no_aug, val_indices.indices)
    
    # æ¸¬è©¦é›†ï¼ˆæ²’æœ‰æ¨™ç±¤ï¼‰
    test_dataset = TaiwanFoodDataset(test_csv, test_img_dir, val_transform, is_test=True)
    
    print(f"ğŸ“¥ æ•¸æ“šåŠ è¼‰å™¨é…ç½®: num_workers={num_workers}, pin_memory={pin_memory}, batch_size={batch_size}")
    
    # æ ¹æ“šåƒæ•¸é…ç½® DataLoader
    train_loader = DataLoader(
        train_subset, 
        batch_size=batch_size, 
        shuffle=True, 
        num_workers=num_workers,
        pin_memory=pin_memory,
        drop_last=True  # ä¸Ÿæ£„ä¸å®Œæ•´æ‰¹æ¬¡ï¼Œé¿å…æ‰¹æ¬¡æ­¸ä¸€åŒ–å•é¡Œ
    )
    val_loader = DataLoader(
        val_subset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=num_workers,
        pin_memory=pin_memory
    )
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=num_workers,
        pin_memory=pin_memory
    )
    
    return train_loader, val_loader, test_loader
