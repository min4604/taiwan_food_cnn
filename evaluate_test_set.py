import torch
import torch.nn as nn
from pytorch_model import TaiwanFoodResNet50
from pytorch_data_loader import TaiwanFoodDataset
from torch.utils.data import DataLoader
from torchvision import transforms
from tqdm import tqdm
import os

def detect_available_devices():
    """
    æª¢æ¸¬å¯ç”¨çš„è¨ˆç®—è£ç½®ï¼ˆNPU/GPU/CPUï¼‰
    ç‰¹åˆ¥æ”¯æ´ AMD Ryzen AI 9HX NPU
    """
    print("ğŸ” æª¢æ¸¬å¯ç”¨çš„è¨ˆç®—è£ç½®")
    print("=" * 60)
    
    devices = []
    device_info = []
    
    # æª¢æ¸¬ AMD Ryzen AI NPU
    amd_npu_available = False
    try:
        # æ–¹å¼ 1: æª¢æ¸¬ DirectML
        try:
            import torch_directml
            if torch_directml.is_available():
                amd_npu_available = True
                devices.append('dml')
                device_info.append("ğŸš€ AMD NPU å¯ç”¨ (DirectML)")
                device_info.append("   æ”¯æ´: Ryzen AI 9HX NPU")
        except ImportError:
            pass
        
        # æ–¹å¼ 2: æª¢æ¸¬ ONNX Runtime
        try:
            import onnxruntime as ort
            providers = ort.get_available_providers()
            if 'DmlExecutionProvider' in providers:
                if not amd_npu_available:  # é¿å…é‡è¤‡é¡¯ç¤º
                    amd_npu_available = True
                    devices.append('onnx_dml')
                device_info.append("âœ… ONNX Runtime DML å¯ç”¨")
                device_info.append("   æ”¯æ´: AMD Ryzen AI NPU")
        except ImportError:
            pass
        
        # æ–¹å¼ 3: æª¢æ¸¬ç³»çµ±è³‡è¨Š
        import platform
        import subprocess
        if platform.system() == 'Windows':
            try:
                # æª¢æŸ¥ AMD Ryzen AI è™•ç†å™¨
                result = subprocess.run(['wmic', 'cpu', 'get', 'name'], 
                                      capture_output=True, text=True, timeout=5)
                if 'AMD Ryzen' in result.stdout and 'AI' in result.stdout:
                    device_info.append("ğŸ’» æª¢æ¸¬åˆ° AMD Ryzen AI è™•ç†å™¨")
                    if not amd_npu_available:
                        device_info.append("   âš ï¸  NPU å¯èƒ½å¯ç”¨ä½†æœªå•Ÿç”¨")
            except:
                pass
    except Exception as e:
        pass
    
    # æª¢æ¸¬ä¼ çµ± NPU æ”¯æ´ï¼ˆåä¸ºç­‰ï¼‰
    npu_available = False
    try:
        if hasattr(torch, 'npu') and torch.npu.is_available():
            npu_count = torch.npu.device_count()
            npu_available = True
            devices.append('npu')
            device_info.append(f"ğŸš€ NPU å¯ç”¨: {npu_count} å€‹è£ç½®")
            for i in range(npu_count):
                try:
                    npu_name = torch.npu.get_device_name(i)
                    device_info.append(f"   NPU {i}: {npu_name}")
                except:
                    device_info.append(f"   NPU {i}: æœªçŸ¥å‹è™Ÿ")
        elif hasattr(torch.backends, 'npu') and torch.backends.npu.is_available():
            npu_available = True
            devices.append('npu')
            device_info.append("ğŸš€ NPU æ”¯æ´å·²å•Ÿç”¨")
    except Exception as e:
        pass
    
    # æª¢æ¸¬ Apple MPS
    mps_available = False
    try:
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            mps_available = True
            devices.append('mps')
            device_info.append("ğŸ MPS (Apple Silicon) å¯ç”¨")
    except:
        pass
    
    # æª¢æ¸¬ GPU æ”¯æ´
    gpu_available = torch.cuda.is_available()
    if gpu_available:
        gpu_count = torch.cuda.device_count()
        devices.append('cuda')
        device_info.append(f"âœ… CUDA GPU å¯ç”¨: {gpu_count} å€‹è£ç½®")
        for i in range(gpu_count):
            gpu_name = torch.cuda.get_device_name(i)
            memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            device_info.append(f"   GPU {i}: {gpu_name} ({memory:.1f} GB)")
    
    # CPU å§‹çµ‚å¯ç”¨
    devices.append('cpu')
    device_info.append("ğŸ’» CPU å¯ç”¨")
    
    # é¡¯ç¤ºæª¢æ¸¬çµæœ
    for info in device_info:
        print(info)
    
    if not amd_npu_available and not npu_available and not gpu_available:
        print("âš ï¸  æ²’æœ‰æª¢æ¸¬åˆ° NPU æˆ– GPUï¼Œå°‡ä½¿ç”¨ CPU")
    
    print("=" * 60)
    return devices, npu_available, gpu_available, amd_npu_available

def choose_device(available_devices, npu_available, gpu_available, amd_npu_available=False, manual_mode=False):
    """
    é¸æ“‡è¨ˆç®—è£ç½®
    manual_mode: True ç‚ºæ‰‹å‹•é¸æ“‡ï¼ŒFalse ç‚ºè‡ªå‹•é¸æ“‡
    """
    print("\nğŸ¯ é¸æ“‡è¨ˆç®—è£ç½®")
    print("-" * 40)
    
    options = []
    if amd_npu_available:
        options.append(('amd_npu', 'ğŸš€ AMD Ryzen AI NPU (æœ€é«˜æ•ˆèƒ½)'))
    if npu_available:
        options.append(('npu:0', 'ğŸš€ NPU (é«˜æ•ˆèƒ½)'))
    if gpu_available:
        options.append(('cuda:0', 'âœ… GPU (é«˜æ•ˆèƒ½)'))
    options.append(('cpu', 'ğŸ’» CPU (ç©©å®š)'))
    
    print("å¯ç”¨çš„è¨ˆç®—è£ç½®:")
    for i, (device, desc) in enumerate(options):
        print(f"  {i}. {desc}")
    
    if manual_mode:
        # æ‰‹å‹•é¸æ“‡æ¨¡å¼
        print(f"\nè«‹é¸æ“‡è¦ä½¿ç”¨çš„è£ç½® (0-{len(options)-1}):")
        print("æˆ–ç›´æ¥æŒ‰ Enter ä½¿ç”¨è‡ªå‹•é¸æ“‡")
        
        while True:
            try:
                user_input = input("ğŸ‘‰ è«‹è¼¸å…¥é¸é …ç·¨è™Ÿ: ").strip()
                
                if user_input == "":
                    # ä½¿ç”¨è€…é¸æ“‡è‡ªå‹•æ¨¡å¼
                    print("ğŸ¤– ä½¿ç”¨è‡ªå‹•é¸æ“‡æ¨¡å¼")
                    break
                
                choice_idx = int(user_input)
                if 0 <= choice_idx < len(options):
                    chosen_device = options[choice_idx][0]
                    print(f"\nâœ… æ‰‹å‹•é¸æ“‡: {options[choice_idx][1]}")
                    return chosen_device
                else:
                    print(f"âš ï¸  è«‹è¼¸å…¥ 0-{len(options)-1} ä¹‹é–“çš„æ•¸å­—")
                    
            except ValueError:
                print("âš ï¸  è«‹è¼¸å…¥æœ‰æ•ˆçš„æ•¸å­—")
            except KeyboardInterrupt:
                print("\n\nâŒ ä½¿ç”¨è€…å–æ¶ˆæ“ä½œ")
                return None
    
    # è‡ªå‹•é¸æ“‡æœ€ä½³è£ç½®
    if amd_npu_available:
        chosen_device = 'amd_npu'
        print(f"\nğŸ¤– è‡ªå‹•é¸æ“‡: AMD Ryzen AI NPU (æœ€ä½³æ•ˆèƒ½)")
    elif npu_available:
        chosen_device = 'npu:0'
        print(f"\nğŸ¤– è‡ªå‹•é¸æ“‡: NPU (é«˜æ•ˆèƒ½)")
    elif gpu_available:
        chosen_device = 'cuda:0'
        print(f"\nğŸ¤– è‡ªå‹•é¸æ“‡: GPU (é«˜æ•ˆèƒ½)")
    else:
        chosen_device = 'cpu'
        print(f"\nğŸ¤– è‡ªå‹•é¸æ“‡: CPU")
    
    return chosen_device

def evaluate_with_amd_npu(model_path, test_csv, test_img_dir, num_classes=101, batch_size=32, img_size=224):
    """
    ä½¿ç”¨æœ€ä½³åŒ–çš„ AMD NPU é€²è¡Œæ¸¬è©¦é›†è©•ä¼° - æé«˜ NPU ä½¿ç”¨ç‡
    """
    try:
        # å˜—è©¦ä½¿ç”¨æœ€ä½³åŒ–ç‰ˆæœ¬
        try:
            from optimized_amd_npu import OptimizedAMDNPUInference
            print("ğŸš€ ä½¿ç”¨æœ€ä½³åŒ– AMD NPU æ¨ç†å¼•æ“...")
            
            # èª¿æ•´æ‰¹æ¬¡å¤§å°ä»¥æœ€å¤§åŒ– NPU ä½¿ç”¨ç‡
            optimized_batch_size = min(batch_size, 32)  # NPU æœ€ä½³æ‰¹æ¬¡å¤§å°
            npu_inference = OptimizedAMDNPUInference(
                model_path, 
                img_size, 
                batch_size=optimized_batch_size,
                num_threads=6  # å¢åŠ ä¸¦è¡ŒåŸ·è¡Œç·’
            )
            
        except ImportError:
            # å›é€€åˆ°åŸå§‹ç‰ˆæœ¬
            from amd_npu_fixed import AMDNPUInference
            print("ğŸ”„ ä½¿ç”¨æ¨™æº– AMD NPU æ¨ç†å¼•æ“...")
            npu_inference = AMDNPUInference(model_path, img_size)
        
        # å»ºç«‹æ¸¬è©¦é›† DataLoader (åƒ…ç”¨æ–¼çµ„ç¹”è³‡æ–™)
        test_transform = transforms.Compose([
            transforms.Resize((img_size, img_size)),
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
        ])
        
        test_dataset = TaiwanFoodDataset(test_csv, test_img_dir, test_transform, is_test=True)
        
        print(f"ğŸ“Š æ¸¬è©¦é›†å¤§å°: {len(test_dataset)}")
        print("ğŸ” é–‹å§‹ AMD NPU é«˜æ•ˆç‡è©•ä¼°...")
        print("âš ï¸  æ³¨æ„ï¼šé€™æ˜¯é¦–æ¬¡åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°ï¼Œçµæœä»£è¡¨æ¨¡å‹çš„çœŸå¯¦æ€§èƒ½")
        print("=" * 60)
        
        # æº–å‚™æ‰€æœ‰æ¸¬è©¦åœ–ç‰‡è·¯å¾‘
        all_image_paths = []
        for i in range(len(test_dataset)):
            # å°‹æ‰¾åœ–ç‰‡æª”æ¡ˆ
            base_path = os.path.join(test_img_dir, str(i))
            img_path = None
            
            # å˜—è©¦ä¸åŒå‰¯æª”å
            for ext in ['.jpg', '.jpeg', '.png', '.JPG', '.JPEG', '.PNG']:
                candidate_path = base_path + ext
                if os.path.exists(candidate_path):
                    img_path = candidate_path
                    break
            
            if img_path:
                all_image_paths.append(img_path)
            else:
                print(f"âš ï¸  æ‰¾ä¸åˆ°åœ–ç‰‡: {base_path}")
                all_image_paths.append(None)
        
        # éæ¿¾å‡ºæœ‰æ•ˆçš„åœ–ç‰‡è·¯å¾‘
        valid_paths = [path for path in all_image_paths if path is not None]
        print(f"ğŸ“¸ æœ‰æ•ˆåœ–ç‰‡: {len(valid_paths)}/{len(all_image_paths)}")
        
        # ä½¿ç”¨æœ€ä½³åŒ–çš„æ‰¹æ¬¡æ¨ç†
        if hasattr(npu_inference, 'predict_image_batch'):
            print("âš¡ ä½¿ç”¨æ‰¹æ¬¡æ¨ç†æ¨¡å¼ä»¥æœ€å¤§åŒ– NPU ä½¿ç”¨ç‡...")
            batch_results = npu_inference.predict_image_batch(valid_paths)
            
            # é‡æ–°å°æ‡‰çµæœåˆ°åŸå§‹ç´¢å¼•
            all_predictions = []
            valid_idx = 0
            
            for i, img_path in enumerate(all_image_paths):
                if img_path is not None:
                    if valid_idx < len(batch_results):
                        all_predictions.append(batch_results[valid_idx]['prediction'])
                        valid_idx += 1
                    else:
                        all_predictions.append(-1)
                else:
                    all_predictions.append(-1)
            
            # æ¸…ç†è³‡æº
            if hasattr(npu_inference, 'shutdown'):
                npu_inference.shutdown()
                
        else:
            # å›é€€åˆ°å–®å¼µè™•ç†
            print("ğŸ”„ ä½¿ç”¨å–®å¼µæ¨ç†æ¨¡å¼...")
            all_predictions = []
            
            with tqdm(total=len(all_image_paths), desc="AMD NPU æ¨ç†ä¸­", ncols=80) as pbar:
                for img_path in all_image_paths:
                    if img_path and os.path.exists(img_path):
                        pred = npu_inference.predict_image(img_path)
                        all_predictions.append(pred)
                    else:
                        all_predictions.append(-1)
                    
                    pbar.update(1)
                    pbar.set_postfix({'å·²è™•ç†': len(all_predictions)})
        
        # å„²å­˜é æ¸¬çµæœ
        results_file = "test_predictions_optimized_amd_npu.csv"
        with open(results_file, 'w', encoding='utf-8') as f:
            f.write("Id,Category\n")
            for i, pred in enumerate(all_predictions):
                f.write(f"{i},{pred}\n")
        
        print(f"\nâœ… æœ€ä½³åŒ– AMD NPU è©•ä¼°å®Œæˆï¼")
        print(f"ğŸ“ é æ¸¬çµæœå·²å„²å­˜è‡³: {results_file}")
        print(f"ğŸ“Š å…±è™•ç† {len(all_predictions)} å¼µæ¸¬è©¦åœ–ç‰‡")
        
        # é¡¯ç¤ºé æ¸¬é¡åˆ¥åˆ†ä½ˆ
        from collections import Counter
        valid_predictions = [p for p in all_predictions if p != -1]
        pred_counts = Counter(valid_predictions)
        print(f"\nğŸ“ˆ é æ¸¬é¡åˆ¥åˆ†ä½ˆ (å‰10å):")
        for class_id, count in pred_counts.most_common(10):
            print(f"   é¡åˆ¥ {class_id}: {count} å¼µåœ–ç‰‡")
        
        print(f"\nğŸš€ AMD Ryzen AI NPU æœ€ä½³åŒ–æ¨ç†å®Œæˆï¼NPU ä½¿ç”¨ç‡å·²æœ€å¤§åŒ–ï¼")
        
    except ImportError as e:
        print(f"âŒ AMD NPU æ¨¡çµ„è¼‰å…¥å¤±æ•—: {e}")
        print("ğŸ”„ å›é€€åˆ°æ¨™æº–è©•ä¼°æ¨¡å¼...")
        return evaluate_standard_mode(model_path, test_csv, test_img_dir, num_classes, batch_size, img_size, 'cpu')
    except Exception as e:
        print(f"âŒ AMD NPU è©•ä¼°å¤±æ•—: {e}")
        print("ğŸ”„ å›é€€åˆ°æ¨™æº–è©•ä¼°æ¨¡å¼...")
        return evaluate_standard_mode(model_path, test_csv, test_img_dir, num_classes, batch_size, img_size, 'cpu')

def evaluate_standard_mode(model_path, test_csv, test_img_dir, num_classes, batch_size, img_size, device_str):
    """
    æ¨™æº–æ¨¡å¼è©•ä¼° (CPU/GPU/å‚³çµ±NPU)
    """
    # è¨­å®šè£ç½®
    device = torch.device(device_str)
    print(f"ğŸ”§ ä½¿ç”¨æ¨™æº–æ¨¡å¼è£ç½®: {device}")
    
    # å¦‚æœä½¿ç”¨ NPUï¼Œèª¿æ•´æ‰¹æ¬¡å¤§å°
    if 'npu' in device_str:
        batch_size = min(batch_size, 16)  # NPU å¯èƒ½éœ€è¦è¼ƒå°çš„æ‰¹æ¬¡
        print(f"ğŸ”§ NPU æœ€ä½³åŒ–: èª¿æ•´æ‰¹æ¬¡å¤§å°ç‚º {batch_size}")
    
    # è¼‰å…¥æ¨¡å‹
    model = TaiwanFoodResNet50(num_classes=num_classes)
    try:
        # å…ˆåœ¨ CPU ä¸Šè¼‰å…¥ï¼Œç„¶å¾Œç§»å‹•åˆ°ç›®æ¨™è£ç½®
        state_dict = torch.load(model_path, map_location='cpu')
        model.load_state_dict(state_dict)
        model = model.to(device)
        model.eval()
        print(f"âœ… æˆåŠŸè¼‰å…¥æ¨¡å‹: {model_path}")
        
        # å¦‚æœä½¿ç”¨ NPUï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šè¨­å®š
        if 'npu' in str(device):
            print("ğŸš€ NPU æ¨¡å¼å·²å•Ÿç”¨")
            
    except Exception as e:
        print(f"âŒ è¼‰å…¥æ¨¡å‹å¤±æ•—: {e}")
        print("ğŸ’¡ å»ºè­°æª¢æŸ¥æ¨¡å‹æª”æ¡ˆå’Œè£ç½®ç›¸å®¹æ€§")
        return
    
    # æ¸¬è©¦é›†è®Šæ›ï¼ˆä¸å¢å¼·ï¼‰
    test_transform = transforms.Compose([
        transforms.Resize((img_size, img_size)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    # å»ºç«‹æ¸¬è©¦é›† DataLoader
    test_dataset = TaiwanFoodDataset(test_csv, test_img_dir, test_transform, is_test=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
    
    print(f"ğŸ“Š æ¸¬è©¦é›†å¤§å°: {len(test_dataset)}")
    print("ğŸ” é–‹å§‹æœ€çµ‚è©•ä¼°...")
    print("âš ï¸  æ³¨æ„ï¼šé€™æ˜¯é¦–æ¬¡åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°ï¼Œçµæœä»£è¡¨æ¨¡å‹çš„çœŸå¯¦æ€§èƒ½")
    print("=" * 60)
    
    # ç”±æ–¼æ¸¬è©¦é›†æ²’æœ‰æ¨™ç±¤ï¼Œæˆ‘å€‘åªèƒ½ç”Ÿæˆé æ¸¬çµæœ
    all_predictions = []
    
    with torch.no_grad():
        test_pbar = tqdm(test_loader, desc="æ¸¬è©¦ä¸­", ncols=80)
        for images, _ in test_pbar:  # æ¨™ç±¤ç‚º -1ï¼Œå¿½ç•¥
            images = images.to(device)
            outputs = model(images)
            _, predicted = outputs.max(1)
            
            all_predictions.extend(predicted.cpu().numpy())
            test_pbar.set_postfix({'å·²è™•ç†': len(all_predictions)})
    
    # å„²å­˜é æ¸¬çµæœ
    results_file = "test_predictions.csv"
    with open(results_file, 'w', encoding='utf-8') as f:
        f.write("Id,Category\n")
        for i, pred in enumerate(all_predictions):
            f.write(f"{i},{pred}\n")
    
    print(f"\nâœ… æ¸¬è©¦å®Œæˆï¼")
    print(f"ğŸ“ é æ¸¬çµæœå·²å„²å­˜è‡³: {results_file}")
    print(f"ğŸ“Š å…±è™•ç† {len(all_predictions)} å¼µæ¸¬è©¦åœ–ç‰‡")
    
    # é¡¯ç¤ºé æ¸¬é¡åˆ¥åˆ†ä½ˆ
    from collections import Counter
    pred_counts = Counter(all_predictions)
    print(f"\nğŸ“ˆ é æ¸¬é¡åˆ¥åˆ†ä½ˆ (å‰10å):")
    for class_id, count in pred_counts.most_common(10):
        print(f"   é¡åˆ¥ {class_id}: {count} å¼µåœ–ç‰‡")

def evaluate_on_test_set(model_path, test_csv, test_img_dir, num_classes=101, batch_size=32, img_size=224, manual_device_selection=False):
    """
    åœ¨æ¸¬è©¦é›†ä¸Šè©•ä¼°è¨“ç·´å¥½çš„æ¨¡å‹
    
    manual_device_selection: True ç‚ºæ‰‹å‹•é¸æ“‡ç¡¬é«”ï¼ŒFalse ç‚ºè‡ªå‹•é¸æ“‡
    æ³¨æ„ï¼šé€™æ˜¯æœ€çµ‚è©•ä¼°ï¼Œæ¸¬è©¦é›†å¾æœªåƒèˆ‡è¨“ç·´éç¨‹
    """
    
    # æª¢æŸ¥æ¨¡å‹æª”æ¡ˆæ˜¯å¦å­˜åœ¨
    if not os.path.exists(model_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ¨¡å‹æª”æ¡ˆ: {model_path}")
        return
    
    # æª¢æ¸¬å¯ç”¨è£ç½®ä¸¦é¸æ“‡
    available_devices, npu_available, gpu_available, amd_npu_available = detect_available_devices()
    
    if manual_device_selection:
        print("\nğŸ® æ‰‹å‹•è£ç½®é¸æ“‡æ¨¡å¼")
    else:
        print("\nğŸ¤– è‡ªå‹•è£ç½®é¸æ“‡æ¨¡å¼")
    
    device_str = choose_device(available_devices, npu_available, gpu_available, amd_npu_available, manual_device_selection)
    
    if device_str is None:
        print("âŒ æœªé¸æ“‡è£ç½®ï¼Œç¨‹å¼çµæŸ")
        return
    
    # è¨­å®šè£ç½®
    if device_str == 'amd_npu':
        # ä½¿ç”¨ AMD NPU å°ˆç”¨æ¨ç†
        print("\nğŸš€ å•Ÿç”¨ AMD Ryzen AI NPU æ¨ç†æ¨¡å¼")
        return evaluate_with_amd_npu(model_path, test_csv, test_img_dir, num_classes, batch_size, img_size)
    else:
        # ä½¿ç”¨æ¨™æº–æ¨¡å¼ (CPU/GPU/å‚³çµ±NPU)
        return evaluate_standard_mode(model_path, test_csv, test_img_dir, num_classes, batch_size, img_size, device_str)
    
if __name__ == '__main__':
    # ä½¿ç”¨æœ€æ–°çš„æ¨¡å‹æª”æ¡ˆé€²è¡Œæ¸¬è©¦
    model_files = [f for f in os.listdir('models') if f.endswith('.pth')]
    if not model_files:
        print("âŒ æ‰¾ä¸åˆ°è¨“ç·´å¥½çš„æ¨¡å‹æª”æ¡ˆ")
        print("è«‹å…ˆåŸ·è¡Œ python train_pytorch.py é€²è¡Œè¨“ç·´")
    else:
        # é¸æ“‡æœ€æ–°çš„æ¨¡å‹
        latest_model = max(model_files, key=lambda x: os.path.getctime(os.path.join('models', x)))
        model_path = os.path.join('models', latest_model)
        
        print(f"ğŸ¯ ä½¿ç”¨æ¨¡å‹: {latest_model}")
        print("=" * 60)
        
        # è©¢å•ä½¿ç”¨è€…é¸æ“‡æ¨¡å¼
        print("ğŸ® è«‹é¸æ“‡æ¨ç†ç¡¬é«”é¸æ“‡æ¨¡å¼:")
        print("  1. ğŸ¤– è‡ªå‹•æ¨¡å¼ (ç³»çµ±è‡ªå‹•é¸æ“‡æœ€ä½³ç¡¬é«”)")
        print("  2. ğŸ® æ‰‹å‹•æ¨¡å¼ (æ‰‹å‹•é¸æ“‡æ¨ç†ç¡¬é«”)")
        print("  3. âŒ é€€å‡ºç¨‹å¼")
        
        while True:
            try:
                mode_choice = input("\nğŸ‘‰ è«‹é¸æ“‡æ¨¡å¼ (1-3): ").strip()
                
                if mode_choice == "1":
                    print("\nğŸ¤– ä½¿ç”¨è‡ªå‹•æ¨¡å¼")
                    manual_mode = False
                    break
                elif mode_choice == "2":
                    print("\nğŸ® ä½¿ç”¨æ‰‹å‹•æ¨¡å¼")
                    manual_mode = True
                    break
                elif mode_choice == "3":
                    print("\nğŸ‘‹ ç¨‹å¼çµæŸ")
                    exit(0)
                else:
                    print("âš ï¸  è«‹è¼¸å…¥ 1ã€2 æˆ– 3")
                    
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ ç¨‹å¼çµæŸ")
                exit(0)
        
        # é–‹å§‹è©•ä¼°
        evaluate_on_test_set(
            model_path=model_path,
            test_csv='archive/tw_food_101/tw_food_101_test_list.csv',
            test_img_dir='archive/tw_food_101/test',
            manual_device_selection=manual_mode
        )