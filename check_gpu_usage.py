"""
GPU ä½¿ç”¨æª¢æŸ¥å·¥å…·
åœ¨è¨“ç·´é–‹å§‹å¾Œé‹è¡Œæ­¤è…³æœ¬ï¼Œå¯ä»¥å¯¦æ™‚ç›£æ§ GPU ä½¿ç”¨æƒ…æ³
"""

import torch
import time
import os

def check_gpu_status():
    """æª¢æŸ¥ç•¶å‰ GPU ç‹€æ…‹"""
    print("=" * 70)
    print("ğŸ” GPU ä½¿ç”¨ç‹€æ…‹æª¢æŸ¥")
    print("=" * 70)
    
    if not torch.cuda.is_available():
        print("âŒ CUDA ä¸å¯ç”¨ - ç„¡æ³•ä½¿ç”¨ GPU")
        print("ğŸ’¡ è«‹ç¢ºèª:")
        print("   1. é›»è…¦æœ‰ NVIDIA GPU")
        print("   2. å·²å®‰è£ NVIDIA é©…å‹•ç¨‹å¼")
        print("   3. å·²å®‰è£ CUDA ç‰ˆæœ¬çš„ PyTorch")
        return False
    
    print(f"âœ… CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    print(f"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"âœ… cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
    print()
    
    # æª¢æŸ¥æ¯å€‹ GPU
    device_count = torch.cuda.device_count()
    print(f"ğŸ¯ æª¢æ¸¬åˆ° {device_count} å€‹ GPU:")
    print("-" * 70)
    
    for i in range(device_count):
        props = torch.cuda.get_device_properties(i)
        memory_total = props.total_memory / 1024**3
        memory_allocated = torch.cuda.memory_allocated(i) / 1024**3
        memory_reserved = torch.cuda.memory_reserved(i) / 1024**3
        memory_free = memory_total - memory_reserved
        
        print(f"\nğŸ“± GPU {i}: {props.name}")
        print(f"   ç¸½è¨˜æ†¶é«”: {memory_total:.2f} GB")
        print(f"   å·²åˆ†é…: {memory_allocated:.2f} GB ({memory_allocated/memory_total*100:.1f}%)")
        print(f"   å·²ä¿ç•™: {memory_reserved:.2f} GB ({memory_reserved/memory_total*100:.1f}%)")
        print(f"   å¯ç”¨: {memory_free:.2f} GB ({memory_free/memory_total*100:.1f}%)")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰å¼µé‡åœ¨ GPU ä¸Š
        if memory_allocated > 0:
            print(f"   âœ… GPU æ­£åœ¨è¢«ä½¿ç”¨ (å·²åˆ†é… {memory_allocated:.2f} GB)")
        else:
            print(f"   âš ï¸  GPU æœªè¢«ä½¿ç”¨ (ç„¡åˆ†é…è¨˜æ†¶é«”)")
    
    print("\n" + "=" * 70)
    return True

def test_gpu_computation():
    """æ¸¬è©¦ GPU è¨ˆç®—åŠŸèƒ½"""
    if not torch.cuda.is_available():
        print("âŒ ç„¡æ³•é€²è¡Œ GPU æ¸¬è©¦")
        return
    
    print("\nğŸ§ª GPU è¨ˆç®—æ¸¬è©¦")
    print("=" * 70)
    
    device = torch.device('cuda')
    
    # æ¸¬è©¦ 1: ç°¡å–®çš„å¼µé‡é‹ç®—
    print("\næ¸¬è©¦ 1: åŸºæœ¬å¼µé‡é‹ç®—")
    try:
        x = torch.randn(1000, 1000).to(device)
        y = torch.randn(1000, 1000).to(device)
        
        start_time = time.time()
        z = torch.matmul(x, y)
        torch.cuda.synchronize()  # ç­‰å¾… GPU å®Œæˆ
        gpu_time = time.time() - start_time
        
        print(f"   âœ… GPU çŸ©é™£ä¹˜æ³•å®Œæˆ")
        print(f"   â±ï¸  è€—æ™‚: {gpu_time*1000:.2f} ms")
        print(f"   ğŸ“Š çµæœå½¢ç‹€: {z.shape}")
        print(f"   ğŸ¯ çµæœè¨­å‚™: {z.device}")
        
    except Exception as e:
        print(f"   âŒ GPU è¨ˆç®—å¤±æ•—: {e}")
        return
    
    # æ¸¬è©¦ 2: CNN æ¨¡å‹æ¨ç†
    print("\næ¸¬è©¦ 2: CNN æ¨¡å‹æ¨ç†")
    try:
        import torch.nn as nn
        
        # å‰µå»ºç°¡å–®çš„ CNN
        model = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d(1),
            nn.Flatten(),
            nn.Linear(128, 10)
        ).to(device)
        
        # å‰µå»ºæ¸¬è©¦è¼¸å…¥
        test_input = torch.randn(32, 3, 224, 224).to(device)
        
        start_time = time.time()
        with torch.no_grad():
            output = model(test_input)
        torch.cuda.synchronize()
        inference_time = time.time() - start_time
        
        print(f"   âœ… CNN æ¨ç†å®Œæˆ")
        print(f"   â±ï¸  è€—æ™‚: {inference_time*1000:.2f} ms")
        print(f"   ğŸ“Š è¼¸å…¥å½¢ç‹€: {test_input.shape}")
        print(f"   ğŸ“Š è¼¸å‡ºå½¢ç‹€: {output.shape}")
        print(f"   ğŸ¯ æ¨¡å‹è¨­å‚™: {next(model.parameters()).device}")
        
    except Exception as e:
        print(f"   âŒ CNN æ¨ç†å¤±æ•—: {e}")
    
    print("\n" + "=" * 70)

def monitor_gpu_realtime(duration=10, interval=1):
    """å¯¦æ™‚ç›£æ§ GPU ä½¿ç”¨æƒ…æ³"""
    if not torch.cuda.is_available():
        print("âŒ CUDA ä¸å¯ç”¨ï¼Œç„¡æ³•ç›£æ§")
        return
    
    print(f"\nğŸ“Š å¯¦æ™‚ GPU ç›£æ§ (æŒçºŒ {duration} ç§’)")
    print("=" * 70)
    print(f"{'æ™‚é–“':<10} {'GPU':<5} {'è¨˜æ†¶é«”ä½¿ç”¨':<15} {'ä½¿ç”¨ç‡':<10} {'æº«åº¦':<10}")
    print("-" * 70)
    
    try:
        for i in range(duration):
            for gpu_id in range(torch.cuda.device_count()):
                memory_allocated = torch.cuda.memory_allocated(gpu_id) / 1024**3
                memory_total = torch.cuda.get_device_properties(gpu_id).total_memory / 1024**3
                utilization = memory_allocated / memory_total * 100
                
                print(f"{i+1:>3}/{duration:<5} GPU{gpu_id:<2} "
                      f"{memory_allocated:>5.2f}/{memory_total:<5.2f} GB "
                      f"{utilization:>6.1f}%", end="")
                
                if memory_allocated > 0.1:
                    print(" âœ… ä½¿ç”¨ä¸­")
                else:
                    print(" âš ï¸  é–’ç½®")
            
            time.sleep(interval)
    
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  ç›£æ§å·²åœæ­¢")
    
    print("=" * 70)

def verify_training_gpu_usage():
    """é©—è­‰è¨“ç·´è…³æœ¬æ˜¯å¦æ­£ç¢ºè¨­ç½® GPU"""
    print("\nğŸ” è¨“ç·´è…³æœ¬ GPU é…ç½®æª¢æŸ¥")
    print("=" * 70)
    
    # æª¢æŸ¥ train_pytorch.py
    train_file = 'train_pytorch.py'
    if not os.path.exists(train_file):
        print(f"âŒ æ‰¾ä¸åˆ° {train_file}")
        return
    
    with open(train_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    checks = {
        'device = torch.device': 'âœ… è¨­å‚™é¸æ“‡ä»£ç¢¼å­˜åœ¨',
        '.to(device)': 'âœ… æ¨¡å‹ç§»è‡³è¨­å‚™ä»£ç¢¼å­˜åœ¨',
        'images.to(device)': 'âœ… æ•¸æ“šç§»è‡³è¨­å‚™ä»£ç¢¼å­˜åœ¨',
        'labels.to(device)': 'âœ… æ¨™ç±¤ç§»è‡³è¨­å‚™ä»£ç¢¼å­˜åœ¨',
        'torch.cuda.is_available()': 'âœ… GPU æª¢æ¸¬ä»£ç¢¼å­˜åœ¨',
    }
    
    print(f"\næª¢æŸ¥ {train_file}:")
    for check, message in checks.items():
        if check in content:
            print(f"   {message}")
        else:
            print(f"   âŒ ç¼ºå°‘: {check}")
    
    print("\n" + "=" * 70)

def main():
    print("\nğŸš€ GPU ä½¿ç”¨ç‹€æ…‹å®Œæ•´æª¢æŸ¥\n")
    
    # 1. æª¢æŸ¥ GPU ç‹€æ…‹
    gpu_available = check_gpu_status()
    
    if not gpu_available:
        print("\nâŒ GPU ä¸å¯ç”¨ï¼Œç„¡æ³•ç¹¼çºŒæ¸¬è©¦")
        return
    
    # 2. é©—è­‰è¨“ç·´è…³æœ¬é…ç½®
    verify_training_gpu_usage()
    
    # 3. æ¸¬è©¦ GPU è¨ˆç®—
    test_gpu_computation()
    
    # 4. è©¢å•æ˜¯å¦é€²è¡Œå¯¦æ™‚ç›£æ§
    print("\nğŸ’¡ å»ºè­°:")
    print("   1. å¦‚æœè¨“ç·´å°šæœªé–‹å§‹ï¼Œè«‹å…ˆé‹è¡Œ: python train_pytorch.py")
    print("   2. è¨“ç·´é–‹å§‹å¾Œï¼Œé‡æ–°é‹è¡Œæ­¤è…³æœ¬æŸ¥çœ‹ GPU ä½¿ç”¨æƒ…æ³")
    print("   3. å¯ä»¥ä½¿ç”¨ 'nvidia-smi' å‘½ä»¤æŸ¥çœ‹è©³ç´° GPU ç‹€æ…‹")
    
    try:
        choice = input("\næ˜¯å¦é€²è¡Œ 10 ç§’å¯¦æ™‚ GPU ç›£æ§? (y/n) [n]: ").strip().lower()
        if choice == 'y':
            monitor_gpu_realtime(duration=10, interval=1)
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ æª¢æŸ¥å®Œæˆ")

if __name__ == '__main__':
    main()
