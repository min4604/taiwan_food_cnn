# 預訓練模型訓練指南

## 📋 更新內容

### ✅ 已實現的功能

1. **所有模型都使用 ImageNet 預訓練權重**
   - ResNet50 (新增增強分類器)
   - EfficientNet-B3
   - ConvNeXt-Tiny
   - RegNet-Y
   - Vision Transformer (ViT)

2. **三種訓練策略**
   - 🔒 凍結骨幹訓練（適合小數據集）
   - 🎯 微調訓練（推薦，兩階段訓練）
   - 🔓 全模型訓練（適合大數據集）

3. **智能訓練流程**
   - 自動載入預訓練權重
   - 可選擇是否凍結預訓練層
   - 動態調整學習率
   - 參數統計顯示

---

## 🚀 使用方法

### 基本訓練流程

```bash
python train_pytorch.py
```

### 互動式選擇流程

#### 步驟 1: GPU 檢測
```
🔍 GPU 環境檢測
============================================================
✅ CUDA 可用: 11.8
🎯 GPU 數量: 1
🚀 當前 GPU: NVIDIA GeForce RTX 3060
```

#### 步驟 2: 選擇訓練模式
```
🎯 選擇訓練模式
============================================================
請選擇訓練模式:
0. 🆕 從頭開始訓練 (新模型)
1. 🔄 繼續訓練: taiwan_food_efficientnet_b3_epoch10.pth
```

#### 步驟 3: 選擇模型架構（新訓練時）
```
🏗️  選擇模型架構
============================================================
1. ResNet50 (基礎模型，速度快)
2. EfficientNet-B3 (推薦，效能佳)
3. ConvNeXt-Tiny (現代架構，精度高)
4. RegNet-Y (高效網路，平衡性好)
5. Vision Transformer (注意力機制，需較大資料集)

💡 提示: 所有模型都使用 ImageNet 預訓練權重
請選擇模型架構 (1-5): 2
```

#### 步驟 4: 選擇訓練策略
```
🎯 選擇訓練策略
============================================================
1. 微調訓練 (推薦)
   先凍結預訓練層訓練分類器，後解凍全模型微調

2. 全模型訓練
   從頭訓練所有層（需要更多時間和數據）

3. 凍結骨幹訓練
   只訓練分類器，預訓練層保持不變（適合小數據集）

💡 推薦:
   - 資料集較小 (<5000張) → 選擇 3 (凍結骨幹)
   - 資料集中等 (5000-20000張) → 選擇 1 (微調訓練)
   - 資料集較大 (>20000張) → 選擇 2 (全模型訓練)

請選擇訓練策略 (1-3) [預設=1]: 1
```

---

## 🎯 三種訓練策略詳解

### 1. 微調訓練（Fine-tuning）- 推薦 ⭐

**適用場景**: 
- 資料集中等規模（5000-20000張）
- 想要平衡訓練時間和精度
- 大多數實際應用場景

**訓練流程**:
```
階段 1 (前 20% epochs):
├─ 🔒 凍結預訓練骨幹層
├─ 🎯 只訓練分類器層
├─ 📚 學習率: 0.001
└─ 目的: 讓分類器適應新任務

階段 2 (後 80% epochs):
├─ 🔓 解凍全模型
├─ 🎯 微調所有層
├─ 📚 學習率: 0.0001 (降低10倍)
└─ 目的: 精細調整特徵提取器
```

**優點**:
- ✅ 訓練速度快
- ✅ 不容易過擬合
- ✅ 通常能獲得最好效果
- ✅ 需要的訓練數據較少

**參數統計示例**:
```
階段 1:
📊 參數統計:
   總參數: 12,233,989
   可訓練參數: 103,525 (0.8%)

階段 2:
📊 參數統計:
   總參數: 12,233,989
   可訓練參數: 12,233,989 (100.0%)
```

### 2. 凍結骨幹訓練（Feature Extraction）

**適用場景**:
- 資料集較小（<5000張）
- 計算資源有限
- 快速原型驗證

**訓練流程**:
```
全程:
├─ 🔒 凍結預訓練骨幹層
├─ 🎯 只訓練分類器層
├─ 📚 學習率: 0.001
└─ 目的: 利用預訓練特徵，只學習分類
```

**優點**:
- ✅ 訓練速度最快
- ✅ 記憶體需求最小
- ✅ 不容易過擬合
- ✅ 適合小數據集

**缺點**:
- ❌ 精度可能稍低
- ❌ 無法適應與 ImageNet 差異大的資料

### 3. 全模型訓練（Full Training）

**適用場景**:
- 資料集較大（>20000張）
- 資料與 ImageNet 差異很大
- 追求極致精度

**訓練流程**:
```
全程:
├─ 🔓 訓練所有層
├─ 🎯 從預訓練權重開始
├─ 📚 學習率: 0.001
└─ 目的: 完全適應新任務特徵
```

**優點**:
- ✅ 可能達到最高精度
- ✅ 完全適應目標任務
- ✅ 靈活性最大

**缺點**:
- ❌ 訓練時間長
- ❌ 需要較多數據
- ❌ 容易過擬合（小數據集）

---

## 📊 訓練輸出示例

### 模型載入
```
🏗️  創建模型: efficientnet_b3
📦 載入 ImageNet 預訓練權重...

🎯 訓練策略: fine_tune
============================================================
🔒 已凍結預訓練層（只訓練分類器）
📊 參數統計:
   總參數: 12,233,989
   可訓練參數: 103,525 (0.8%)
📚 階段1: 凍結骨幹訓練分類器 (epoch 1-10)
📚 階段2: 將在 epoch 11 後解凍全模型微調
```

### 訓練過程
```
Epoch 1/50: 100%|████████████| 500/500 [02:15<00:00]
訓練 Loss: 2.3456 | 訓練 Acc: 0.4523
驗證 Loss: 2.1234 | 驗證 Acc: 0.5012
💾 模型已保存: models/taiwan_food_efficientnet_b3_epoch1.pth

...

============================================================
🔓 解凍模型，開始全模型微調 (epoch 11)
============================================================
🔓 已解凍所有層（全模型訓練）
📊 參數統計:
   總參數: 12,233,989
   可訓練參數: 12,233,989 (100.0%)
📉 調整學習率: 0.001 → 0.0001

Epoch 11/50: 100%|████████████| 500/500 [03:30<00:00]
訓練 Loss: 0.8234 | 訓練 Acc: 0.7845
驗證 Loss: 0.7512 | 驗證 Acc: 0.8123
💾 模型已保存: models/taiwan_food_efficientnet_b3_epoch11.pth
```

---

## 💡 最佳實踐建議

### 1. 模型選擇

| 需求 | 推薦模型 | 原因 |
|------|---------|------|
| 速度優先 | ResNet50 | 輕量快速 |
| 精度優先 | EfficientNet-B3 | 效能最佳 |
| 平衡性能 | ConvNeXt-Tiny | 現代架構 |
| 創新嘗試 | ViT | 注意力機制 |

### 2. 訓練策略選擇

```python
資料集大小 = len(訓練集)

if 資料集大小 < 5000:
    選擇 → 凍結骨幹訓練
    理由 → 避免過擬合，快速收斂
    
elif 5000 <= 資料集大小 <= 20000:
    選擇 → 微調訓練 (推薦)
    理由 → 平衡精度與訓練時間
    
else:  # 資料集大小 > 20000
    選擇 → 全模型訓練
    理由 → 充分利用數據，追求最高精度
```

### 3. 超參數建議

#### 微調訓練
```python
階段 1 (凍結骨幹):
├─ 學習率: 1e-3
├─ Batch Size: 32 (GPU) / 16 (CPU)
├─ Epochs: 5-10
└─ 優化器: Adam

階段 2 (全模型微調):
├─ 學習率: 1e-4 (降低10倍)
├─ Batch Size: 32 (GPU) / 16 (CPU)
├─ Epochs: 40-45
└─ 優化器: Adam
```

#### 凍結骨幹訓練
```python
├─ 學習率: 1e-3
├─ Batch Size: 32 (GPU) / 16 (CPU)
├─ Epochs: 30-50
└─ 優化器: Adam
```

#### 全模型訓練
```python
├─ 學習率: 1e-3 (初始)
├─ 學習率衰減: 每10 epoch * 0.5
├─ Batch Size: 32 (GPU) / 16 (CPU)
├─ Epochs: 50-100
└─ 優化器: Adam with weight_decay=1e-4
```

### 4. 資料增強

所有預訓練模型都經過 ImageNet 訓練，使用標準化：
```python
mean = [0.485, 0.456, 0.406]
std = [0.229, 0.224, 0.225]
```

建議的資料增強策略：
```python
訓練集增強:
├─ RandomResizedCrop(224)
├─ RandomHorizontalFlip()
├─ ColorJitter(brightness=0.2, contrast=0.2)
├─ RandomRotation(15)
└─ Normalize(mean, std)

驗證集增強:
├─ Resize(256)
├─ CenterCrop(224)
└─ Normalize(mean, std)
```

---

## 🔧 進階功能

### 手動控制凍結/解凍

如果需要更精細的控制，可以修改代碼：

```python
from pytorch_model import freeze_backbone

# 載入模型
model = get_model('efficientnet_b3', num_classes=101, pretrained=True)

# 凍結骨幹
model = freeze_backbone(model, freeze=True)

# ... 訓練 N 個 epochs ...

# 解凍骨幹
model = freeze_backbone(model, freeze=False)

# 降低學習率
for param_group in optimizer.param_groups:
    param_group['lr'] = param_group['lr'] * 0.1
```

### 選擇性凍結特定層

```python
# 只凍結前幾層
for name, param in model.named_parameters():
    if 'layer1' in name or 'layer2' in name:
        param.requires_grad = False
    else:
        param.requires_grad = True
```

### 不使用預訓練權重

如果想從隨機初始化開始訓練：

```python
# 在 pytorch_model.py 中修改
model = get_model('efficientnet_b3', num_classes=101, pretrained=False)
```

---

## 📈 預期效果

### Taiwan Food 101 資料集

使用預訓練模型 vs 隨機初始化：

| 模型 | 預訓練 | 驗證準確率 | 訓練時間 |
|------|-------|-----------|---------|
| EfficientNet-B3 | ✅ 是 | **82-85%** | 3-4 hours |
| EfficientNet-B3 | ❌ 否 | 65-70% | 8-10 hours |
| ResNet50 | ✅ 是 | **78-82%** | 2-3 hours |
| ResNet50 | ❌ 否 | 60-65% | 6-8 hours |

**結論**: 
- ✅ 預訓練模型可提升 **15-20%** 準確率
- ✅ 訓練時間減少 **50-60%**
- ✅ 更穩定的收斂過程

---

## 🐛 故障排除

### 問題 1: 記憶體不足
```
RuntimeError: CUDA out of memory
```
**解決方案**:
1. 減少 batch_size (32 → 16 → 8)
2. 選擇較小的模型 (ViT → EfficientNet → ResNet)
3. 使用梯度累積

### 問題 2: 訓練不收斂
```
Loss 不下降或震盪
```
**解決方案**:
1. 確認使用了預訓練權重
2. 降低學習率 (1e-3 → 1e-4)
3. 使用微調策略而非全模型訓練

### 問題 3: 過擬合
```
訓練準確率高，驗證準確率低
```
**解決方案**:
1. 使用凍結骨幹訓練策略
2. 增加 dropout_rate (0.3 → 0.5)
3. 增加資料增強
4. 減少訓練 epochs

---

## 📚 參考資源

- [Transfer Learning Tutorial](https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)
- [Fine-tuning Best Practices](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)
- [EfficientNet Paper](https://arxiv.org/abs/1905.11946)
- [Vision Transformer Paper](https://arxiv.org/abs/2010.11929)

---

**更新日期**: 2025-10-08  
**版本**: v1.0.0  
**狀態**: ✅ 完成並可用