"""
ç°¡å–®çš„ GPU æ¸¬è©¦è…³æœ¬
å¿«é€Ÿé©—è­‰ PyTorch æ˜¯å¦èƒ½æ­£ç¢ºä½¿ç”¨ GPU
"""

import torch
import torch.nn as nn
import time

print("=" * 70)
print("ğŸ” PyTorch GPU å¿«é€Ÿæ¸¬è©¦")
print("=" * 70)

# 1. åŸºæœ¬æª¢æŸ¥
print("\n1ï¸âƒ£ åŸºæœ¬æª¢æŸ¥")
print("-" * 70)
print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
print(f"CUDA æ˜¯å¦å¯ç”¨: {torch.cuda.is_available()}")

if not torch.cuda.is_available():
    print("\nâŒ CUDA ä¸å¯ç”¨ï¼")
    print("\nå¯èƒ½çš„åŸå› :")
    print("1. PyTorch æ˜¯ CPU ç‰ˆæœ¬ï¼ˆéœ€è¦é‡æ–°å®‰è£ GPU ç‰ˆæœ¬ï¼‰")
    print("2. NVIDIA é©…å‹•æœªå®‰è£")
    print("3. CUDA ç’°å¢ƒæœªé…ç½®")
    print("\nå®‰è£ GPU ç‰ˆæœ¬ PyTorch:")
    print("pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
    exit(1)

print(f"CUDA ç‰ˆæœ¬: {torch.version.cuda}")
print(f"cuDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
print(f"GPU æ•¸é‡: {torch.cuda.device_count()}")

for i in range(torch.cuda.device_count()):
    print(f"GPU {i}: {torch.cuda.get_device_name(i)}")
    props = torch.cuda.get_device_properties(i)
    print(f"   è¨˜æ†¶é«”: {props.total_memory / 1024**3:.2f} GB")
    print(f"   è¨ˆç®—èƒ½åŠ›: {props.major}.{props.minor}")

# 2. è¨­å‚™æ¸¬è©¦
print("\n2ï¸âƒ£ è¨­å‚™é…ç½®æ¸¬è©¦")
print("-" * 70)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"é¸æ“‡çš„è¨­å‚™: {device}")

# å‰µå»ºæ¸¬è©¦å¼µé‡
test_tensor = torch.randn(100, 100).to(device)
print(f"æ¸¬è©¦å¼µé‡è¨­å‚™: {test_tensor.device}")
print(f"æ¸¬è©¦å¼µé‡å½¢ç‹€: {test_tensor.shape}")

if test_tensor.device.type == 'cuda':
    print("âœ… å¼µé‡æˆåŠŸå‰µå»ºåœ¨ GPU ä¸Š")
else:
    print("âŒ å¼µé‡åœ¨ CPU ä¸Šï¼ˆæœ‰å•é¡Œï¼‰")

# 3. è¨ˆç®—æ€§èƒ½æ¸¬è©¦
print("\n3ï¸âƒ£ GPU vs CPU æ€§èƒ½å°æ¯”")
print("-" * 70)

# CPU æ¸¬è©¦
print("æ¸¬è©¦ CPU æ€§èƒ½...")
a_cpu = torch.randn(2000, 2000)
b_cpu = torch.randn(2000, 2000)
start = time.time()
c_cpu = torch.matmul(a_cpu, b_cpu)
cpu_time = time.time() - start
print(f"CPU çŸ©é™£ä¹˜æ³•: {cpu_time*1000:.2f} ms")

# GPU æ¸¬è©¦
print("æ¸¬è©¦ GPU æ€§èƒ½...")
a_gpu = torch.randn(2000, 2000).to(device)
b_gpu = torch.randn(2000, 2000).to(device)
torch.cuda.synchronize()  # ç¢ºä¿é–‹å§‹è¨ˆæ™‚å‰ GPU ç©ºé–’
start = time.time()
c_gpu = torch.matmul(a_gpu, b_gpu)
torch.cuda.synchronize()  # ç­‰å¾… GPU è¨ˆç®—å®Œæˆ
gpu_time = time.time() - start
print(f"GPU çŸ©é™£ä¹˜æ³•: {gpu_time*1000:.2f} ms")

speedup = cpu_time / gpu_time
print(f"\nâš¡ GPU åŠ é€Ÿæ¯”: {speedup:.2f}x")

if speedup > 2:
    print("âœ… GPU æ­£å¸¸å·¥ä½œï¼")
elif speedup > 1:
    print("âš ï¸  GPU æœ‰åŠ é€Ÿï¼Œä½†å¯èƒ½æœ‰å•é¡Œ")
else:
    print("âŒ GPU å¯èƒ½æ²’æœ‰æ­£å¸¸å·¥ä½œ")

# 4. CNN æ¨¡å‹æ¸¬è©¦
print("\n4ï¸âƒ£ CNN æ¨¡å‹ GPU æ¸¬è©¦")
print("-" * 70)

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, 3, padding=1)
        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)
        self.fc = nn.Linear(128 * 56 * 56, 101)
        self.relu = nn.ReLU()
        self.pool = nn.MaxPool2d(2)
    
    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x

print("å‰µå»º CNN æ¨¡å‹...")
model = SimpleCNN().to(device)
print(f"æ¨¡å‹è¨­å‚™: {next(model.parameters()).device}")

# æ¸¬è©¦å‰å‘å‚³æ’­
print("æ¸¬è©¦æ¨¡å‹æ¨ç†...")
test_input = torch.randn(16, 3, 224, 224).to(device)
print(f"è¼¸å…¥æ•¸æ“šè¨­å‚™: {test_input.device}")

with torch.no_grad():
    output = model(test_input)
print(f"è¼¸å‡ºè¨­å‚™: {output.device}")
print(f"è¼¸å‡ºå½¢ç‹€: {output.shape}")

if output.device.type == 'cuda':
    print("âœ… æ¨¡å‹åœ¨ GPU ä¸ŠæˆåŠŸæ¨ç†")
else:
    print("âŒ æ¨¡å‹æ¨ç†åœ¨ CPU ä¸Šï¼ˆæœ‰å•é¡Œï¼‰")

# 5. è¨˜æ†¶é«”æª¢æŸ¥
print("\n5ï¸âƒ£ GPU è¨˜æ†¶é«”ä½¿ç”¨")
print("-" * 70)
memory_allocated = torch.cuda.memory_allocated(0) / 1024**2
memory_reserved = torch.cuda.memory_reserved(0) / 1024**2
memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**2

print(f"å·²åˆ†é…: {memory_allocated:.2f} MB")
print(f"å·²ä¿ç•™: {memory_reserved:.2f} MB")
print(f"ç¸½è¨˜æ†¶é«”: {memory_total:.2f} MB")
print(f"ä½¿ç”¨ç‡: {memory_allocated/memory_total*100:.2f}%")

if memory_allocated > 10:
    print("âœ… GPU è¨˜æ†¶é«”æœ‰åˆ†é…ï¼ˆæ­£åœ¨ä½¿ç”¨ï¼‰")
else:
    print("âš ï¸  GPU è¨˜æ†¶é«”åˆ†é…è¼ƒå°‘")

# 6. è¨“ç·´æ¨¡æ“¬æ¸¬è©¦
print("\n6ï¸âƒ£ è¨“ç·´æµç¨‹æ¨¡æ“¬")
print("-" * 70)

print("æ¨¡æ“¬è¨“ç·´æ­¥é©Ÿ...")
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# å‰µå»ºå‡æ•¸æ“š
images = torch.randn(32, 3, 224, 224).to(device)
labels = torch.randint(0, 101, (32,)).to(device)

print(f"è¨“ç·´æ•¸æ“šè¨­å‚™: {images.device}")
print(f"è¨“ç·´æ¨™ç±¤è¨­å‚™: {labels.device}")

# è¨“ç·´æ­¥é©Ÿ
model.train()
start = time.time()
optimizer.zero_grad()
outputs = model(images)
loss = criterion(outputs, labels)
loss.backward()
optimizer.step()
train_time = time.time() - start

print(f"è¨“ç·´æ­¥é©Ÿå®Œæˆ: {train_time*1000:.2f} ms")
print(f"Loss: {loss.item():.4f}")
print(f"Loss å¼µé‡è¨­å‚™: {loss.device}")

if loss.device.type == 'cuda':
    print("âœ… è¨“ç·´éç¨‹åœ¨ GPU ä¸ŠåŸ·è¡Œ")
else:
    print("âŒ è¨“ç·´éç¨‹åœ¨ CPU ä¸ŠåŸ·è¡Œï¼ˆæœ‰å•é¡Œï¼‰")

# æ¸…ç†
del model, test_input, output, images, labels, loss
torch.cuda.empty_cache()

print("\n" + "=" * 70)
print("âœ… æ¸¬è©¦å®Œæˆï¼")
print("=" * 70)

print("\nğŸ“‹ ç¸½çµ:")
if torch.cuda.is_available():
    print("âœ… CUDA å¯ç”¨")
    print("âœ… GPU è¨ˆç®—æ­£å¸¸")
    print("âœ… æ¨¡å‹å¯ä»¥åœ¨ GPU ä¸Šé‹è¡Œ")
    print("âœ… è¨“ç·´æµç¨‹å¯ä»¥ä½¿ç”¨ GPU")
    print("\nğŸ‰ æ‚¨çš„ç’°å¢ƒå·²æº–å‚™å¥½ä½¿ç”¨ GPU è¨“ç·´ï¼")
    print("\nä¸‹ä¸€æ­¥: åŸ·è¡Œ python train_pytorch.py é–‹å§‹è¨“ç·´")
else:
    print("âŒ GPU ä¸å¯ç”¨ï¼Œè«‹æª¢æŸ¥é…ç½®")
