#!/usr/bin/env python3
"""
ç°¡å–®çš„ NPU/GPU è¨­å‚™æ¸¬è©¦è…³æœ¬
æ¸¬è©¦æ˜¯å¦èƒ½æ­£ç¢ºæª¢æ¸¬å’Œä½¿ç”¨å„ç¨®åŠ é€Ÿè¨­å‚™
"""

import torch
import sys

def test_cuda():
    """æ¸¬è©¦ CUDA GPU"""
    print("ğŸ” æ¸¬è©¦ CUDA GPU...")
    try:
        if torch.cuda.is_available():
            device = torch.device('cuda')
            x = torch.randn(100, 100).to(device)
            y = torch.randn(100, 100).to(device)
            z = torch.mm(x, y)
            print(f"âœ… CUDA GPU å¯ç”¨: {torch.cuda.get_device_name()}")
            return True, 'cuda'
        else:
            print("âŒ CUDA GPU ä¸å¯ç”¨")
            return False, None
    except Exception as e:
        print(f"âŒ CUDA GPU æ¸¬è©¦å¤±æ•—: {e}")
        return False, None

def test_amd_npu():
    """æ¸¬è©¦ AMD Ryzen AI NPU"""
    print("ğŸ” æ¸¬è©¦ AMD Ryzen AI NPU...")
    try:
        import onnxruntime as ort
        providers = ort.get_available_providers()
        if 'DmlExecutionProvider' in providers:
            # å‰µå»ºæ¸¬è©¦æ¨¡å‹
            import numpy as np
            
            # ç°¡å–®çš„æ¸¬è©¦é‹ç®—
            session_options = ort.SessionOptions()
            session_options.enable_mem_pattern = False
            
            print("âœ… AMD Ryzen AI NPU å¯ç”¨ (ONNX Runtime)")
            print(f"   å¯ç”¨æä¾›è€…: {providers[:3]}...")  # åªé¡¯ç¤ºå‰å¹¾å€‹
            return True, 'amd_npu'
        else:
            print("âŒ AMD NPU ä¸å¯ç”¨")
            print(f"   å¯ç”¨æä¾›è€…: {providers}")
            return False, None
    except ImportError:
        print("âŒ ONNX Runtime æœªå®‰è£")
        print("   åŸ·è¡Œ: pip install onnxruntime-directml")
        return False, None
    except Exception as e:
        print(f"âŒ AMD NPU æ¸¬è©¦å¤±æ•—: {e}")
        return False, None

def test_directml():
    """æ¸¬è©¦ DirectML (Intel NPU)"""
    print("ğŸ” æ¸¬è©¦ Intel DirectML NPU...")
    try:
        import torch_directml
        if torch_directml.is_available():
            device = torch_directml.device()
            x = torch.randn(100, 100).to(device)
            y = torch.randn(100, 100).to(device)
            z = torch.mm(x, y)
            print(f"âœ… Intel DirectML NPU å¯ç”¨: {device}")
            return True, 'intel_dml'
        else:
            print("âŒ Intel DirectML NPU ä¸å¯ç”¨")
            return False, None
    except ImportError:
        print("âŒ torch_directml æœªå®‰è£")
        print("   åŸ·è¡Œ: pip install torch-directml")
        return False, None
    except Exception as e:
        print(f"âŒ Intel DirectML NPU æ¸¬è©¦å¤±æ•—: {e}")
        return False, None

def test_mps():
    """æ¸¬è©¦ Apple MPS"""
    print("ğŸ” æ¸¬è©¦ Apple MPS...")
    try:
        if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            device = torch.device('mps')
            x = torch.randn(100, 100).to(device)
            y = torch.randn(100, 100).to(device)
            z = torch.mm(x, y)
            print("âœ… Apple MPS å¯ç”¨")
            return True, 'mps'
        else:
            print("âŒ Apple MPS ä¸å¯ç”¨")
            return False, None
    except Exception as e:
        print(f"âŒ Apple MPS æ¸¬è©¦å¤±æ•—: {e}")
        return False, None

def test_cpu():
    """æ¸¬è©¦ CPU"""
    print("ğŸ” æ¸¬è©¦ CPU...")
    try:
        device = torch.device('cpu')
        x = torch.randn(100, 100).to(device)
        y = torch.randn(100, 100).to(device)
        z = torch.mm(x, y)
        print("âœ… CPU å¯ç”¨")
        return True, 'cpu'
    except Exception as e:
        print(f"âŒ CPU æ¸¬è©¦å¤±æ•—: {e}")
        return False, None

def main():
    print("=" * 50)
    print("ğŸ§ª è¨­å‚™åŠ é€Ÿæ¸¬è©¦")
    print("=" * 50)
    print(f"Python ç‰ˆæœ¬: {sys.version}")
    print(f"PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print("=" * 50)
    
    available_devices = []
    
    # æ¸¬è©¦æ‰€æœ‰è¨­å‚™
    tests = [
        ("CUDA GPU", test_cuda),
        ("AMD Ryzen AI NPU", test_amd_npu),
        ("Intel DirectML NPU", test_directml),
        ("Apple MPS", test_mps),
        ("CPU", test_cpu)
    ]
    
    for name, test_func in tests:
        print()
        success, device_type = test_func()
        if success:
            available_devices.append((name, device_type))
    
    # é¡¯ç¤ºçµæœ
    print("\n" + "=" * 50)
    print("ğŸ“Š æ¸¬è©¦çµæœæ‘˜è¦")
    print("=" * 50)
    
    if available_devices:
        print("âœ… å¯ç”¨çš„è¨­å‚™:")
        for i, (name, device_type) in enumerate(available_devices, 1):
            print(f"   {i}. {name} ({device_type})")
    else:
        print("âŒ æ²’æœ‰å¯ç”¨çš„è¨­å‚™")
    
    print("\nğŸ’¡ å»ºè­°:")
    if any('AMD' in name or 'Ryzen AI' in name for name, _ in available_devices):
        print("   ğŸš€ AMD Ryzen AI NPU å¯ç”¨ï¼å»ºè­°åœ¨è¨“ç·´æ™‚é¸æ“‡ AMD NPU ä»¥ç²å¾—æ›´å¥½çš„æ€§èƒ½")
    elif any('NPU' in name or 'DirectML' in name for name, _ in available_devices):
        print("   ğŸš€ Intel NPU å¯ç”¨ï¼å»ºè­°åœ¨è¨“ç·´æ™‚é¸æ“‡ Intel NPU ä»¥ç²å¾—æ›´å¥½çš„æ€§èƒ½")
    elif any('CUDA' in name for name, _ in available_devices):
        print("   ğŸ¯ GPU å¯ç”¨ï¼å»ºè­°åœ¨è¨“ç·´æ™‚é¸æ“‡ GPU ä»¥ç²å¾—æ›´å¥½çš„æ€§èƒ½")
    else:
        print("   ğŸ’» åªæœ‰ CPU å¯ç”¨ï¼Œè€ƒæ…®å®‰è£ GPU æˆ– NPU æ”¯æ´")
    
    # å®‰è£å»ºè­°
    if not any('DirectML' in name or 'AMD' in name for name, _ in available_devices):
        print("\nğŸ› ï¸  å®‰è£ NPU æ”¯æ´:")
        print("   Intel NPU: pip install torch-directml")
        print("   AMD NPU:   pip install onnxruntime-directml")
        print("   æˆ–åŸ·è¡Œ:    install_npu.bat")
    
    print("\nâœ… æ¸¬è©¦å®Œæˆï¼")

if __name__ == "__main__":
    main()