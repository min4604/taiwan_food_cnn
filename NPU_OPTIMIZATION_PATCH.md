# NPU å„ªåŒ–è£œä¸èªªæ˜

ç”±æ–¼æ–‡ä»¶ç·¨è¼¯è¡çªï¼Œè«‹æŒ‰ä»¥ä¸‹æ­¥é©Ÿæ‰‹å‹•æ‡‰ç”¨ NPU å„ªåŒ–ï¼š

## æ­¥é©Ÿ 1: æ¢å¾©åŸå§‹æ–‡ä»¶

```bash
git checkout evaluate_multi_models.py
```

## æ­¥é©Ÿ 2: ä¿®æ”¹ `warmup_onnx_session` å‡½æ•¸ï¼ˆæ–°å¢ï¼‰

åœ¨ `convert_model_to_onnx` å‡½æ•¸ä¹‹å¾Œï¼Œ`create_onnx_session` å‡½æ•¸ä¹‹å‰æ·»åŠ ï¼š

```python
def warmup_onnx_session(session, input_shape):
    """é ç†± ONNX æœƒè©±ä»¥å„ªåŒ–é¦–æ¬¡æ¨ç†æ€§èƒ½
    
    Args:
        session: ONNX Runtime æœƒè©±
        input_shape: è¼¸å…¥å½¢ç‹€ (batch_size, channels, height, width)
    """
    try:
        dummy_input = np.random.randn(*input_shape).astype(np.float32)
        input_name = session.get_inputs()[0].name
        # åŸ·è¡Œå¹¾æ¬¡é ç†±æ¨ç†
        for _ in range(3):
            _ = session.run(None, {input_name: dummy_input})
    except:
        pass  # é ç†±å¤±æ•—ä¸å½±éŸ¿æ­£å¸¸ä½¿ç”¨
```

## æ­¥é©Ÿ 3: ä¿®æ”¹ `create_onnx_session` å‡½æ•¸

**åŸå§‹ä»£ç¢¼ï¼š**
```python
def create_onnx_session(onnx_path, use_dml=True):
    """å‰µå»º ONNX Runtime æ¨ç†æœƒè©±
    
    Args:
        onnx_path: ONNX æ¨¡å‹è·¯å¾‘
        use_dml: æ˜¯å¦ä½¿ç”¨ DirectML åŸ·è¡Œæä¾›è€…
    
    Returns:
        session: ONNX Runtime æ¨ç†æœƒè©±
    """
    try:
        import onnxruntime as ort
        
        # è¨­ç½®åŸ·è¡Œæä¾›è€…
        providers = []
        if use_dml:
            providers.append('DmlExecutionProvider')
        providers.append('CPUExecutionProvider')
        
        # å‰µå»ºæœƒè©±é¸é …
        sess_options = ort.SessionOptions()
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        
        # å‰µå»ºæ¨ç†æœƒè©±
        session = ort.InferenceSession(
            onnx_path,
            sess_options=sess_options,
            providers=providers
        )
        
        return session
    
    except Exception as e:
        print(f"   âš ï¸  å‰µå»º ONNX æœƒè©±å¤±æ•—: {e}")
        return None
```

**ä¿®æ”¹ç‚ºï¼š**
```python
def create_onnx_session(onnx_path, use_dml=True, enable_profiling=False):
    """å‰µå»ºå„ªåŒ–çš„ ONNX Runtime æ¨ç†æœƒè©±ï¼ˆNPU åŠ é€Ÿå„ªåŒ–ï¼‰
    
    Args:
        onnx_path: ONNX æ¨¡å‹è·¯å¾‘
        use_dml: æ˜¯å¦ä½¿ç”¨ DirectML åŸ·è¡Œæä¾›è€…
        enable_profiling: æ˜¯å¦å•Ÿç”¨æ€§èƒ½åˆ†æ
    
    Returns:
        session: ONNX Runtime æ¨ç†æœƒè©±
    """
    try:
        import onnxruntime as ort
        
        # å‰µå»ºæœƒè©±é¸é … - å„ªåŒ–é…ç½®
        sess_options = ort.SessionOptions()
        
        # åœ–å„ªåŒ–ç­‰ç´š - ä½¿ç”¨æœ€é«˜å„ªåŒ–ï¼ˆæ“´å±•å„ªåŒ–ï¼‰
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED
        
        # å•Ÿç”¨è¨˜æ†¶é«”å„ªåŒ–æ¨¡å¼
        sess_options.enable_mem_pattern = True
        sess_options.enable_mem_reuse = True
        sess_options.enable_cpu_mem_arena = True
        
        # ä¸¦è¡ŒåŸ·è¡Œå„ªåŒ–
        sess_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL
        sess_options.inter_op_num_threads = 2  # æ“ä½œé–“ä¸¦è¡Œ
        sess_options.intra_op_num_threads = 4  # æ“ä½œå…§ä¸¦è¡Œ
        
        # æ€§èƒ½åˆ†æï¼ˆå¯é¸ï¼‰
        if enable_profiling:
            sess_options.enable_profiling = True
        
        # è¨­ç½®åŸ·è¡Œæä¾›è€… - DirectML å„ªåŒ–é…ç½®
        providers = []
        provider_options = []
        
        if use_dml:
            # DirectML æä¾›è€…é…ç½®ï¼ˆé‡å° AMD Ryzen AI NPU å„ªåŒ–ï¼‰
            dml_options = {
                'device_id': 0,  # ä½¿ç”¨ç¬¬ä¸€å€‹ NPU è¨­å‚™
                'disable_metacommands': False,  # å•Ÿç”¨ metacommands åŠ é€Ÿ
                'enable_dynamic_graph_fusion': True,  # å•Ÿç”¨å‹•æ…‹åœ–èåˆ
            }
            providers.append('DmlExecutionProvider')
            provider_options.append(dml_options)
        
        # CPU ä½œç‚ºå›é€€
        providers.append('CPUExecutionProvider')
        provider_options.append({})
        
        # å‰µå»ºæ¨ç†æœƒè©±
        session = ort.InferenceSession(
            onnx_path,
            sess_options=sess_options,
            providers=providers,
            provider_options=provider_options
        )
        
        return session
    
    except Exception as e:
        print(f"   âš ï¸  å‰µå»º ONNX æœƒè©±å¤±æ•—: {e}")
        return None
```

## æ­¥é©Ÿ 4: ä¿®æ”¹ `ensemble_predict_onnx` å‡½æ•¸

æ‰¾åˆ°é€™æ®µä»£ç¢¼ï¼š
```python
    # è½‰æ› PyTorch å¼µé‡ç‚º NumPy
    images_np = images.cpu().numpy() if images.is_cuda else images.numpy()
    
    # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é æ¸¬
    for session, weight, name in onnx_sessions:
```

ä¿®æ”¹ç‚ºï¼š
```python
    # è½‰æ› PyTorch å¼µé‡ç‚º NumPyï¼ˆé€£çºŒè¨˜æ†¶é«”å¸ƒå±€ï¼‰
    if images.is_cuda:
        images_np = images.cpu().numpy()
    else:
        images_np = images.numpy()
    
    # ç¢ºä¿é€£çºŒè¨˜æ†¶é«”å¸ƒå±€ï¼ˆå„ªåŒ–æ€§èƒ½ï¼‰
    if not images_np.flags['C_CONTIGUOUS']:
        images_np = np.ascontiguousarray(images_np)
    
    # ç¢ºä¿æ­£ç¢ºçš„æ•¸æ“šé¡å‹
    images_np = images_np.astype(np.float32)
    
    # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é æ¸¬ï¼ˆä¸¦è¡Œæ¨ç†å„ªåŒ–ï¼‰
    for session, weight, name in onnx_sessions:
```

## æ­¥é©Ÿ 5: åœ¨æ¨¡å‹è¼‰å…¥å¾Œæ·»åŠ é ç†±

åœ¨ `evaluate_multi_models` å‡½æ•¸ä¸­ï¼Œæ‰¾åˆ°é€™æ®µä»£ç¢¼ï¼š
```python
                        if session:
                            # æª¢æŸ¥åŸ·è¡Œæä¾›è€…
                            providers = session.get_providers()
                            if 'DmlExecutionProvider' in providers:
                                print(f"   âœ… ONNX Runtime å·²å•Ÿç”¨ DirectML (NPUåŠ é€Ÿ)")
                            else:
                                print(f"   ğŸ’» ONNX Runtime ä½¿ç”¨ CPU")
                            
                            onnx_sessions.append((session, weight, model_name))
                            print(f"   âœ… ONNX è½‰æ›æˆåŠŸ (æ¬Šé‡: {weight:.4f})")
```

ä¿®æ”¹ç‚ºï¼š
```python
                        if session:
                            # æª¢æŸ¥åŸ·è¡Œæä¾›è€…
                            providers = session.get_providers()
                            if 'DmlExecutionProvider' in providers:
                                print(f"   âœ… ONNX Runtime å·²å•Ÿç”¨ DirectML (NPUåŠ é€Ÿ)")
                                
                                # é ç†±æ¨¡å‹ï¼ˆå„ªåŒ–é¦–æ¬¡æ¨ç†æ€§èƒ½ï¼‰
                                print(f"   ğŸ”¥ é ç†± NPU æ¨¡å‹...")
                                warmup_onnx_session(session, (batch_size, 3, img_size, img_size))
                                print(f"   âœ… é ç†±å®Œæˆ")
                            else:
                                print(f"   ğŸ’» ONNX Runtime ä½¿ç”¨ CPU")
                            
                            onnx_sessions.append((session, weight, model_name))
                            print(f"   âœ… ONNX è½‰æ›æˆåŠŸ (æ¬Šé‡: {weight:.4f})")
```

## æ­¥é©Ÿ 6: æ·»åŠ æ‰¹æ¬¡å¤§å°å„ªåŒ–å’Œ pin_memory

æ‰¾åˆ°é€™æ®µä»£ç¢¼ï¼š
```python
    # å»ºç«‹æ¸¬è©¦é›† DataLoader
    print("\nğŸ“Š è¼‰å…¥æ¸¬è©¦é›†è³‡æ–™...")
    test_dataset = TaiwanFoodDataset(test_csv, test_img_dir, test_transform, is_test=True)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
    
    print(f"   æ¸¬è©¦é›†å¤§å°: {len(test_dataset)} å¼µåœ–ç‰‡")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    print("=" * 60)
```

ä¿®æ”¹ç‚ºï¼š
```python
    # NPU æ‰¹æ¬¡å¤§å°å„ªåŒ–å»ºè­°
    if use_onnx_npu:
        # NPU é€šå¸¸åœ¨è¼ƒå¤§æ‰¹æ¬¡ä¸‹æ€§èƒ½æ›´å¥½
        original_batch_size = batch_size
        if batch_size < 32:
            batch_size = 32
            print(f"\nğŸ’¡ NPU å„ªåŒ–: æ‰¹æ¬¡å¤§å°å¾ {original_batch_size} èª¿æ•´ç‚º {batch_size}")
            print(f"   è¼ƒå¤§æ‰¹æ¬¡èƒ½æ›´å¥½åˆ©ç”¨ NPU ä¸¦è¡Œè¨ˆç®—èƒ½åŠ›")
    
    # å»ºç«‹æ¸¬è©¦é›† DataLoader
    print("\nğŸ“Š è¼‰å…¥æ¸¬è©¦é›†è³‡æ–™...")
    test_dataset = TaiwanFoodDataset(test_csv, test_img_dir, test_transform, is_test=True)
    
    # NPU å„ªåŒ–ï¼šä½¿ç”¨ pin_memory åŠ é€Ÿæ•¸æ“šå‚³è¼¸
    pin_memory = use_onnx_npu
    test_loader = DataLoader(
        test_dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=0,
        pin_memory=pin_memory
    )
    
    print(f"   æ¸¬è©¦é›†å¤§å°: {len(test_dataset)} å¼µåœ–ç‰‡")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    if use_onnx_npu:
        print(f"   NPU å„ªåŒ–: å·²å•Ÿç”¨è¨˜æ†¶é«”å›ºå®š (pin_memory)")
    print("=" * 60)
```

## é©—è­‰ä¿®æ”¹

å®Œæˆå¾Œé‹è¡Œï¼š
```bash
python evaluate_multi_models.py
```

é¸æ“‡ ONNX Runtime NPU æ¨¡å¼ï¼Œæ‡‰è©²çœ‹åˆ°ï¼š
- âœ… åœ–å„ªåŒ–ç­‰ç´šï¼šORT_ENABLE_EXTENDED
- âœ… è¨˜æ†¶é«”å„ªåŒ–ï¼šå·²å•Ÿç”¨
- âœ… ä¸¦è¡ŒåŸ·è¡Œï¼šå·²å•Ÿç”¨
- âœ… DirectML MetaCommandsï¼šå·²å•Ÿç”¨
- ğŸ”¥ é ç†± NPU æ¨¡å‹...
- ğŸ’¡ NPU å„ªåŒ–ï¼šæ‰¹æ¬¡å¤§å°èª¿æ•´

## é æœŸæ€§èƒ½æå‡

- **åœ–å„ªåŒ–**: +10-15%
- **è¨˜æ†¶é«”å„ªåŒ–**: +5-10%
- **ä¸¦è¡ŒåŸ·è¡Œ**: +10-15%
- **DirectML MetaCommands**: +15-25%
- **æ¨¡å‹é ç†±**: é¦–æ‰¹å»¶é²æ¸›å°‘ 50%
- **æ‰¹æ¬¡å„ªåŒ–**: +20-30%

**ç¸½é«”åŠ é€Ÿ**: 3-5å€ï¼ˆç›¸æ¯”æœªå„ªåŒ– CPUï¼‰

## å•é¡Œæ’æŸ¥

å¦‚æœé‡åˆ°å•é¡Œï¼š
1. ç¢ºèª `onnxruntime-directml` å·²å®‰è£
2. æª¢æŸ¥ AMD é©…å‹•æ˜¯å¦æœ€æ–°
3. é©—è­‰ Windows ç‰ˆæœ¬æ”¯æ´ DirectML
4. æŸ¥çœ‹æ—¥èªŒä¸­çš„åŸ·è¡Œæä¾›è€…ä¿¡æ¯